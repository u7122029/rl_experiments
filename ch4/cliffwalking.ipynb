{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T18:16:36.580835Z",
     "start_time": "2025-06-23T18:16:36.577804Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:53:12.887412Z",
     "start_time": "2025-06-24T03:53:12.883648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = TimeLimit(gym.make('CliffWalking-v0'), max_episode_steps=60)\n",
    "train_seed = 42\n",
    "test_seed = 44"
   ],
   "id": "febaa082a68ad8bc",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:45:04.530407Z",
     "start_time": "2025-06-24T03:45:04.527169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "observation, _ = env.reset(seed=test_seed)\n",
    "observation"
   ],
   "id": "ab2760a5742d3ff9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:45:04.981866Z",
     "start_time": "2025-06-24T03:45:04.979031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def play_policy(env, policy=None):\n",
    "    observation, _ = env.reset()\n",
    "    episode_reward, elapsed_steps = 0, 0\n",
    "\n",
    "    while True:\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.random.choice(env.action_space.n, p=policy[observation])\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        elapsed_steps += 1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return episode_reward, elapsed_steps"
   ],
   "id": "fb0dc4555d50c40d",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:45:05.338012Z",
     "start_time": "2025-06-24T03:45:05.335408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_policy(n, env, policy, test_seed):\n",
    "    env.reset(seed=test_seed)\n",
    "    rewards = [play_policy(env, policy)[0] for _ in range(n)]\n",
    "    return np.mean(rewards), np.std(rewards)"
   ],
   "id": "e4cc3584418c8029",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:45:06.195275Z",
     "start_time": "2025-06-24T03:45:06.187352Z"
    }
   },
   "cell_type": "code",
   "source": "env.unwrapped.__dict__",
   "id": "9d23f032da387a50",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shape': (4, 12),\n",
       " 'start_state_index': np.int64(36),\n",
       " 'nS': np.int64(48),\n",
       " 'nA': 4,\n",
       " 'is_slippery': False,\n",
       " '_cliff': array([[False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True, False]]),\n",
       " 'P': {0: {0: [(1.0, np.int64(0), -1, False)],\n",
       "   1: [(1.0, np.int64(1), -1, False)],\n",
       "   2: [(1.0, np.int64(12), -1, False)],\n",
       "   3: [(1.0, np.int64(0), -1, False)]},\n",
       "  1: {0: [(1.0, np.int64(1), -1, False)],\n",
       "   1: [(1.0, np.int64(2), -1, False)],\n",
       "   2: [(1.0, np.int64(13), -1, False)],\n",
       "   3: [(1.0, np.int64(0), -1, False)]},\n",
       "  2: {0: [(1.0, np.int64(2), -1, False)],\n",
       "   1: [(1.0, np.int64(3), -1, False)],\n",
       "   2: [(1.0, np.int64(14), -1, False)],\n",
       "   3: [(1.0, np.int64(1), -1, False)]},\n",
       "  3: {0: [(1.0, np.int64(3), -1, False)],\n",
       "   1: [(1.0, np.int64(4), -1, False)],\n",
       "   2: [(1.0, np.int64(15), -1, False)],\n",
       "   3: [(1.0, np.int64(2), -1, False)]},\n",
       "  4: {0: [(1.0, np.int64(4), -1, False)],\n",
       "   1: [(1.0, np.int64(5), -1, False)],\n",
       "   2: [(1.0, np.int64(16), -1, False)],\n",
       "   3: [(1.0, np.int64(3), -1, False)]},\n",
       "  5: {0: [(1.0, np.int64(5), -1, False)],\n",
       "   1: [(1.0, np.int64(6), -1, False)],\n",
       "   2: [(1.0, np.int64(17), -1, False)],\n",
       "   3: [(1.0, np.int64(4), -1, False)]},\n",
       "  6: {0: [(1.0, np.int64(6), -1, False)],\n",
       "   1: [(1.0, np.int64(7), -1, False)],\n",
       "   2: [(1.0, np.int64(18), -1, False)],\n",
       "   3: [(1.0, np.int64(5), -1, False)]},\n",
       "  7: {0: [(1.0, np.int64(7), -1, False)],\n",
       "   1: [(1.0, np.int64(8), -1, False)],\n",
       "   2: [(1.0, np.int64(19), -1, False)],\n",
       "   3: [(1.0, np.int64(6), -1, False)]},\n",
       "  8: {0: [(1.0, np.int64(8), -1, False)],\n",
       "   1: [(1.0, np.int64(9), -1, False)],\n",
       "   2: [(1.0, np.int64(20), -1, False)],\n",
       "   3: [(1.0, np.int64(7), -1, False)]},\n",
       "  9: {0: [(1.0, np.int64(9), -1, False)],\n",
       "   1: [(1.0, np.int64(10), -1, False)],\n",
       "   2: [(1.0, np.int64(21), -1, False)],\n",
       "   3: [(1.0, np.int64(8), -1, False)]},\n",
       "  10: {0: [(1.0, np.int64(10), -1, False)],\n",
       "   1: [(1.0, np.int64(11), -1, False)],\n",
       "   2: [(1.0, np.int64(22), -1, False)],\n",
       "   3: [(1.0, np.int64(9), -1, False)]},\n",
       "  11: {0: [(1.0, np.int64(11), -1, False)],\n",
       "   1: [(1.0, np.int64(11), -1, False)],\n",
       "   2: [(1.0, np.int64(23), -1, False)],\n",
       "   3: [(1.0, np.int64(10), -1, False)]},\n",
       "  12: {0: [(1.0, np.int64(0), -1, False)],\n",
       "   1: [(1.0, np.int64(13), -1, False)],\n",
       "   2: [(1.0, np.int64(24), -1, False)],\n",
       "   3: [(1.0, np.int64(12), -1, False)]},\n",
       "  13: {0: [(1.0, np.int64(1), -1, False)],\n",
       "   1: [(1.0, np.int64(14), -1, False)],\n",
       "   2: [(1.0, np.int64(25), -1, False)],\n",
       "   3: [(1.0, np.int64(12), -1, False)]},\n",
       "  14: {0: [(1.0, np.int64(2), -1, False)],\n",
       "   1: [(1.0, np.int64(15), -1, False)],\n",
       "   2: [(1.0, np.int64(26), -1, False)],\n",
       "   3: [(1.0, np.int64(13), -1, False)]},\n",
       "  15: {0: [(1.0, np.int64(3), -1, False)],\n",
       "   1: [(1.0, np.int64(16), -1, False)],\n",
       "   2: [(1.0, np.int64(27), -1, False)],\n",
       "   3: [(1.0, np.int64(14), -1, False)]},\n",
       "  16: {0: [(1.0, np.int64(4), -1, False)],\n",
       "   1: [(1.0, np.int64(17), -1, False)],\n",
       "   2: [(1.0, np.int64(28), -1, False)],\n",
       "   3: [(1.0, np.int64(15), -1, False)]},\n",
       "  17: {0: [(1.0, np.int64(5), -1, False)],\n",
       "   1: [(1.0, np.int64(18), -1, False)],\n",
       "   2: [(1.0, np.int64(29), -1, False)],\n",
       "   3: [(1.0, np.int64(16), -1, False)]},\n",
       "  18: {0: [(1.0, np.int64(6), -1, False)],\n",
       "   1: [(1.0, np.int64(19), -1, False)],\n",
       "   2: [(1.0, np.int64(30), -1, False)],\n",
       "   3: [(1.0, np.int64(17), -1, False)]},\n",
       "  19: {0: [(1.0, np.int64(7), -1, False)],\n",
       "   1: [(1.0, np.int64(20), -1, False)],\n",
       "   2: [(1.0, np.int64(31), -1, False)],\n",
       "   3: [(1.0, np.int64(18), -1, False)]},\n",
       "  20: {0: [(1.0, np.int64(8), -1, False)],\n",
       "   1: [(1.0, np.int64(21), -1, False)],\n",
       "   2: [(1.0, np.int64(32), -1, False)],\n",
       "   3: [(1.0, np.int64(19), -1, False)]},\n",
       "  21: {0: [(1.0, np.int64(9), -1, False)],\n",
       "   1: [(1.0, np.int64(22), -1, False)],\n",
       "   2: [(1.0, np.int64(33), -1, False)],\n",
       "   3: [(1.0, np.int64(20), -1, False)]},\n",
       "  22: {0: [(1.0, np.int64(10), -1, False)],\n",
       "   1: [(1.0, np.int64(23), -1, False)],\n",
       "   2: [(1.0, np.int64(34), -1, False)],\n",
       "   3: [(1.0, np.int64(21), -1, False)]},\n",
       "  23: {0: [(1.0, np.int64(11), -1, False)],\n",
       "   1: [(1.0, np.int64(23), -1, False)],\n",
       "   2: [(1.0, np.int64(35), -1, False)],\n",
       "   3: [(1.0, np.int64(22), -1, False)]},\n",
       "  24: {0: [(1.0, np.int64(12), -1, False)],\n",
       "   1: [(1.0, np.int64(25), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -1, False)],\n",
       "   3: [(1.0, np.int64(24), -1, False)]},\n",
       "  25: {0: [(1.0, np.int64(13), -1, False)],\n",
       "   1: [(1.0, np.int64(26), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(24), -1, False)]},\n",
       "  26: {0: [(1.0, np.int64(14), -1, False)],\n",
       "   1: [(1.0, np.int64(27), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(25), -1, False)]},\n",
       "  27: {0: [(1.0, np.int64(15), -1, False)],\n",
       "   1: [(1.0, np.int64(28), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(26), -1, False)]},\n",
       "  28: {0: [(1.0, np.int64(16), -1, False)],\n",
       "   1: [(1.0, np.int64(29), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(27), -1, False)]},\n",
       "  29: {0: [(1.0, np.int64(17), -1, False)],\n",
       "   1: [(1.0, np.int64(30), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(28), -1, False)]},\n",
       "  30: {0: [(1.0, np.int64(18), -1, False)],\n",
       "   1: [(1.0, np.int64(31), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(29), -1, False)]},\n",
       "  31: {0: [(1.0, np.int64(19), -1, False)],\n",
       "   1: [(1.0, np.int64(32), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(30), -1, False)]},\n",
       "  32: {0: [(1.0, np.int64(20), -1, False)],\n",
       "   1: [(1.0, np.int64(33), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(31), -1, False)]},\n",
       "  33: {0: [(1.0, np.int64(21), -1, False)],\n",
       "   1: [(1.0, np.int64(34), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(32), -1, False)]},\n",
       "  34: {0: [(1.0, np.int64(22), -1, False)],\n",
       "   1: [(1.0, np.int64(35), -1, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(33), -1, False)]},\n",
       "  35: {0: [(1.0, np.int64(23), -1, False)],\n",
       "   1: [(1.0, np.int64(35), -1, False)],\n",
       "   2: [(1.0, np.int64(47), -1, True)],\n",
       "   3: [(1.0, np.int64(34), -1, False)]},\n",
       "  36: {0: [(1.0, np.int64(24), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -1, False)],\n",
       "   3: [(1.0, np.int64(36), -1, False)]},\n",
       "  37: {0: [(1.0, np.int64(25), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -1, False)]},\n",
       "  38: {0: [(1.0, np.int64(26), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  39: {0: [(1.0, np.int64(27), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  40: {0: [(1.0, np.int64(28), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  41: {0: [(1.0, np.int64(29), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  42: {0: [(1.0, np.int64(30), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  43: {0: [(1.0, np.int64(31), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  44: {0: [(1.0, np.int64(32), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  45: {0: [(1.0, np.int64(33), -1, False)],\n",
       "   1: [(1.0, np.int64(36), -100, False)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  46: {0: [(1.0, np.int64(34), -1, False)],\n",
       "   1: [(1.0, np.int64(47), -1, True)],\n",
       "   2: [(1.0, np.int64(36), -100, False)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]},\n",
       "  47: {0: [(1.0, np.int64(35), -1, False)],\n",
       "   1: [(1.0, np.int64(47), -1, True)],\n",
       "   2: [(1.0, np.int64(47), -1, True)],\n",
       "   3: [(1.0, np.int64(36), -100, False)]}},\n",
       " 'initial_state_distrib': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'observation_space': Discrete(48),\n",
       " 'action_space': Discrete(4),\n",
       " 'render_mode': None,\n",
       " 'cell_size': (60, 60),\n",
       " 'window_size': (720, 240),\n",
       " 'window_surface': None,\n",
       " 'clock': None,\n",
       " 'elf_images': None,\n",
       " 'start_img': None,\n",
       " 'goal_img': None,\n",
       " 'cliff_img': None,\n",
       " 'mountain_bg_img': None,\n",
       " 'near_cliff_img': None,\n",
       " 'tree_img': None,\n",
       " 'spec': EnvSpec(id='CliffWalking-v0', entry_point='gymnasium.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, disable_env_checker=True, kwargs={}, namespace=None, name='CliffWalking', version=0, additional_wrappers=(), vector_entry_point=None),\n",
       " '_np_random': Generator(PCG64) at 0x1D7C8C67A00,\n",
       " '_np_random_seed': 44,\n",
       " 's': np.int64(36),\n",
       " 'lastaction': None}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def display(data):\n",
    "    figure = plt.figure()\n"
   ],
   "id": "3efb34252f6c865a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:51:28.166540Z",
     "start_time": "2025-06-24T03:51:28.162789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mc_exploring_start(env, train_seed, no_episodes=500000, gamma=1):\n",
    "    env.reset(seed=train_seed)\n",
    "\n",
    "    no_states = env.env.env.observation_space.n\n",
    "    no_actions = env.env.env.action_space.n\n",
    "\n",
    "    policy = np.ones((no_states, no_actions)) / no_actions\n",
    "    #policy[:, 0] = 1 # Only move up.\n",
    "    q = np.zeros(policy.shape)\n",
    "    c = np.zeros(policy.shape)\n",
    "\n",
    "    for _ in tqdm(range(no_episodes)):\n",
    "        # Choose initial state randomly. Ensure it is not on an invalid position\n",
    "        state = np.random.randint(37)\n",
    "        action = np.random.randint(4)\n",
    "\n",
    "        env.reset()\n",
    "        env.unwrapped.s = state\n",
    "\n",
    "        state_actions = []\n",
    "        rewards = []\n",
    "        while True:\n",
    "            state_actions.append((state, action))\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            action = np.random.choice(no_actions, p=policy[state])\n",
    "\n",
    "        g = 0\n",
    "        for (state, action), reward in zip(reversed(state_actions), reversed(rewards)):\n",
    "            g = gamma * g + reward\n",
    "            c[state, action] += 1\n",
    "            q[state, action] += (g - q[state, action]) / c[state, action]\n",
    "\n",
    "            a = q[state].argmax()\n",
    "            #policy[state] = softmax(q[state])\n",
    "            policy[state] = 0\n",
    "            policy[state, a] = 1\n",
    "    return policy, q"
   ],
   "id": "d3022fdd101551bb",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:51:28.660381Z",
     "start_time": "2025-06-24T03:51:28.658267Z"
    }
   },
   "cell_type": "code",
   "source": "directions = [\"↑\", \"→\", \"↓\", \"←\"]",
   "id": "1172fd29422c4ce",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:54:36.139088Z",
     "start_time": "2025-06-24T03:53:22.532782Z"
    }
   },
   "cell_type": "code",
   "source": "policy, q = mc_exploring_start(env, train_seed, no_episodes=600000, gamma=1)",
   "id": "e252c77da537a16c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600000/600000 [01:13<00:00, 8151.86it/s]\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:54:38.106343Z",
     "start_time": "2025-06-24T03:54:38.103248Z"
    }
   },
   "cell_type": "code",
   "source": "[[directions[x] for x in lst] for lst in policy.argmax(axis=1).reshape(4, -1).tolist()]",
   "id": "e34478188adb1d1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['→', '↓', '→', '↓', '→', '↓', '↓', '→', '→', '→', '↓', '↓'],\n",
       " ['↓', '↓', '→', '↓', '→', '→', '→', '→', '→', '→', '→', '↓'],\n",
       " ['→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓'],\n",
       " ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑']]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T03:54:48.918856Z",
     "start_time": "2025-06-24T03:54:48.878365Z"
    }
   },
   "cell_type": "code",
   "source": "test_policy(200, env, policy, test_seed)",
   "id": "f64cee8535ba0313",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-13.0), np.float64(0.0))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T04:46:48.873463Z",
     "start_time": "2025-06-24T04:46:48.828722Z"
    }
   },
   "cell_type": "code",
   "source": "plt.imshow(q.argmax(axis=1).reshape(4,-1), cmap='gray')",
   "id": "61bc0a27180ee36b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d7cfea7b10>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T05:55:51.813865Z",
     "start_time": "2025-06-24T05:55:51.809989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mc_soft_policy(env, train_seed, no_episodes=500000, gamma=1, epsilon=0.1, epsilon_decay=1e-5):\n",
    "    env.reset(seed=train_seed)\n",
    "    no_states = env.env.env.observation_space.n\n",
    "    no_actions = env.env.env.action_space.n\n",
    "    policy = np.ones((no_states, no_actions)) / no_actions\n",
    "    q = np.zeros(policy.shape)\n",
    "    c = np.zeros(policy.shape)\n",
    "    for _ in tqdm(range(no_episodes)):\n",
    "        state_actions = []\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            action = np.random.choice(no_actions, p=policy[state])\n",
    "            state_actions.append((state, action))\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        g = 0\n",
    "        for (state, action), reward in zip(reversed(state_actions), reversed(rewards)):\n",
    "            g = gamma * g + reward\n",
    "            c[state, action] += 1\n",
    "            q[state, action] += (g - q[state, action]) / c[state, action]\n",
    "\n",
    "            # soft update\n",
    "            a = q[state].argmax()\n",
    "            policy[state] = epsilon / no_actions\n",
    "            policy[state, a] += (1 - epsilon)\n",
    "        epsilon = max(epsilon - epsilon_decay, 0.1)\n",
    "    return policy, q"
   ],
   "id": "8ce1d16a6e134375",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T09:15:40.921566Z",
     "start_time": "2025-06-24T09:15:40.917566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mc_soft_policy(env, train_seed, no_episodes=500000, gamma=1, epsilon=0.1, epsilon_decay=1e-5):\n",
    "    env.reset(seed=train_seed)\n",
    "    no_states = env.env.env.observation_space.n\n",
    "    no_actions = env.env.env.action_space.n\n",
    "    policy = np.ones((no_states, no_actions)) / no_actions\n",
    "    q = np.zeros(policy.shape)\n",
    "    c = np.zeros(policy.shape)\n",
    "\n",
    "    for _ in tqdm(range(no_episodes)):\n",
    "        state_actions = []\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "        # Generate an episode\n",
    "        while True:\n",
    "            action = np.random.choice(no_actions, p=policy[state])\n",
    "            state_actions.append((state, action))\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Calculate returns and update Q-values\n",
    "        g = 0\n",
    "        # Track visited states for policy update\n",
    "        visited_states = set()\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            state, action = state_actions[i]\n",
    "            g = gamma * g + rewards[i]\n",
    "            c[state, action] += 1\n",
    "            q[state, action] += (g - q[state, action]) / c[state, action]\n",
    "            visited_states.add(state)  # Mark state for policy update\n",
    "\n",
    "        # Update policy for all states visited in this episode\n",
    "        for state in visited_states:\n",
    "            best_action = np.argmax(q[state])\n",
    "            policy[state] = epsilon / no_actions\n",
    "            policy[state, best_action] += 1 - epsilon  # Ensure probabilities sum to 1\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon - epsilon_decay, 0.1)\n",
    "\n",
    "    return policy, q"
   ],
   "id": "e57725062bb21d7c",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T09:47:40.746529Z",
     "start_time": "2025-06-24T09:47:40.743013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mc_soft_policy(env, train_seed, no_episodes=500000, gamma=1.0, epsilon=1.0, epsilon_min=0.1, epsilon_decay=1e-6):\n",
    "    # Set seed for reproducibility\n",
    "    env.reset(seed=train_seed)\n",
    "\n",
    "    no_states = env.observation_space.n\n",
    "    no_actions = env.action_space.n\n",
    "\n",
    "    # Initialize policy and Q-table\n",
    "    policy = np.ones((no_states, no_actions)) / no_actions\n",
    "    q = -1 * np.ones((no_states, no_actions))  # Optimistic initialization\n",
    "    c = np.zeros((no_states, no_actions))      # Visit counts for averaging\n",
    "\n",
    "    for _ in tqdm(range(no_episodes)):\n",
    "        state, _ = env.reset()\n",
    "        episode = []\n",
    "        done = False\n",
    "\n",
    "        # Generate one episode\n",
    "        while not done:\n",
    "            action = np.random.choice(no_actions, p=policy[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Calculate returns and first-visit updates\n",
    "        g = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            g = gamma * g + reward\n",
    "\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                c[state, action] += 1\n",
    "                q[state, action] += (g - q[state, action]) / c[state, action]\n",
    "\n",
    "                # Soft policy improvement\n",
    "                best_action = np.argmax(q[state])\n",
    "                policy[state] = epsilon / no_actions\n",
    "                policy[state, best_action] += (1.0 - epsilon)\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon - epsilon_decay, epsilon_min)\n",
    "\n",
    "    return policy, q"
   ],
   "id": "4f8a484fb2e1a2b3",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T09:54:14.902614Z",
     "start_time": "2025-06-24T09:47:47.819851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# I tried really hard to make this soft policy algorithm work, but it seems that it cannot effectively solve the cliffwalking environment.\n",
    "env = gym.make('CliffWalking-v0')\n",
    "policy_soft, q_soft = mc_soft_policy(env, train_seed)"
   ],
   "id": "1dc97a92f4c11550",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5713/500000 [06:27<9:18:06, 14.76it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[120]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m env = gym.make(\u001B[33m'\u001B[39m\u001B[33mCliffWalking-v0\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m policy_soft, q_soft = \u001B[43mmc_soft_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_seed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[119]\u001B[39m\u001B[32m, line 20\u001B[39m, in \u001B[36mmc_soft_policy\u001B[39m\u001B[34m(env, train_seed, no_episodes, gamma, epsilon, epsilon_min, epsilon_decay)\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Generate one episode\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m     action = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrandom\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mno_actions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m     next_state, reward, terminated, truncated, _ = env.step(action)\n\u001B[32m     22\u001B[39m     episode.append((state, action, reward))\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T06:22:11.195864Z",
     "start_time": "2025-06-24T06:22:11.190948Z"
    }
   },
   "cell_type": "code",
   "source": "[[directions[x] for x in lst] for lst in policy_soft.argmax(axis=1).reshape(4, -1).tolist()]",
   "id": "6927e5ccd6a7c0b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['↑', '→', '→', '↑', '→', '→', '→', '→', '→', '→', '→', '↓'],\n",
       " ['→', '→', '→', '→', '→', '→', '↑', '→', '→', '→', '→', '↓'],\n",
       " ['←', '↑', '↓', '↑', '←', '↑', '→', '↑', '↑', '↑', '→', '↓'],\n",
       " ['←', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑']]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T06:16:40.681550Z",
     "start_time": "2025-06-24T06:16:40.440080Z"
    }
   },
   "cell_type": "code",
   "source": "test_policy(200, env, policy_soft, test_seed)",
   "id": "229c2fed335ce9b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-227.78), np.float64(236.61111892723892))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:39:21.329278Z",
     "start_time": "2025-06-24T12:39:21.325538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mc_importance_sampling(env, train_seed, gamma=1, no_episodes=500000):\n",
    "    state, _ = env.reset(seed=train_seed)\n",
    "    no_states = int(env.env.env.observation_space.n)\n",
    "    no_actions = int(env.env.env.action_space.n)\n",
    "\n",
    "    policy = np.zeros((no_states, no_actions))\n",
    "    policy[:, 0] = 1 # only go up.\n",
    "    behaviour_policy = np.ones(policy.shape) / no_actions\n",
    "\n",
    "    q = np.zeros(policy.shape)\n",
    "    c = np.zeros(policy.shape)\n",
    "    for _ in tqdm(range(no_episodes)):\n",
    "        state_actions = []\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.random.choice(no_actions, p=behaviour_policy[state])\n",
    "            state_actions.append((state, action))\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        g = 0\n",
    "        rho = 1\n",
    "        for (state, action), reward in zip(reversed(state_actions), reversed(rewards)):\n",
    "            g = gamma * g + reward\n",
    "            c[state, action] += rho\n",
    "            q[state, action] += rho / c[state, action] * (g - q[state, action])\n",
    "\n",
    "            a = q[state].argmax()\n",
    "            policy[state] = 0\n",
    "            policy[state, a] = 1\n",
    "            if a != action: break # early stop.\n",
    "            rho /= behaviour_policy[state, action]\n",
    "    return policy, q"
   ],
   "id": "556f35533fc44379",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:48:50.013781Z",
     "start_time": "2025-06-24T12:43:19.761728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = TimeLimit(gym.make('CliffWalking-v0'), max_episode_steps=60)\n",
    "policy_imp, q_imp = mc_importance_sampling(env, train_seed)"
   ],
   "id": "e92e5e0e0dbedf0c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [05:30<00:00, 1514.02it/s]\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:48:51.095158Z",
     "start_time": "2025-06-24T12:48:51.091804Z"
    }
   },
   "cell_type": "code",
   "source": "policy_imp.argmax(axis=1).reshape(4,-1)",
   "id": "9573c7e3020f388",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 0, 1],\n",
       "       [0, 3, 0, 1, 1, 1, 1, 1, 0, 2, 0, 2],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 2],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:49:18.511518Z",
     "start_time": "2025-06-24T12:49:18.373759Z"
    }
   },
   "cell_type": "code",
   "source": "test_policy(200, env, policy_imp, test_seed)",
   "id": "f38aa7f9bb43a283",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-60.0), np.float64(0.0))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9def0e3336c839e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
