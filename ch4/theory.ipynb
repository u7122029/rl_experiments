{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Monte Carlo Learning\n",
    "The previous chapters did not cover real ML methods. This was mainly due to the algorithms having full knowledge of the dynamics of the system. ML algorithms instead learn from interactions.\n",
    "\n",
    "Model-free learning can be further classified as:\n",
    "- Monte Carlo (MC) learning, and\n",
    "- Temporal Difference (TD) learning."
   ],
   "id": "84078b28c624488"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "At end of episode, MC learning estimates values of policy using samples collected during the episodes.\n",
    "\n",
    "Consequently, ML learning can only be used in episodic tasks, since sequential tasks never end."
   ],
   "id": "37dc733fff23a51a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## On-Policy MC Learning\n",
    "### Policy Evaluation\n",
    "Key ideas:\n",
    "- State and action values are expectations of returns on conditions of states and state-action pairs respectively.\n",
    "- Can use MC method to estimate expectation.\n",
    "\n",
    "For example, among $c$ trajectories that have visited a given state/state-action pair, they have returns $g_1, g_2, \\dots, g_c$.\n",
    "\n",
    "MC method estimates the state/action value as\n",
    "$$\\frac{1}{c}\\sum_{i = 1}^cg_i$$"
   ],
   "id": "4770e4fdf75282b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Let return samples be $g_1, g_2, \\dots, g_{c-1}, g_c$.\n",
    "2. $\\overline{g}_{c-1} = \\frac{1}{c - 1}\\sum_{i = 1}^\\infty g_i$.\n",
    "3. Can prove that $\\overline{g}_c = \\overline{g}_{c-1} + \\frac{1}{c}(g_c - \\overline{g}_{c-1})$.\n",
    "\n",
    "This is a space saving method to calculate each average incremental return."
   ],
   "id": "8626274c4f8fc344"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Robbins-Monro Algorithm\n",
    "Attempts to find root of equation $f(x) = 0$ with limitation that we can only obtain the measurements of the random functions $F(x)$, where $f(x) = \\mathbb{E}[F(x)]$.\n",
    "\n",
    "Problem is solved by iteratively using\n",
    "$$X_k = X_{k-1} - \\alpha_kF(X_{k-1})$$\n",
    "$\\{\\alpha_k\\}_{k\\geq 1}$ is a learning rate sequence with following conditions:\n",
    "1. (non-negative) $\\alpha_k \\geq 0$ for all $k$.\n",
    "2. (diverges regardless of start point) $\\sum_{k = 1}^{\\infty}\\alpha_k = \\infty$.\n",
    "3. (diverges regardless of noise) $\\sum_{k = 1}^\\infty \\alpha_k^2 = \\infty$.\n",
    "\n",
    "From this, the iteration converges to a solution under some condition."
   ],
   "id": "8a9de05fdd59ca35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Implementation of Algorithm\n",
    "Consider estimating action values. Let $F(q) = G - q$, where $q$ is the value to estimate.\n",
    "\n",
    "Observe many samples of returns, and update $q$ using\n",
    "$$q_k \\leftarrow q_{k-1} + \\alpha_k(g_k - q_{k-1})$$"
   ],
   "id": "cc2b6905f836c953"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$q_0$ arbitrary initial value, $\\alpha_k = 1/k$ sequence of learning rates.\n",
    "\n",
    "After convergence, we have $\\mathbb{E}[F(q(s,a))] = \\mathbb{E}[G_t\\mid S_t = s, A_t = a] - q(s,a) = 0$.\n",
    "\n",
    "Can analyse estimations of state values similarly by letting $F(v) = G - v$."
   ],
   "id": "deaccef4fc6fbe77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Policy evaluation can directly estimate state values or directly estimate action values.\n",
    "\n",
    "With Bellman expectation equations, can:\n",
    "1. Use state values to back up action values with dynamics $p$, or\n",
    "2. Use action values to back up state values with knowledge of policy $\\pi$.\n",
    "\n",
    "Unfortunately, $p$ is unknown in model-free learning, so we can only use action values to back up state values."
   ],
   "id": "d414c6f631b08157"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Every-visit MC update uses all return samples to update value estimations.\n",
    "- First-visit MC update only uses sample when state (or state-action pair) is first visited.\n",
    "\n",
    "Both techniques converge to true value, one way or another."
   ],
   "id": "97bced2bfcd0b691"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.1: Evaluation of action values using Every-Visit MC Policy Evaluation\n",
    "Inputs: env (without model), policy $\\pi$.\n",
    "\n",
    "Output: action value estimates $q(s,a)$\n",
    "\n",
    "1. (Initialise) Set $q(s,a)$ arbitrarily. If using incremental implementation, set $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use policy $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. (Initialise return) $G\\leftarrow 0$.\n",
    "    3. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update action value) Update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$. For incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t, A_t)}[G - q(S_t, A_t)]$."
   ],
   "id": "2974391302ce4761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.2: Every-visit MC update to evaluate state values.\n",
    "1. (Initialise) Initialise $v(s)$ arbitrarily. If using incremental implemenation, initialise $c(s) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. (Initialise return) $G \\leftarrow 0$.\n",
    "    3. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update action value) Update $v(S_t)$ to reduce $[G - v(S_t)]^2$. For incremental implementation, perform the following:\n",
    "            1. $c(S_t) \\leftarrow c(S_t) + 1$.\n",
    "            2. $v(S_t) \\leftarrow v(S_t) + \\frac{1}{c(S_t)}[G - v(S_t)]$."
   ],
   "id": "184990196fb378e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.3: First-visit MC update to estimate action values.\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily. If using incremental implementation, set $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. (Initialise return) $G \\leftarrow 0$.\n",
    "    3. (Calculate steps that state-action pairs are first visited within episode) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. If $f(S_t, A_t) < 0$, then $f(S_t, A_t) \\leftarrow t$.\n",
    "    4. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update when first visited) If $f(S_t, A_t) = t$, update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$. For incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t, A_t)}[G - q(S_t, A_t)]$"
   ],
   "id": "79aabb89e4133688"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.4: First-visit MC update to estimate state values.\n",
    "1. (Initialise) Initialise $v(s)$ arbitrarily. If using incremental implementation, set $c(s) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. (Initialise return) $G \\leftarrow 0$.\n",
    "    3. (Calculate steps that state-action pairs are first visited within episode) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. If $f(S_t) < 0$, then $f(S_t) \\leftarrow t$.\n",
    "    4. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update when first visited) If $f(S_t) = t$, update $v(S_t)$ to reduce $[G - v(S_t)]^2$. For incremental implementation, perform the following:\n",
    "            1. $c(S_t) \\leftarrow c(S_t) + 1$.\n",
    "            2. $v(S_t) \\leftarrow v(S_t) + \\frac{1}{c(S_t)}[G - v(S_t)]$."
   ],
   "id": "c92edf6b4e9a358b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MC Learning with Exploration Start\n",
    "Introduce MC update algorithms to find optimal policy.\n",
    "\n",
    "Up to now, we know how to evaluate action values using MC updates. Upon getting these estimates, can improve the policy, and get a new one.\n",
    "\n",
    "Repetition of estimation and improvement may lead to optimality.\n",
    "\n",
    "Unfortunately, not all start states lead to optimality due to initial bad policy which gets us stuck in bad states, and induce bad update values for those states."
   ],
   "id": "dd6a1184ae33c8cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exploring start changes initial state dist so that episode can start with any state-action pair.\n",
    "### Algorithm 4.5: MC update with exploring state (maintaining policy explicitly)\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily. If using incremental implementation, set $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) for each episode:\n",
    "    1. (Initialise episode start) Choose a $S_0, A_0$ pair as start. Any one can be chosen.\n",
    "    2. (Sample trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    3. If using first-visit version, perform the following:\n",
    "        1. $f(s,a) \\leftarrow -1$ for all $s,a$.\n",
    "        2. For each $t\\leftarrow 0, 1, \\dots, T-1$: If $f(S_t, A_t) < 0$ then $f(S_t, A_t) \\leftarrow t$.\n",
    "    4. (Initialise return) $G \\leftarrow 0$.\n",
    "    5. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Upd. act-val estim.) Update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$. If using incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t, A_t)}[G - q(S_t, A_t)]$.\n",
    "            3. If using first-visit version, update counter and action estimates only when $f(S_t, A_t) = t$.\n",
    "            4. (Improve Policy) $\\pi(S_t) \\leftarrow \\arg\\max_a q(S_t, A_t) = t$."
   ],
   "id": "4a0fe9d60563f367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.6: MC update with exploring start (maintaining policy implicitly)\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily. If using incremental implementation, initialise $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Initialise episode start) Choose $(S_0, A_0)$ pair randomly.\n",
    "    2. (Sample trajectory) Starting from $(S_0, A_0)$, use policy derived from action values $q$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$ (choose action that maximises action value).\n",
    "    3. If using first visit version:\n",
    "        1. $f(s,a) \\leftarrow -1$ for all $s, a$.\n",
    "        2. For each $t \\leftarrow 0, 1, \\dots, T-1$: if $f(S_t, A_t) < 0$, then $f(S_t, A_t) \\leftarrow t$.\n",
    "    4. (Initialise return) $G \\leftarrow 0$.\n",
    "    5. (Update) $For t\\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G\\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Upd. act-val estim.) Update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$.\n",
    "        3. If using incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t,A_t)}[G - q(S_t, A_t)]$\n",
    "            3. For first-visit version, perform the above only if $f(S_t, A_t) = t$."
   ],
   "id": "f17c3a043c051986"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MC Learning on Soft Policy\n",
    "Can explore without exploring start.\n",
    "\n",
    "### What is a Soft Policy?\n",
    "$\\pi$ is a soft policy iff $\\pi(a\\mid s) > 0$ holds for every $s, a$. It can thereby choose all possible actions.\n",
    "\n",
    "Soft policies can help explore more states and state-action pairs.\n",
    "\n",
    "### $\\epsilon$-Soft Policies\n",
    "$\\pi$ is $\\epsilon$-soft iff exists $\\epsilon > 0$ s.t $\\pi(a\\mid s) > \\epsilon / |A(s)|$ for all $s, a$.\n",
    "\n",
    "All $\\epsilon$-soft policies are soft policies.\n",
    "\n",
    "### $\\epsilon$-Greedy Policies\n",
    "$\\epsilon$-soft policiy that is the closes to the deterministic policy is called an $\\epsilon$-greedy policy of the deterministic policy.\n",
    "\n",
    "If the deterministic policy is as shown below:\n",
    "$$\n",
    "\\pi(a\\mid s) = \\begin{cases}\n",
    "1 & s \\in S, a = a^*\\\\\n",
    "0 & s \\in A, a \\neq a^*\n",
    "\\end{cases}\n",
    "$$\n",
    "Then the $\\epsilon$-soft policy will appear as follows:\n",
    "$$\n",
    "\\pi(a\\mid s) = \\begin{cases}\n",
    "1 - \\epsilon - \\frac{\\epsilon}{|A(s)|} & s \\in S, a = a^*\\\\\n",
    "\\frac{\\epsilon}{|A(s)|} & s \\in S, a \\neq a^*\n",
    "\\end{cases}\n",
    "$$\n",
    "This policy assigns probability $\\epsilon$ equally to all actions, and assigns the remaining $(1-\\epsilon)$ to the greedy, exploitative action $a^*$.\n",
    "\n",
    "MC update with soft policy uses $\\epsilon$-soft policy during iterations. Particularly, the policy improvement updates an old $\\epsilon$-soft policy to a new $\\epsilon$-greedy policy, which can be explained by the policy improvement theorem, too. In other words, if $\\pi$ is an $\\epsilon$-soft policy, and $\\pi'$ is an $\\epsilon$-greedy policy, then we have $\\pi \\preccurlyeq \\pi'$, which means for any $s \\in S$:\n",
    "$$\\sum_{a}\\pi'(a\\mid s)q_\\pi(s,a) \\geq v_\\pi(s)$$"
   ],
   "id": "24764fa793667720"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Algorithm 4.7: MC Update with Soft Policy (maintaining policy explicitly)",
   "id": "41e23fe7c2fc1d0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad9223cdc4408fca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
