{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Monte Carlo Learning\n",
    "The previous chapters did not cover real ML methods. This was mainly due to the algorithms having full knowledge of the dynamics of the system. ML algorithms instead learn from interactions.\n",
    "\n",
    "Model-free learning can be further classified as:\n",
    "- Monte Carlo (MC) learning, and\n",
    "- Temporal Difference (TD) learning."
   ],
   "id": "84078b28c624488"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "At end of episode, MC learning estimates values of policy using samples collected during the episodes.\n",
    "\n",
    "Consequently, ML learning can only be used in episodic tasks, since sequential tasks never end."
   ],
   "id": "37dc733fff23a51a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## On-Policy MC Learning\n",
    "### Policy Evaluation\n",
    "Key ideas:\n",
    "- State and action values are expectations of returns on conditions of states and state-action pairs respectively.\n",
    "- Can use MC method to estimate expectation.\n",
    "\n",
    "For example, among $c$ trajectories that have visited a given state/state-action pair, they have returns $g_1, g_2, \\dots, g_c$.\n",
    "\n",
    "MC method estimates the state/action value as\n",
    "$$\\frac{1}{c}\\sum_{i = 1}^cg_i$$"
   ],
   "id": "4770e4fdf75282b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Let return samples be $g_1, g_2, \\dots, g_{c-1}, g_c$.\n",
    "2. $\\overline{g}_{c-1} = \\frac{1}{c - 1}\\sum_{i = 1}^\\infty g_i$.\n",
    "3. Can prove that $\\overline{g}_c = \\overline{g}_{c-1} + \\frac{1}{c}(g_c - \\overline{g}_{c-1})$.\n",
    "\n",
    "This is a space saving method to calculate each average incremental return."
   ],
   "id": "8626274c4f8fc344"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Robbins-Monro Algorithm\n",
    "Attempts to find root of equation $f(x) = 0$ with limitation that we can only obtain the measurements of the random functions $F(x)$, where $f(x) = \\mathbb{E}[F(x)]$.\n",
    "\n",
    "Problem is solved by iteratively using\n",
    "$$X_k = X_{k-1} - \\alpha_kF(X_{k-1})$$\n",
    "$\\{\\alpha_k\\}_{k\\geq 1}$ is a learning rate sequence with following conditions:\n",
    "1. (non-negative) $\\alpha_k \\geq 0$ for all $k$.\n",
    "2. (diverges regardless of start point) $\\sum_{k = 1}^{\\infty}\\alpha_k = \\infty$.\n",
    "3. (diverges regardless of noise) $\\sum_{k = 1}^\\infty \\alpha_k^2 = \\infty$.\n",
    "\n",
    "From this, the iteration converges to a solution under some condition."
   ],
   "id": "8a9de05fdd59ca35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Implementation of Algorithm\n",
    "Consider estimating action values. Let $F(q) = G - q$, where $q$ is the value to estimate.\n",
    "\n",
    "Observe many samples of returns, and update $q$ using\n",
    "$$q_k \\leftarrow q_{k-1} + \\alpha_k(g_k - q_{k-1})$$"
   ],
   "id": "cc2b6905f836c953"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$q_0$ arbitrary initial value, $\\alpha_k = 1/k$ sequence of learning rates.\n",
    "\n",
    "After convergence, we have $\\mathbb{E}[F(q(s,a))] = \\mathbb{E}[G_t\\mid S_t = s, A_t = a] - q(s,a) = 0$.\n",
    "\n",
    "Can analyse estimations of state values similarly by letting $F(v) = G - v$."
   ],
   "id": "deaccef4fc6fbe77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Policy evaluation can directly estimate state values or directly estimate action values.\n",
    "\n",
    "With Bellman expectation equations, can:\n",
    "1. Use state values to back up action values with dynamics $p$, or\n",
    "2. Use action values to back up state values with knowledge of policy $\\pi$.\n",
    "\n",
    "Unfortunately, $p$ is unknown in model-free learning, so we can only use action values to back up state values."
   ],
   "id": "d414c6f631b08157"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Every-visit MC update uses all return samples to update value estimations.\n",
    "- First-visit MC update only uses sample when state (or state-action pair) is first visited.\n",
    "\n",
    "Both techniques converge to true value, one way or another."
   ],
   "id": "97bced2bfcd0b691"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.1: Evaluation of action values using Every-Visit MC Policy Evaluation\n",
    "Inputs: env (without model), policy $\\pi$.\n",
    "\n",
    "Output: action value estimates $q(s,a)$\n",
    "\n",
    "1. (Initialise) Set $q(s,a)$ arbitrarily. If using incremental implementation, set $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use policy $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. (Initialise return) $G\\leftarrow 0$.\n",
    "    3. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update action value) Update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$. For incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t, A_t)}[G - q(S_t, A_t)]$."
   ],
   "id": "2974391302ce4761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.2: Every-visit MC update to evaluate state values.\n",
    "1. (Initialise) Initialise $v(s)$ arbitrarily. If using incremental implemenation, initialise $c(s) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. (Initialise return) $G \\leftarrow 0$.\n",
    "    3. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update action value) Update $v(S_t)$ to reduce $[G - v(S_t)]^2$. For incremental implementation, perform the following:\n",
    "            1. $c(S_t) \\leftarrow c(S_t) + 1$.\n",
    "            2. $v(S_t) \\leftarrow v(S_t) + \\frac{1}{c(S_t)}[G - v(S_t)]$."
   ],
   "id": "184990196fb378e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.3: First-visit MC update to estimate action values.\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily. If using incremental implementation, set $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. (Initialise return) $G \\leftarrow 0$.\n",
    "    3. (Calculate steps that state-action pairs are first visited within episode) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. If $f(S_t, A_t) < 0$, then $f(S_t, A_t) \\leftarrow t$.\n",
    "    4. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update when first visited) If $f(S_t, A_t) = t$, update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$. For incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t, A_t)}[G - q(S_t, A_t)]$"
   ],
   "id": "79aabb89e4133688"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.4: First-visit MC update to estimate state values.\n",
    "1. (Initialise) Initialise $v(s)$ arbitrarily. If using incremental implementation, set $c(s) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. (Initialise return) $G \\leftarrow 0$.\n",
    "    3. (Calculate steps that state-action pairs are first visited within episode) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. If $f(S_t) < 0$, then $f(S_t) \\leftarrow t$.\n",
    "    4. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update when first visited) If $f(S_t) = t$, update $v(S_t)$ to reduce $[G - v(S_t)]^2$. For incremental implementation, perform the following:\n",
    "            1. $c(S_t) \\leftarrow c(S_t) + 1$.\n",
    "            2. $v(S_t) \\leftarrow v(S_t) + \\frac{1}{c(S_t)}[G - v(S_t)]$."
   ],
   "id": "c92edf6b4e9a358b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MC Learning with Exploration Start\n",
    "Introduce MC update algorithms to find optimal policy.\n",
    "\n",
    "Up to now, we know how to evaluate action values using MC updates. Upon getting these estimates, can improve the policy, and get a new one.\n",
    "\n",
    "Repetition of estimation and improvement may lead to optimality.\n",
    "\n",
    "Unfortunately, not all start states lead to optimality due to initial bad policy which gets us stuck in bad states, and induce bad update values for those states."
   ],
   "id": "dd6a1184ae33c8cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exploring start changes initial state dist so that episode can start with any state-action pair.\n",
    "### Algorithm 4.5: MC update with exploring state (maintaining policy explicitly)\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily. If using incremental implementation, set $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) for each episode:\n",
    "    1. (Initialise episode start) Choose a $S_0, A_0$ pair as start. Any one can be chosen.\n",
    "    2. (Sample trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    3. If using first-visit version, perform the following:\n",
    "        1. $f(s,a) \\leftarrow -1$ for all $s,a$.\n",
    "        2. For each $t\\leftarrow 0, 1, \\dots, T-1$: If $f(S_t, A_t) < 0$ then $f(S_t, A_t) \\leftarrow t$.\n",
    "    4. (Initialise return) $G \\leftarrow 0$.\n",
    "    5. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Upd. act-val estim.) Update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$. If using incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t, A_t)}[G - q(S_t, A_t)]$.\n",
    "            3. If using first-visit version, update counter and action estimates only when $f(S_t, A_t) = t$.\n",
    "            4. (Improve Policy) $\\pi(S_t) \\leftarrow \\arg\\max_a q(S_t, A_t) = t$."
   ],
   "id": "4a0fe9d60563f367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.6: MC update with exploring start (maintaining policy implicitly)\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily. If using incremental implementation, initialise $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Initialise episode start) Choose $(S_0, A_0)$ pair randomly.\n",
    "    2. (Sample trajectory) Starting from $(S_0, A_0)$, use policy derived from action values $q$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$ (choose action that maximises action value).\n",
    "    3. If using first visit version:\n",
    "        1. $f(s,a) \\leftarrow -1$ for all $s, a$.\n",
    "        2. For each $t \\leftarrow 0, 1, \\dots, T-1$: if $f(S_t, A_t) < 0$, then $f(S_t, A_t) \\leftarrow t$.\n",
    "    4. (Initialise return) $G \\leftarrow 0$.\n",
    "    5. (Update) $For t\\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G\\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Upd. act-val estim.) Update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$.\n",
    "        3. If using incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t,A_t)}[G - q(S_t, A_t)]$\n",
    "            3. For first-visit version, perform the above only if $f(S_t, A_t) = t$."
   ],
   "id": "f17c3a043c051986"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MC Learning on Soft Policy\n",
    "Can explore without exploring start.\n",
    "\n",
    "### What is a Soft Policy?\n",
    "$\\pi$ is a soft policy iff $\\pi(a\\mid s) > 0$ holds for every $s, a$. It can thereby choose all possible actions.\n",
    "\n",
    "Soft policies can help explore more states and state-action pairs.\n",
    "\n",
    "### $\\epsilon$-Soft Policies\n",
    "$\\pi$ is $\\epsilon$-soft iff exists $\\epsilon > 0$ s.t $\\pi(a\\mid s) > \\epsilon / |A(s)|$ for all $s, a$.\n",
    "\n",
    "All $\\epsilon$-soft policies are soft policies.\n",
    "\n",
    "### $\\epsilon$-Greedy Policies\n",
    "$\\epsilon$-soft policiy that is the closes to the deterministic policy is called an $\\epsilon$-greedy policy of the deterministic policy.\n",
    "\n",
    "If the deterministic policy is as shown below:\n",
    "$$\n",
    "\\pi(a\\mid s) = \\begin{cases}\n",
    "1 & s \\in S, a = a^*\\\\\n",
    "0 & s \\in A, a \\neq a^*\n",
    "\\end{cases}\n",
    "$$\n",
    "Then the $\\epsilon$-soft policy will appear as follows:\n",
    "$$\n",
    "\\pi(a\\mid s) = \\begin{cases}\n",
    "1 - \\epsilon - \\frac{\\epsilon}{|A(s)|} & s \\in S, a = a^*\\\\\n",
    "\\frac{\\epsilon}{|A(s)|} & s \\in S, a \\neq a^*\n",
    "\\end{cases}\n",
    "$$\n",
    "This policy assigns probability $\\epsilon$ equally to all actions, and assigns the remaining $(1-\\epsilon)$ to the greedy, exploitative action $a^*$.\n",
    "\n",
    "MC update with soft policy uses $\\epsilon$-soft policy during iterations. Particularly, the policy improvement updates an old $\\epsilon$-soft policy to a new $\\epsilon$-greedy policy, which can be explained by the policy improvement theorem, too. In other words, if $\\pi$ is an $\\epsilon$-soft policy, and $\\pi'$ is an $\\epsilon$-greedy policy, then we have $\\pi \\preccurlyeq \\pi'$, which means for any $s \\in S$:\n",
    "$$\\sum_{a}\\pi'(a\\mid s)q_\\pi(s,a) \\geq v_\\pi(s)$$"
   ],
   "id": "24764fa793667720"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.7: MC Update with Soft Policy (maintaining policy explicitly)\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily. If we use incremental implementation, $c(s,a) \\leftarrow 0$.\n",
    "2. Set $\\pi$ to arbitrary $\\epsilon$-soft policy.\n",
    "3. (MC update) For each episode:\n",
    "    1. (Sample Trajectory) Use $\\pi$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    2. If using first visit version, perform the following when a state-action pair is first visited:\n",
    "        1. $f(s,a)\\leftarrow -1$ for all $s,a$.\n",
    "        2. For every $t\\leftarrow 0, 1, \\dots, T-1$: if $f(S_t, A_t) < 0$, then set $f(S_t, A_t) \\leftarrow t$.\n",
    "    3. (Initialise return) $G \\leftarrow 0$.\n",
    "    4. (Update) For $t\\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G\\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Upd. act-val estimate) Update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$.\n",
    "        3. If using incremental implementation:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t, A_t)}[G - q(S_t, A_t)]$.\n",
    "            3. If first-visit version being usd, update $c$ and $q$ only when $f(S_t, A_t) = t$.\n",
    "            4. (Improve policy) $A^* \\leftarrow \\arg\\max_a q(S_t, a)$.\n",
    "            5. Update $\\pi(\\cdot, S_t)$ to $\\epsilon$-greedy policy of deterministic policy $\\pi(a\\mid S_t) = 0$ ($a \\neq A^*$)"
   ],
   "id": "41e23fe7c2fc1d0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Algorithm 4.8: MC Update with Soft Policy (maintaining policy implicitly)\n",
    "1. (Initialise) Initialise action value estimates $q(s,a)$ arbitrarily. If using incremental implementation, initialise $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Sample trajectory) Use $\\epsilon$-greedy policy derived from $q$ to generate trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T - 1}, A_{T-1}, R_T, S_T$.\n",
    "    2. If using first-visit version, find when state-action pair is first visited:\n",
    "        1. $f(s,a) \\leftarrow -1$ for each $s, a$.\n",
    "        2. For each $t\\leftarrow 0, 1, \\dots, T-1$, if $f(S_t, A_t) < 0$, then set $f(S_t, A_t) \\leftarrow t$.\n",
    "    1. (Initialise return) $G\\leftarrow 0$.\n",
    "    2. (Update) For $t \\leftarrow T - 1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G \\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Update action-value estimate) Update $q(S_t, A_t)$ to reduce $[G - q(S_t, A_t)]^2$.\n",
    "        3. If using incremental implementation:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + 1$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{1}{c(S_t, A_t)}[G - q(S_t, A_t)]$.\n",
    "            3. If using first-visit version, update $c$ and $q$ only when $f(S_t, A_t) = t$."
   ],
   "id": "c3f8043d7a98935a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Off-Policy MC Learning\n",
    "In off-policy algorithm, policy that is updated and policy that generates samples can be different.\n",
    "\n",
    "Will use Importance sampling to evaluate policy and find an optimal one.\n",
    "\n",
    "### Importance Sampling\n",
    "Sometimes when we want to estimate state value of particular state, that state can be very difficult to reach with some given policy.\n",
    "\n",
    "There are some samples that can be used to estimate $q$ for that state, making the variance of the estimate very large.\n",
    "\n",
    "Importance Sampling considers using another policy to generate samples to visit the state more frequently, making the samples more efficiently used.\n",
    "\n",
    "Importance sampling is a technique to rewduce the variance in MC algorithm.\n",
    "- Changes sampling probability distributions so that the sampling can be more efficient.\n",
    "- Ratio of new probability to old probability is called **importance sampling ratio**.\n",
    "\n",
    "We consider off-policy RL using importance sampling.\n",
    "- Target policy $\\pi$ is policy to update.\n",
    "- Policy to generate samples is behaviour policy $b$.\n",
    "\n",
    "Use $b$ to generate samples, use the samples to update statistics about target policy.\n",
    "\n",
    "Given state $S_t$ at time $t$, can generate trajectory\n",
    "$$S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$$\n",
    "using either $\\pi$ or $b$. The probabilities of the given trajectory generated are as follows:\n",
    "\\begin{align*}\n",
    "\\text{Pr}_x[A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\dots, S_{T-1}, A_{T-1}, R_T, S_T\\mid S_t] &= x(A_t\\mid S_t)p(S_{t+1}, R_{t+1}\\mid S_t, A_t)x(A_{t+1}\\mid S_{t+1})\\dots p(S_T, R_T\\mid S_{T-1}, A_{T-1})\\\\\n",
    "&= \\prod_{\\tau = t}^{T-1}x(A_\\tau\\mid S_\\tau)\\prod_{\\tau = t}^{T-1}p(S_{\\tau+1}, R_{\\tau+1}\\mid S_\\tau, A_\\tau)\n",
    "\\end{align*}\n",
    "Where $x$ is either $\\pi$ or $b$."
   ],
   "id": "86fb88996b15819f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The ratio of the two probabilities is the importance sample ratio:\n",
    "$$\\rho_{t:T-1} = \\frac{\\text{Pr}_\\pi[A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\dots, S_{T-1}, A_{T-1}, R_T, S_T\\mid S_t]}{\\text{Pr}_b[A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\dots, S_{T-1}, A_{T-1}, R_T, S_T\\mid S_t]} = \\prod_{\\tau = t}^{T-1}\\frac{\\pi(A_\\tau\\mid S_\\tau)}{b(A_\\tau\\mid S_\\tau)}$$\n",
    "Notice that the simplification shows the ratio depends on the policies only, not the dynamics.\n",
    "\n",
    "To make the ratio well-defined, require $\\pi$ absolutely continuous w.r.t $b$ ($\\pi \\ll b$).\n",
    "\n",
    "This means that for all $s, a$ s.t. $\\pi(a\\mid s) > 0$, we have $b(a\\mid s) > 0$. Furthermore, to clear up divisions by zero, if $\\pi(a\\mid s) = 0$, then\n",
    "$$\\frac{\\pi(a\\mid s)}{b(a\\mid s)} = 0$$\n",
    "regardless of the value of $b(a\\mid s)$.\n",
    "\n",
    "Now we consider state action pair $(S_t, A_t)$, and we can use either $\\pi$ or $b$ to generate the trajectory\n",
    "$$S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$$\n",
    "The probability to generate this trajectory using policy $x$ is\n",
    "\\begin{align*}\n",
    "\\text{Pr}_x[R_{t+1}, S_{t+1}, A_{t+1}, \\dots, S_{T-1}, A_{T-1}, S_t, A_t\\mid S_t, A_t] &= p(S_{t+1}, R_{t+1}\\mid S_t, A_t)x(A_{t+1}\\mid S_{t+1})\\dots p(S_T, R_T\\mid S_{T-1}, A_{T-1})\\\\\n",
    "&= \\prod_{\\tau = t+1}^{T-1}x(A_\\tau\\mid S_\\tau)\\prod_{\\tau = t}^{T-1}p(S_{\\tau+1}, R_{\\tau + 1}\\mid S_\\tau, A_\\tau)\n",
    "\\end{align*}\n",
    "Importance sample ratio is\n",
    "$$\\rho_{t+1: T-1}\\prod_{\\tau = t+1}^{T-1}\\frac{\\pi(A_\\tau\\mid S_\\tau)}{b(A_\\tau\\mid A_\\tau)}$$"
   ],
   "id": "95355841c85e1d10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In on-policy MC update, after getting the return samples $g_1, \\dots, g_c$, we use mean average $\\frac{1}{c}\\sum_{i=1}^cg_i$ for the value estimates. This assumes that each $g_i$ are of equal probabilities.\n",
    "\n",
    "These return samples are not with equal probabilities between $b$ and $\\pi$.\n",
    "\n",
    "For $\\pi$, the probabilities of those samples are propotional to the important sample ratios - can use weighted average of these samples to estimate. Take $\\rho_i$ for $1 \\leq i \\leq c$ as the importance sample ratio of sample $g_i$, which will also act as its weight. Then the weighted average of those samples is:\n",
    "$$\\frac{\\sum_{i=1}^c\\rho_ig_i}{\\sum_{i=1}^c\\rho_i}$$\n",
    "MC upadate with importance sampling can be implemented incrementally too, but don't need number of samples for each state or state-action pair. Instead, record summation of weights. For example, updating state values can be written as\n",
    "\\begin{align*}\n",
    "c &\\leftarrow c + \\rho\\\\\n",
    "v &\\leftarrow v + \\frac{\\rho}{c}(g - v)\n",
    "\\end{align*}\n",
    "where $c = \\sum\\rho_i$. Updating the action values can be written as:\n",
    "\\begin{align*}\n",
    "c &\\leftarrow c + \\rho\\\\\n",
    "q &\\leftarrow q + \\frac{\\rho}{c}(g - q)\n",
    "\\end{align*}\n"
   ],
   "id": "5e1ad38f4673b12b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Off-Policy MC Policy Evaluation\n",
    "On-policy algorithms perform the following in a nutshell:\n",
    "1. Use target policy to generate samples,\n",
    "2. Use the samples to update value estimates.\n",
    "\n",
    "Policy to generate samples and policy to update are the same, so those algorithms are on-policy.\n",
    "\n",
    "### Algorithm 4.9: Evaluate action values using off-policy MC update based on importance sampling.\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily. If using incremental implementation, initialise $c(s,a) \\leftarrow 0$.\n",
    "2. (MC update) For each episode:\n",
    "    1. (Designate behaviour policy $b$) Designate $b$ s.t. $\\pi \\ll b$.\n",
    "    2. (Sample trajectory) Use $b$ to generate a trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    3. If using first-visit version, perform the following:\n",
    "        1. Initialise $f(s,a) \\leftarrow -1$.\n",
    "        2. For each $t\\leftarrow 0, 1, \\dots, T-1$: if $f(S_t,A_t) < 0$, then set $f(S_t, A_t) \\leftarrow t$.\n",
    "    4. (Initialise return and ratio) $G\\leftarrow 0$; $\\rho \\leftarrow 1$.\n",
    "    5. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G\\leftarrow \\gamma G + R_{t+1}$\n",
    "        2. (Upd. act-val. estim.) Update $q(S_t, A_t)$ to reduce $\\rho[G - q(S_t, A_t)]^2$.\n",
    "        3. If using incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + \\rho$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{\\rho}{c(S_t, A_t)}[G-q(S_t, A_t)]$.\n",
    "            3. If using first visit person, update counter and action value estimates only when $f(S_t, A_t) = t$.\n",
    "        4. (Update importance sampling ratio) $\\rho \\leftarrow \\rho \\frac{\\pi(A_t\\mid S_t)}{b(A_t\\mid S_t)}$.\n",
    "        5. (Check early stop condition) If $\\rho = 0$, **break**."
   ],
   "id": "b376133a598e48b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Off-Policy MC Policy Optimisation\n",
    "When finding optimal policies, they:\n",
    "1. Use latest estimates of the optimal policy to generate samples,\n",
    "2. Use samples to update optimal policy.\n",
    "\n",
    "Policy to generate samples and policy to be updated are the same, so those algorithms are on-policy.\n",
    "\n",
    "### Algorithm 4.10: Find an optimal policy using off-policy MC update based on importance sampling.\n",
    "1. (Initialise) Initialise $q(s,a)$ arbitrarily.\n",
    "2. If using incremental implementation, initialise $c(s,a) \\leftarrow 0$.\n",
    "3. If policy is maintained explicitly, initialise $\\pi(s) \\leftarrow \\arg\\max_{a}q(s,a)$.\n",
    "4. (MC update) For each episode:\n",
    "    1. (Designate behaviour policy $b$) Designate $b$ s.t. $\\pi \\ll b$.\n",
    "    2. (Sample trajectory) Use $b$ to generate a trajectory $S_0, A_0, R_1, S_1, \\dots, S_{T-1}, A_{T-1}, R_T, S_T$.\n",
    "    3. If using first-visit version, perform the following:\n",
    "        1. Initialise $f(s,a) \\leftarrow -1$.\n",
    "        2. For each $t\\leftarrow 0, 1, \\dots, T-1$: if $f(S_t,A_t) < 0$, then set $f(S_t, A_t) \\leftarrow t$.\n",
    "    4. (Initialise return and ratio) $G\\leftarrow 0$; $\\rho \\leftarrow 1$.\n",
    "    5. (Update) For $t \\leftarrow T-1, T-2, \\dots, 0$:\n",
    "        1. (Calculate return) $G\\leftarrow \\gamma G + R_{t+1}$.\n",
    "        2. (Upd. act-val. estim.) Update $q(S_t, A_t)$ to reduce $\\rho[G - q(S_t, A_t)]^2$.\n",
    "        3. If using incremental implementation, perform the following:\n",
    "            1. $c(S_t, A_t) \\leftarrow c(S_t, A_t) + \\rho$.\n",
    "            2. $q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\frac{\\rho}{c(S_t, A_t)}[G-q(S_t, A_t)]$.\n",
    "            3. If using first visit person, update counter and action value estimates only when $f(S_t, A_t) = t$.\n",
    "        4. If maintaining policy explicitly, $\\pi(S_t) \\leftarrow \\arg\\max_a q(S_t, a)$.\n",
    "        5. (Check early stop condition) If $A_t \\neq \\pi(S_t)$, **break**.\n",
    "        6. (Update importance sampling ratio) $\\rho \\leftarrow \\rho \\frac{\\pi(A_t\\mid S_t)}{b(A_t\\mid S_t)}$."
   ],
   "id": "fa566d5ddbb1d081"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
