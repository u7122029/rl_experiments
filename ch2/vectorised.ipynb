{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State space $S = \\{\\text{hungry}, \\text{full}\\}$.\n",
    "\n",
    "Action space $A = \\{\\text{ignore}, \\text{feed}\\}$.\n",
    "\n",
    "Initial state distribution $p_{S_0} = \\begin{bmatrix}1/2 & 1/2\\end{bmatrix}^T$.\n",
    "\n",
    "Rewards = $\\{-3, -2, 1, 2\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_S0 = np.array([1/2, 1/2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "```math\n",
    "p(s',r\\mid s, a) = \\mathrm{Pr}[S_{t+1} = s', R_{t+1}\\mid S_t = s, A_t = a]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamics as a matrix\n",
    "dynamics_matrix = [[0, 1/3,   0,   0], # (hungry, -3)\n",
    "                   [1,   0, 3/4,   0], # (hungry, -2)\n",
    "                   [0, 2/3,   0,   1], # (full, 1)\n",
    "                   [0,   0, 1/4,   0], # (full, 2)\n",
    "                   ]\n",
    "dynamics_matrix = np.array(dynamics_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability Matrix\n",
    "```math\n",
    "\\begin{align*}\n",
    "p(s'\\mid s,a) &= \\mathrm{Pr}\\left[S_{t+1} = s' \\mid S_t = s, A_t = a\\right]\\\\\n",
    "&= \\sum_{r \\in R} p(s', r \\mid s, a)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_sp_given_s_a(dynamics_matrix):\n",
    "    p_sp_given_s_a = np.array([[1,1,0,0], [0,0,1,1]]) @ dynamics_matrix\n",
    "    return p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a = get_p_sp_given_s_a(dynamics_matrix)\n",
    "p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected reward given State-Action Pair\n",
    "\n",
    "```math\n",
    "\\begin{align*}\n",
    "r(s,a) &= \\mathbb{E}[R_{t + 1} \\mid S_t = s, A_t = a]\\\\\n",
    "&= \\sum_{s' \\in S, r \\in R} r \\cdot p(s', r \\mid s, a)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r(dynamics_matrix):\n",
    "    r = np.sum(dynamics_matrix * np.array([[-3], [-2], [1], [2]]), axis=0) # Keep in vector form just in case.\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.        , -0.33333333, -1.        ,  1.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = get_r(dynamics_matrix)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected reward given state, action and next state.\n",
    "```math\n",
    "\\begin{align*}\n",
    "r(s,a,s') &= \\mathbb{E}[R_{t + 1} \\mid S_t = s, A_t = a, S_{t + 1} = s']\\\\\n",
    "&= \\frac{\\sum_{r \\in R}r \\cdot p(s', r \\mid s, a)}{p(s'\\mid s, a)}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_sp(dynamics_matrix, p_sp_given_s_a):\n",
    "    r_sp = np.array([[1,1,0,0],[0,0,1,1]]) @ (dynamics_matrix * np.array([[-3], [-2], [1], [2]]))\n",
    "    r_sp = np.nan_to_num(r_sp / p_sp_given_s_a)\n",
    "    return r_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\_Kookie\\AppData\\Local\\Temp\\ipykernel_26936\\1014741899.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  r_sp = np.nan_to_num(r_sp / p_sp_given_s_a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2., -3., -2.,  0.],\n",
       "       [ 0.,  1.,  2.,  1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_sp = get_r_sp(dynamics_matrix, p_sp_given_s_a)\n",
    "r_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.array([1/4, 3/4, 5/6, 1/6])\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial State-Action Distribution\n",
    "```math\n",
    "\\begin{align*}\n",
    "p_{S_0, A_0, \\pi}(s, a) &= \\mathrm{Pr}_\\pi[S_0 = s, A_0 = a]\\\\\n",
    "&= p_{S_0}(s)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_S0_A0_pi(pi, p_S0):\n",
    "    pi_matrix = pi.reshape(2,2)\n",
    "    p_S0_A0_pi = p_S0.reshape(-1,1) * pi_matrix\n",
    "    return p_S0_A0_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.125     , 0.375     ],\n",
       "       [0.41666667, 0.08333333]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_S0_A0_pi = get_p_S0_A0_pi(pi, p_S0)\n",
    "p_S0_A0_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability between States\n",
    "```math\n",
    "\\begin{align*}\n",
    "p_\\pi(s'\\mid s) &= \\mathrm{Pr}_\\pi[S_{t + 1} = s'\\mid S_t = s]\\\\\n",
    "&= \\sum_{a \\in A}p(s'\\mid s, a)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_pi_sp_given_s(pi, p_sp_given_s_a):\n",
    "    p_pi_sp_given_s = (p_sp_given_s_a * pi) @ np.array([[1,1,0,0], [0,0,1,1]]).T\n",
    "    return p_pi_sp_given_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.625],\n",
       "       [0.5  , 0.375]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pi_sp_given_s = get_p_pi_sp_given_s(pi, p_sp_given_s_a)\n",
    "p_pi_sp_given_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability between State-Action pairs.\n",
    "```math\n",
    "\\begin{align*}\n",
    "p_\\pi(s', a'\\mid s, a) &= \\mathrm{Pr}_\\pi[S_{t + 1} = s', A_{t + 1} = a' \\mid S_t = s, A_t = a]\\\\\n",
    "&= \\pi(a' \\mid s')p(s'\\mid s, a)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_pi_sp_given_s_a(pi, p_sp_given_s_a):\n",
    "    p_pi_sp_ap_given_s_a = pi.reshape(-1,1) * np.repeat(p_sp_given_s_a,2, axis=0)\n",
    "    return p_pi_sp_ap_given_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 0.08333333, 0.1875    , 0.        ],\n",
       "       [0.75      , 0.25      , 0.5625    , 0.        ],\n",
       "       [0.        , 0.55555556, 0.20833333, 0.83333333],\n",
       "       [0.        , 0.11111111, 0.04166667, 0.16666667]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pi_sp_ap_given_s_a = get_p_pi_sp_given_s_a(pi, p_sp_given_s_a)\n",
    "p_pi_sp_ap_given_s_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected State Reward\n",
    "```math\n",
    "\\begin{align*}\n",
    "r_\\pi(s) &= \\mathbb{E}_\\pi[R_{t+1}\\mid S_t = s]\\\\\n",
    "&= \\sum_{a \\in A}r(s,a)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_pi(pi, r):\n",
    "    pi_matrix = pi.reshape(2,2)\n",
    "    r_pi = np.diag(r.reshape(2,2) @ pi_matrix.T)\n",
    "    return r_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75      , -0.66666667])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_pi = get_r_pi(pi, r)\n",
    "r_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Return\n",
    "Episodic task without discount.\n",
    "```math\n",
    "G_t = \\sum_{\\tau = t+1}^TR_{\\tau}\n",
    "```\n",
    "Episodic task with discount $\\gamma \\in [0,1]$:\n",
    "```math\n",
    "G_t = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau R_{t+\\tau+1}\n",
    "```\n",
    "Recursive Relationship:\n",
    "```math\n",
    "G_t = R_{t+1} + \\gamma G_{t + 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value\n",
    "State Value - expected return starting from $s$, then following policy $\\pi$.\n",
    "```math\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t\\mid S_t = s]\n",
    "```\n",
    "Action Value - expected return starting from $(s,a)$, then following $\\pi$.\n",
    "```math\n",
    "q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t\\mid S_t = s, A_t = a]\n",
    "```\n",
    "### Properties of Value\n",
    "Using action-value pairs to back up state values\n",
    "```math\n",
    "v_\\pi(s) = \\sum_{a \\in A}\\pi(a\\mid s)q_\\pi(s,a)\n",
    "```\n",
    "In other words: $v_\\pi(S_t) = \\mathbb{E}_\\pi[q_\\pi(S_t,A_t)]$.\n",
    "\n",
    "Vector form:\n",
    "```math\n",
    "\\vec{v}_\\pi = \\vec{\\pi} \\odot \\vec{q}_\\pi\n",
    "```\n",
    "\n",
    "Using state values to back up action values:\n",
    "```math\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= r(s,a) + \\gamma\\sum_{s' \\in S}p(s'\\mid s,a)v_\\pi(s')\\\\\n",
    "&= \\sum_{s' \\in S^+, r \\in R}p(s',r\\mid s,a)[r + \\gamma v_\\pi(s')]\n",
    "\\end{align*}\n",
    "```\n",
    "In other words, $q_\\pi(S_t,A_t) = \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1})]$.\n",
    "\n",
    "Vector form:\n",
    "```math\n",
    "\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}\\mid S_t,A_t}\\vec{v}_\\pi\n",
    "```\n",
    "- $\\vec{v}_\\pi = (v_\\pi(s): s \\in S)^\\top \\in \\mathbb{R}^{|S|}$\n",
    "- $\\vec{q}_\\pi = (q_\\pi(s,a):(s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $\\vec{\\pi} = (\\pi(a\\mid s): (s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $\\vec{r} = (r(s,a) : (s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $P_{S_{t+1}\\mid S_t, A_t} = (p(s'\\mid s,a): s' \\in S, (s,a) \\in S \\times A) \\in [0,1]^{|S| \\times |S||A|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4), (4,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a.shape, r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equations\n",
    "The two relations above combine together to form the two relations below.\n",
    "\n",
    "Use state values at time $t+1$ to back up the state values at time $t$:\n",
    "```math\n",
    "v_\\pi(s) = r_\\pi(s) + \\gamma \\sum_{s' \\in S}p_\\pi(s'\\mid s)v_\\pi(s')\n",
    "```\n",
    "In other words: $v_\\pi(S_t) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})]$.\n",
    "\n",
    "Vector form:\n",
    "```math\n",
    "\\vec{v}_\\pi = \\vec{r}_\\pi + \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi}\\vec{v}_\\pi\n",
    "```\n",
    "where $\\vec{r}_\\pi = (r_\\pi(s): s\\in S)^\\top \\in \\mathbb{R}^{|S|}$.\n",
    "\n",
    "Use action values at time $t+1$ to represent the action values at time $t$.\n",
    "```math\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= r(s,a) + \\gamma \\sum_{s'\\in S, a'\\in A}p_\\pi(s', a'\\mid s, a)q_\\pi(s',a')\\\\\n",
    "&= \\sum_{s'\\in S, r\\in R}p(s', r\\mid s, a)\\left[r + \\gamma\\sum_{a'\\in A}\\pi(a'\\mid s')q_\\pi(s', a')\\right]\n",
    "\\end{align*}\n",
    "```\n",
    "In other words, $q_\\pi(S_t, A_t) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1})]$\n",
    "\n",
    "Vector form:\n",
    "```math\n",
    "\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi} \\vec{q}_\\pi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranged relations:\n",
    "```math\n",
    "\\begin{align*}\n",
    "\\vec{v}_\\pi &= (I - \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi})^{-1}\\vec{r}_\\pi\\\\\n",
    "\\vec{q}_\\pi &= (I - \\gamma P^\\top_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi})^{-1}\\vec{r}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1\n",
    "1. Use $\\vec{v}_\\pi = (I - \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi})^{-1}\\vec{r}_\\pi$ to obtain $\\vec{v}_\\pi$.\n",
    "2. Use $\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}\\mid S_t, A_t; \\pi}\\vec{v}_\\pi$ to obtain $\\vec{q}_\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed and Full Example\n",
    "def get_vq1(p_pi_sp_given_s, p_sp_given_s_a, gamma, r_pi, r):\n",
    "    v_pi = np.linalg.inv(np.eye(2) - gamma * p_pi_sp_given_s.T) @ r_pi\n",
    "    q_pi = r + (gamma * p_sp_given_s_a.T) @ v_pi\n",
    "    return v_pi, q_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.59848485, -3.52272727]),\n",
       " array([-4.87878788, -3.17171717, -3.86363636, -1.81818182]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 4/5\n",
    "v_pi1, q_pi1 = get_vq1(p_pi_sp_given_s, p_sp_given_s_a, gamma, r_pi, r)\n",
    "v_pi1, q_pi1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "1. Use $\\vec{q}_\\pi = (I - \\gamma P^\\top_{S_{t+1}\\mid S_t, A_t;\\pi})^{-1}\\vec{r}$ to obtain $\\vec{q}_\\pi$.\n",
    "2. Use $\\vec{v}_\\pi = \\vec{\\pi} \\odot \\vec{q}_\\pi$ to obtain $\\vec{v}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vq2(p_pi_sp_given_s, p_sp_given_s_a, gamma, pi, r):\n",
    "    q_pi = np.linalg.inv(np.eye(4) - gamma * p_pi_sp_ap_given_s_a.T) @ r\n",
    "    v_pi = np.array([[1,1,0,0],[0,0,1,1]]) @ (pi * q_pi)\n",
    "    return v_pi, q_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.59848485, -3.52272727]),\n",
       " array([-4.87878788, -3.17171717, -3.86363636, -1.81818182]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_pi2, q_pi2 = get_vq2(p_pi_sp_given_s, p_sp_given_s_a, gamma, pi, r)\n",
    "v_pi2, q_pi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that both approaches achieve the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Expected Returns using Values\n",
    "Expected return at $t = 0$:\n",
    "```math\n",
    "\\begin{align*}\n",
    "g_\\pi &= \\mathbb{E}_{S_0 \\sim p_{S_0}}[v_\\pi(S_0)]\\\\\n",
    "&= \\vec{p}_{S_0}^\\top \\vec{v}_\\pi\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g_pi(p_S0, v_pi):\n",
    "    g_pi = p_S0 @ v_pi.reshape(-1,1)\n",
    "    return g_pi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-3.5606060606060614)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_pi = get_g_pi(p_S0, v_pi1)\n",
    "g_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement Theorem\n",
    "If for all $s \\in S$,\n",
    "```math\n",
    "v_\\pi(s) \\leq \\sum_a \\pi'(a\\mid s)q_\\pi(s, a) = \\mathbb{E}_{A\\sim \\pi'(s)}[q_\\pi(s,A)]\n",
    "```\n",
    "then $\\pi \\preccurlyeq \\pi'$. I.e: $v_\\pi(s) \\leq v_{\\pi'}(s)$.\n",
    "\n",
    "Additionally, if there is a state $s \\in S$ such that the former inequality holds, there is a state $s \\in S$ such that the latter inequality also holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_le(v_p1, q_p1, p2):\n",
    "    return np.all(v_p1 <= (np.array([[1,1,0,0], [0,0,1,1]]) @ (p2 * q_p1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi2 = np.array([0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_le(v_pi1, q_pi1, pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if a Policy is Optimal\n",
    "- For each state-action pair $s \\in S$ and $a \\in A(s)$: if $\\pi(a\\mid s) > 0$ and $q_\\pi(s, a) < \\max_{a' \\in A}q_\\pi(s, a')$, then the policy is not optimal.\n",
    "- Otherwise, policy $\\pi$ is optimal and cannot be further improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_policy_optimal(pi, q_pi):\n",
    "    m = int(len(q_pi) ** (1/2))\n",
    "    mask = (pi.reshape(m,m) > 0) & (q_pi.reshape(m,m) < np.max(q_pi.reshape(m,m), axis=1).reshape(-1,1))\n",
    "    return not np.any(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_policy_optimal(pi, q_pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.75757576, 1.96969697]),\n",
       " array([-1.39393939,  0.91919192, -0.15151515,  2.57575758]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pi2_sp_given_s = get_p_pi_sp_given_s(pi2,p_sp_given_s_a)\n",
    "r_pi2 = get_r_pi(pi2, r) \n",
    "v_pi_det, q_pi_det = get_vq1(p_pi_sp_given_s, p_sp_given_s_a, gamma, r_pi2, r)\n",
    "v_pi_det, q_pi_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_policy_optimal(pi2, q_pi_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement Algorithm\n",
    "- For each state $s \\in S$, set $\\pi'(s) \\leftarrow \\arg\\max_{a \\in A}q_\\pi(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
