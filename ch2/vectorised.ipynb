{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State space $S = \\{\\text{hungry}, \\text{full}\\}$.\n",
    "\n",
    "Action space $A = \\{\\text{ignore}, \\text{feed}\\}$.\n",
    "\n",
    "Initial state distribution $p_{S_0} = \\begin{bmatrix}1/2 & 1/2\\end{bmatrix}^T$.\n",
    "\n",
    "Rewards = $\\{-3, -2, 1, 2\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_S0 = np.array([1/2, 1/2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "```math\n",
    "p(s',r\\mid s, a) = \\mathrm{Pr}[S_{t+1} = s', R_{t+1}\\mid S_t = s, A_t = a]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamics as a matrix\n",
    "dynamics_matrix = [[0, 1/3,   0,   0], # (hungry, -3)\n",
    "                   [1,   0, 3/4,   0], # (hungry, -2)\n",
    "                   [0, 2/3,   0,   1], # (full, 1)\n",
    "                   [0,   0, 1/4,   0], # (full, 2)\n",
    "                   ]\n",
    "dynamics_matrix = np.array(dynamics_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability Matrix\n",
    "```math\n",
    "\\begin{align*}\n",
    "p(s'\\mid s,a) &= \\mathrm{Pr}\\left[S_{t+1} = s' \\mid S_t = s, A_t = a\\right]\\\\\n",
    "&= \\sum_{r \\in R} p(s', r \\mid s, a)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a = np.array([[1,1,0,0], [0,0,1,1]]) @ dynamics_matrix\n",
    "p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected reward given State-Action Pair\n",
    "\n",
    "```math\n",
    "\\begin{align*}\n",
    "r(s,a) &= \\mathbb{E}[R_{t + 1} \\mid S_t = s, A_t = a]\\\\\n",
    "&= \\sum_{s' \\in S, r \\in R} r \\cdot p(s', r \\mid s, a)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.        , -0.33333333, -1.        ,  1.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.sum(dynamics_matrix * np.array([[-3], [-2], [1], [2]]), axis=0) # Keep in vector form just in case.\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected reward given state, action and next state.\n",
    "```math\n",
    "\\begin{align*}\n",
    "r(s,a,s') &= \\mathbb{E}[R_{t + 1} \\mid S_t = s, A_t = a, S_{t + 1} = s']\\\\\n",
    "&= \\frac{\\sum_{r \\in R}r \\cdot p(s', r \\mid s, a)}{p(s'\\mid s, a)}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\_Kookie\\AppData\\Local\\Temp\\ipykernel_42448\\1777567165.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  r_sp = np.nan_to_num(r_sp / p_sp_given_s_a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2., -3., -2.,  0.],\n",
       "       [ 0.,  1.,  2.,  1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_sp = np.array([[1,1,0,0],[0,0,1,1]]) @ (dynamics_matrix * np.array([[-3], [-2], [1], [2]]))\n",
    "r_sp = np.nan_to_num(r_sp / p_sp_given_s_a)\n",
    "r_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.array([1/4, 3/4, 5/6, 1/6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial State-Action Distribution\n",
    "```math\n",
    "\\begin{align*}\n",
    "p_{S_0, A_0, \\pi}(s, a) &= \\mathrm{Pr}_\\pi[S_0 = s, A_0 = a]\\\\\n",
    "&= p_{S_0}(s)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.125     , 0.375     ],\n",
       "       [0.41666667, 0.08333333]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_matrix = pi.reshape(2,2)\n",
    "p_S0_A0_pi = p_S0.reshape(-1,1) * pi_matrix\n",
    "p_S0_A0_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability between States\n",
    "```math\n",
    "\\begin{align*}\n",
    "p_\\pi(s'\\mid s) &= \\mathrm{Pr}_\\pi[S_{t + 1} = s'\\mid S_t = s]\\\\\n",
    "&= \\sum_{a \\in A}p(s'\\mid s, a)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pi_sp_given_s = (p_sp_given_s_a * pi) @ np.array([[1,1,0,0], [0,0,1,1]]).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.625],\n",
       "       [0.5  , 0.375]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pi_sp_given_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability between State-Action pairs.\n",
    "```math\n",
    "\\begin{align*}\n",
    "p_\\pi(s', a'\\mid s, a) &= \\mathrm{Pr}_\\pi[S_{t + 1} = s', A_{t + 1} = a' \\mid S_t = s, A_t = a]\\\\\n",
    "&= \\pi(a' \\mid s')p(s'\\mid s, a)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pi_sp_ap_given_s_a = pi.reshape(-1,1) * np.repeat(p_sp_given_s_a,2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 0.08333333, 0.1875    , 0.        ],\n",
       "       [0.75      , 0.25      , 0.5625    , 0.        ],\n",
       "       [0.        , 0.55555556, 0.20833333, 0.83333333],\n",
       "       [0.        , 0.11111111, 0.04166667, 0.16666667]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pi_sp_ap_given_s_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected State Reward\n",
    "```math\n",
    "\\begin{align*}\n",
    "r_\\pi(s) &= \\mathbb{E}_\\pi[R_{t+1}\\mid S_t = s]\\\\\n",
    "&= \\sum_{a \\in A}r(s,a)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.        , -0.33333333],\n",
       "       [-1.        ,  1.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.reshape(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pi = np.diag(r.reshape(2,2) @ pi_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75      , -0.66666667])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Return\n",
    "Episodic task without discount.\n",
    "```math\n",
    "G_t = \\sum_{\\tau = t+1}^TR_{\\tau}\n",
    "```\n",
    "Episodic task with discount $\\gamma \\in [0,1]$:\n",
    "```math\n",
    "G_t = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau R_{t+\\tau+1}\n",
    "```\n",
    "Recursive Relationship:\n",
    "```math\n",
    "G_t = R_{t+1} + \\gamma G_{t + 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value\n",
    "State Value - expected return starting from $s$, then following policy $\\pi$.\n",
    "```math\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t\\mid S_t = s]\n",
    "```\n",
    "Action Value - expected return starting from $(s,a)$, then following $\\pi$.\n",
    "```math\n",
    "q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t\\mid S_t = s, A_t = a]\n",
    "```\n",
    "### Properties of Value\n",
    "Using action-value pairs to back up state values\n",
    "```math\n",
    "v_\\pi(s) = \\sum_{a \\in A}\\pi(a\\mid s)q_\\pi(s,a)\n",
    "```\n",
    "In other words: $v_\\pi(S_t) = \\mathbb{E}_\\pi[q_\\pi(S_t,A_t)]$.\n",
    "\n",
    "Vector form:\n",
    "```math\n",
    "\\vec{v}_\\pi = \\vec{\\pi} \\odot \\vec{q}_\\pi\n",
    "```\n",
    "\n",
    "Using state values to back up action values:\n",
    "```math\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= r(s,a) + \\gamma\\sum_{s' \\in S}p(s'\\mid s,a)v_\\pi(s')\\\\\n",
    "&= \\sum_{s' \\in S^+, r \\in R}p(s',r\\mid s,a)[r + \\gamma v_\\pi(s')]\n",
    "\\end{align*}\n",
    "```\n",
    "In other words, $q_\\pi(S_t,A_t) = \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1})]$.\n",
    "\n",
    "Vector form:\n",
    "```math\n",
    "\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}\\mid S_t,A_t}\\vec{v}_\\pi\n",
    "```\n",
    "- $\\vec{v}_\\pi = (v_\\pi(s): s \\in S)^\\top \\in \\mathbb{R}^{|S|}$\n",
    "- $\\vec{q}_\\pi = (q_\\pi(s,a):(s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $\\vec{\\pi} = (\\pi(a\\mid s): (s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $\\vec{r} = (r(s,a) : (s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $P_{S_{t+1}\\mid S_t, A_t} = (p(s'\\mid s,a): s' \\in S, (s,a) \\in S \\times A) \\in [0,1]^{|S| \\times |S||A|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4), (4,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a.shape, r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equations\n",
    "The two relations above combine together to form the two relations below.\n",
    "\n",
    "Use state values at time $t+1$ to back up the state values at time $t$:\n",
    "```math\n",
    "v_\\pi(s) = r_\\pi(s) + \\gamma \\sum_{s' \\in S}p_\\pi(s'\\mid s)v_\\pi(s')\n",
    "```\n",
    "In other words: $v_\\pi(S_t) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})]$.\n",
    "\n",
    "Vector form:\n",
    "```math\n",
    "\\vec{v}_\\pi = \\vec{r}_\\pi + \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi}\\vec{v}_\\pi\n",
    "```\n",
    "where $\\vec{r}_\\pi = (r_\\pi(s): s\\in S)^\\top \\in \\mathbb{R}^{|S|}$.\n",
    "\n",
    "Use action values at time $t+1$ to represent the action values at time $t$.\n",
    "```math\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= r(s,a) + \\gamma \\sum_{s'\\in S, a'\\in A}p_\\pi(s', a'\\mid s, a)q_\\pi(s',a')\\\\\n",
    "&= \\sum_{s'\\in S, r\\in R}p(s', r\\mid s, a)\\left[r + \\gamma\\sum_{a'\\in A}\\pi(a'\\mid s')q_\\pi(s', a')\\right]\n",
    "\\end{align*}\n",
    "```\n",
    "In other words, $q_\\pi(S_t, A_t) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1})]$\n",
    "\n",
    "Vector form:\n",
    "```math\n",
    "\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi} \\vec{q}_\\pi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranged relations:\n",
    "```math\n",
    "\\begin{align*}\n",
    "\\vec{v}_\\pi &= (I - \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi})^{-1}\\vec{r}_\\pi\\\\\n",
    "\\vec{q}_\\pi &= (I - \\gamma P^\\top_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi})^{-1}\\vec{r}\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1\n",
    "1. Use $\\vec{v}_\\pi = (I - \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi})^{-1}\\vec{r}_\\pi$ to obtain $\\vec{v}_\\pi$.\n",
    "2. Use $\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}\\mid S_t, A_t; \\pi}\\vec{v}_\\pi$ to obtain $\\vec{q}_\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.59848485, -3.52272727])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed and Full Example\n",
    "gamma = 4/5\n",
    "v_pi1 = np.linalg.inv(np.eye(2) - gamma * p_pi_sp_given_s.T) @ r_pi\n",
    "v_pi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.87878788, -3.17171717, -3.86363636, -1.81818182])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pi1 = r + (gamma * p_sp_given_s_a.T) @ v_pi\n",
    "q_pi1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "1. Use $\\vec{q}_\\pi = (I - \\gamma P^\\top_{S_{t+1}\\mid S_t, A_t;\\pi})^{-1}\\vec{r}$ to obtain $\\vec{q}_\\pi$.\n",
    "2. Use $\\vec{v}_\\pi = \\vec{\\pi} \\odot \\vec{q}_\\pi$ to obtain $\\vec{v}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.87878788, -3.17171717, -3.86363636, -1.81818182])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_pi2 = np.linalg.inv(np.eye(4) - gamma * p_pi_sp_ap_given_s_a.T) @ r\n",
    "q_pi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.59848485, -3.52272727])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_pi2 = np.array([[1,1,0,0],[0,0,1,1]]) @ (pi * q_pi2)\n",
    "v_pi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that both approaches achieve the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Expected Returns using Values\n",
    "Expected return at $t = 0$:\n",
    "```math\n",
    "\\begin{align*}\n",
    "g_\\pi &= \\mathbb{E}_{S_0 \\sim p_{S_0}}[v_\\pi(S_0)]\\\\\n",
    "&= \\vec{p}_{S_0}^\\top \\vec{v}_\\pi\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.56060606])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_pi = p_S0 @ v_pi.reshape(-1,1)\n",
    "g_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_S0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.59848485, -3.52272727])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement Theorem\n",
    "If for all $s \\in S$,\n",
    "```math\n",
    "v_\\pi(s) \\leq \\sum_a \\pi'(a\\mid s)q_\\pi(s, a) = \\mathbb{E}_{A\\sim \\pi'(s)}[q_\\pi(s,A)]\n",
    "```\n",
    "then $\\pi \\preccurlyeq \\pi'$. I.e: $v_\\pi(s) \\leq v_{\\pi'}(s)$.\n",
    "\n",
    "Additionally, if there is a state $s \\in S$ such that the former inequality holds, there is a state $s \\in S$ such that the latter inequality also holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_le(v_p1, q_p1, p2):\n",
    "    return np.all(v_p1 <= (np.array([[1,1,0,0], [0,0,1,1]]) @ (p2 * q_p1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi2 = np.array([0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_le(v_pi, q_pi, pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_policy_optimal(pi, q_pi):\n",
    "    mask = q_pi < np.max(q_pi, axis=1)\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAxisError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mis_policy_optimal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_pi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mis_policy_optimal\u001b[39m\u001b[34m(pi, q_pi)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_policy_optimal\u001b[39m(pi, q_pi):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     mask = q_pi < \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_pi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\git_d\\rl_experiments\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3199\u001b[39m, in \u001b[36mmax\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3080\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[32m   3081\u001b[39m \u001b[38;5;129m@set_module\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3082\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, initial=np._NoValue,\n\u001b[32m   3083\u001b[39m          where=np._NoValue):\n\u001b[32m   3084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3085\u001b[39m \u001b[33;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[32m   3086\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3197\u001b[39m \u001b[33;03m    5\u001b[39;00m\n\u001b[32m   3198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3200\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\git_d\\rl_experiments\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAxisError\u001b[39m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "is_policy_optimal(pi, q_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
