{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:19.879241Z",
     "start_time": "2025-06-01T13:38:19.828856Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Markdown"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State space $S = \\{\\text{hungry}, \\text{full}\\}$.\n",
    "\n",
    "Action space $A = \\{\\text{ignore}, \\text{feed}\\}$.\n",
    "\n",
    "Initial state distribution $p_{S_0} = \\begin{bmatrix}1/2 & 1/2\\end{bmatrix}^T$.\n",
    "\n",
    "Rewards = $\\{-3, -2, 1, 2\\}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:20.803984Z",
     "start_time": "2025-06-01T13:38:20.802020Z"
    }
   },
   "source": [
    "p_S0 = np.array([1/2, 1/2])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "$$p(s',r\\mid s, a) = \\mathrm{Pr}[S_{t+1} = s', R_{t+1}=r\\mid S_t = s, A_t = a]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:21.233346Z",
     "start_time": "2025-06-01T13:38:21.231129Z"
    }
   },
   "source": [
    "# Dynamics as a matrix\n",
    "dynamics_matrix = [[0, 1/3,   0,   0], # (hungry, -3)\n",
    "                   [1,   0, 3/4,   0], # (hungry, -2)\n",
    "                   [0, 2/3,   0,   1], # (full, 1)\n",
    "                   [0,   0, 1/4,   0], # (full, 2)\n",
    "                   ]\n",
    "dynamics_matrix = np.array(dynamics_matrix) "
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability Matrix\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(s'\\mid s,a) &= \\mathrm{Pr}\\left[S_{t+1} = s' \\mid S_t = s, A_t = a\\right]\\\\\n",
    "&= \\sum_{r \\in R} p(s', r \\mid s, a)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:21.697483Z",
     "start_time": "2025-06-01T13:38:21.695394Z"
    }
   },
   "source": [
    "def get_p_sp_given_s_a(dynamics_matrix):\n",
    "    p_sp_given_s_a = np.array([[1,1,0,0], [0,0,1,1]]) @ dynamics_matrix\n",
    "    return p_sp_given_s_a"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:21.887187Z",
     "start_time": "2025-06-01T13:38:21.883704Z"
    }
   },
   "source": [
    "p_sp_given_s_a = get_p_sp_given_s_a(dynamics_matrix)\n",
    "p_sp_given_s_a"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Reward given State-Action Pair\n",
    "$$\n",
    "\\begin{align*}\n",
    "r(s,a) &= \\mathbb{E}[R_{t + 1} \\mid S_t = s, A_t = a]\\\\\n",
    "&= \\sum_{s' \\in S, r \\in R} r \\cdot p(s', r \\mid s, a)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:22.279968Z",
     "start_time": "2025-06-01T13:38:22.277620Z"
    }
   },
   "source": [
    "def get_r(dynamics_matrix):\n",
    "    r = np.sum(dynamics_matrix * np.array([[-3], [-2], [1], [2]]), axis=0) # Keep in vector form just in case.\n",
    "    return r"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:22.477177Z",
     "start_time": "2025-06-01T13:38:22.474753Z"
    }
   },
   "source": [
    "r = get_r(dynamics_matrix)\n",
    "r"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.        , -0.33333333, -1.        ,  1.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Reward given State, Action and Next State.\n",
    "$$\n",
    "\\begin{align*}\n",
    "r(s,a,s') &= \\mathbb{E}[R_{t + 1} \\mid S_t = s, A_t = a, S_{t + 1} = s']\\\\\n",
    "&= \\frac{\\sum_{r \\in R}r \\cdot p(s', r \\mid s, a)}{p(s'\\mid s, a)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:22.846752Z",
     "start_time": "2025-06-01T13:38:22.844534Z"
    }
   },
   "source": [
    "def get_r_sp(dynamics_matrix, p_sp_given_s_a):\n",
    "    r_sp = np.array([[1,1,0,0],[0,0,1,1]]) @ (dynamics_matrix * np.array([[-3], [-2], [1], [2]]))\n",
    "    r_sp = np.nan_to_num(r_sp / p_sp_given_s_a)\n",
    "    return r_sp"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:23.070180Z",
     "start_time": "2025-06-01T13:38:23.067095Z"
    }
   },
   "source": [
    "r_sp = get_r_sp(dynamics_matrix, p_sp_given_s_a)\n",
    "r_sp"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\_Kookie\\AppData\\Local\\Temp\\ipykernel_10788\\3337454226.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  r_sp = np.nan_to_num(r_sp / p_sp_given_s_a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2., -3., -2.,  0.],\n",
       "       [ 0.,  1.,  2.,  1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:23.424782Z",
     "start_time": "2025-06-01T13:38:23.421429Z"
    }
   },
   "source": [
    "pi = np.array([1/4, 3/4, 5/6, 1/6])\n",
    "pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial State-Action Distribution, According to $\\pi$\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_{S_0, A_0, \\pi}(s, a) &= \\mathrm{Pr}_\\pi[S_0 = s, A_0 = a]\\\\\n",
    "&= p_{S_0}(s)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:23.779860Z",
     "start_time": "2025-06-01T13:38:23.777124Z"
    }
   },
   "source": [
    "def get_p_S0_A0_pi(pi, p_S0):\n",
    "    pi_matrix = pi.reshape(2,2)\n",
    "    p_S0_A0_pi = p_S0.reshape(-1,1) * pi_matrix\n",
    "    return p_S0_A0_pi"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:24.000775Z",
     "start_time": "2025-06-01T13:38:23.998355Z"
    }
   },
   "source": [
    "p_S0_A0_pi = get_p_S0_A0_pi(pi, p_S0)\n",
    "p_S0_A0_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.125     , 0.375     ],\n",
       "       [0.41666667, 0.08333333]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability between States, According to $\\pi$\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_\\pi(s'\\mid s) &= \\mathrm{Pr}_\\pi[S_{t + 1} = s'\\mid S_t = s]\\\\\n",
    "&= \\sum_{a \\in A}p(s'\\mid s, a)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:24.388354Z",
     "start_time": "2025-06-01T13:38:24.386012Z"
    }
   },
   "source": [
    "def get_p_pi_sp_given_s(pi, p_sp_given_s_a):\n",
    "    p_pi_sp_given_s = (p_sp_given_s_a * pi) @ np.array([[1,1,0,0], [0,0,1,1]]).T\n",
    "    return p_pi_sp_given_s"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:24.618989Z",
     "start_time": "2025-06-01T13:38:24.616373Z"
    }
   },
   "source": [
    "p_pi_sp_given_s = get_p_pi_sp_given_s(pi, p_sp_given_s_a)\n",
    "p_pi_sp_given_s"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.625],\n",
       "       [0.5  , 0.375]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability between State-Action Pairs, According to $\\pi$\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_\\pi(s', a'\\mid s, a) &= \\mathrm{Pr}_\\pi[S_{t + 1} = s', A_{t + 1} = a' \\mid S_t = s, A_t = a]\\\\\n",
    "&= \\pi(a' \\mid s')p(s'\\mid s, a)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:25.031047Z",
     "start_time": "2025-06-01T13:38:25.029142Z"
    }
   },
   "source": [
    "def get_p_pi_sp_given_s_a(pi, p_sp_given_s_a):\n",
    "    p_pi_sp_ap_given_s_a = pi.reshape(-1,1) * np.repeat(p_sp_given_s_a,2, axis=0)\n",
    "    return p_pi_sp_ap_given_s_a"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:25.241163Z",
     "start_time": "2025-06-01T13:38:25.238550Z"
    }
   },
   "source": [
    "p_pi_sp_ap_given_s_a = get_p_pi_sp_given_s_a(pi, p_sp_given_s_a)\n",
    "p_pi_sp_ap_given_s_a"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 0.08333333, 0.1875    , 0.        ],\n",
       "       [0.75      , 0.25      , 0.5625    , 0.        ],\n",
       "       [0.        , 0.55555556, 0.20833333, 0.83333333],\n",
       "       [0.        , 0.11111111, 0.04166667, 0.16666667]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected State Reward, According to $\\pi$\n",
    "$$\n",
    "\\begin{align*}\n",
    "r_\\pi(s) &= \\mathbb{E}_\\pi[R_{t+1}\\mid S_t = s]\\\\\n",
    "&= \\sum_{a \\in A}r(s,a)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:25.600788Z",
     "start_time": "2025-06-01T13:38:25.598697Z"
    }
   },
   "source": [
    "def get_r_pi(pi, r):\n",
    "    pi_matrix = pi.reshape(2,2)\n",
    "    r_pi = np.diag(r.reshape(2,2) @ pi_matrix.T)\n",
    "    return r_pi"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:25.805921Z",
     "start_time": "2025-06-01T13:38:25.802853Z"
    }
   },
   "source": [
    "r_pi = get_r_pi(pi, r)\n",
    "r_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75      , -0.66666667])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Return\n",
    "Episodic task without discount.\n",
    "$$\n",
    "G_t = \\sum_{\\tau = t+1}^TR_{\\tau}\n",
    "$$\n",
    "Episodic task with discount $\\gamma \\in [0,1]$:\n",
    "$$\n",
    "G_t = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau R_{t+\\tau+1}\n",
    "$$\n",
    "Recursive Relationship:\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma G_{t + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value\n",
    "State Value - expected return starting from $s$, then following policy $\\pi$.\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t\\mid S_t = s]\n",
    "$$\n",
    "Action Value - expected return starting from $(s,a)$, then following $\\pi$.\n",
    "$$\n",
    "q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t\\mid S_t = s, A_t = a]\n",
    "$$\n",
    "### Properties of Value\n",
    "Using action-value pairs to back up state values\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in A}\\pi(a\\mid s)q_\\pi(s,a)\n",
    "$$\n",
    "In other words: $v_\\pi(S_t) = \\mathbb{E}_\\pi[q_\\pi(S_t,A_t)]$.\n",
    "\n",
    "Vector form:\n",
    "$$\n",
    "\\vec{v}_\\pi = \\vec{\\pi} \\odot \\vec{q}_\\pi\n",
    "$$\n",
    "Using state values to back up action values:\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= r(s,a) + \\gamma\\sum_{s' \\in S}p(s'\\mid s,a)v_\\pi(s')\\\\\n",
    "&= \\sum_{s' \\in S^+, r \\in R}p(s',r\\mid s,a)[r + \\gamma v_\\pi(s')]\n",
    "\\end{align*}\n",
    "$$\n",
    "In other words, $q_\\pi(S_t,A_t) = \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1})]$.\n",
    "\n",
    "Vector form:\n",
    "$$\n",
    "\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}\\mid S_t,A_t}\\vec{v}_\\pi\n",
    "$$\n",
    "- $\\vec{v}_\\pi = (v_\\pi(s): s \\in S)^\\top \\in \\mathbb{R}^{|S|}$\n",
    "- $\\vec{q}_\\pi = (q_\\pi(s,a):(s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $\\vec{\\pi} = (\\pi(a\\mid s): (s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $\\vec{r} = (r(s,a) : (s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $P_{S_{t+1}\\mid S_t, A_t} = (p(s'\\mid s,a): s' \\in S, (s,a) \\in S \\times A) \\in [0,1]^{|S| \\times |S||A|}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:26.466189Z",
     "start_time": "2025-06-01T13:38:26.463472Z"
    }
   },
   "source": [
    "p_sp_given_s_a.shape, r.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4), (4,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equations\n",
    "The two relations above combine to form the two relations below.\n",
    "\n",
    "### Use state values at time $t+1$ to back up the state values at time $t$\n",
    "$$\n",
    "v_\\pi(s) = r_\\pi(s) + \\gamma \\sum_{s' \\in S}p_\\pi(s'\\mid s)v_\\pi(s')\n",
    "$$\n",
    "In other words: $v_\\pi(S_t) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})]$.\n",
    "\n",
    "Vector form:\n",
    "$$\n",
    "\\vec{v}_\\pi = \\vec{r}_\\pi + \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi}\\vec{v}_\\pi\n",
    "$$\n",
    "where $\\vec{r}_\\pi = (r_\\pi(s): s\\in S)^\\top \\in \\mathbb{R}^{|S|}$.\n",
    "\n",
    "### Use action values at time $t+1$ to represent the action values at time $t$\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= r(s,a) + \\gamma \\sum_{s'\\in S, a'\\in A}p_\\pi(s', a'\\mid s, a)q_\\pi(s',a')\\\\\n",
    "&= \\sum_{s'\\in S, r\\in R}p(s', r\\mid s, a)\\left[r + \\gamma\\sum_{a'\\in A}\\pi(a'\\mid s')q_\\pi(s', a')\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "In other words, $q_\\pi(S_t, A_t) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1})]$\n",
    "\n",
    "Vector form:\n",
    "$$\n",
    "\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi} \\vec{q}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranged relations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{v}_\\pi &= (I - \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi})^{-1}\\vec{r}_\\pi\\\\\n",
    "\\vec{q}_\\pi &= (I - \\gamma P^\\top_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi})^{-1}\\vec{r}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1\n",
    "1. Use $\\vec{v}_\\pi = (I - \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi})^{-1}\\vec{r}_\\pi$ to obtain $\\vec{v}_\\pi$.\n",
    "2. Use $\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}\\mid S_t, A_t; \\pi}\\vec{v}_\\pi$ to obtain $\\vec{q}_\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:27.687011Z",
     "start_time": "2025-06-01T13:38:27.683962Z"
    }
   },
   "source": [
    "# Feed and Full Example\n",
    "def get_vq1(p_pi_sp_given_s, p_sp_given_s_a, gamma, r_pi, r):\n",
    "    v_pi = np.linalg.inv(np.eye(2) - gamma * p_pi_sp_given_s.T) @ r_pi\n",
    "    q_pi = r + (gamma * p_sp_given_s_a.T) @ v_pi\n",
    "    return v_pi, q_pi"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:27.932606Z",
     "start_time": "2025-06-01T13:38:27.929921Z"
    }
   },
   "source": [
    "gamma = 4/5\n",
    "v_pi1, q_pi1 = get_vq1(p_pi_sp_given_s, p_sp_given_s_a, gamma, r_pi, r)\n",
    "v_pi1, q_pi1"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.59848485, -3.52272727]),\n",
       " array([-4.87878788, -3.17171717, -3.86363636, -1.81818182]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "1. Use $\\vec{q}_\\pi = (I - \\gamma P^\\top_{S_{t+1}\\mid S_t, A_t;\\pi})^{-1}\\vec{r}$ to obtain $\\vec{q}_\\pi$.\n",
    "2. Use $\\vec{v}_\\pi = \\vec{\\pi} \\odot \\vec{q}_\\pi$ to obtain $\\vec{v}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:28.424166Z",
     "start_time": "2025-06-01T13:38:28.421843Z"
    }
   },
   "source": [
    "def get_vq2(p_pi_sp_given_s, p_sp_given_s_a, gamma, pi, r):\n",
    "    q_pi = np.linalg.inv(np.eye(4) - gamma * p_pi_sp_ap_given_s_a.T) @ r\n",
    "    v_pi = np.array([[1,1,0,0],[0,0,1,1]]) @ (pi * q_pi)\n",
    "    return v_pi, q_pi"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:28.716251Z",
     "start_time": "2025-06-01T13:38:28.713467Z"
    }
   },
   "source": [
    "v_pi2, q_pi2 = get_vq2(p_pi_sp_given_s, p_sp_given_s_a, gamma, pi, r)\n",
    "v_pi2, q_pi2"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.59848485, -3.52272727]),\n",
       " array([-4.87878788, -3.17171717, -3.86363636, -1.81818182]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that both approaches achieve the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Expected Returns using Values\n",
    "Expected return at $t = 0$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_\\pi &= \\mathbb{E}_{S_0 \\sim p_{S_0}}[v_\\pi(S_0)]\\\\\n",
    "&= \\vec{p}_{S_0}^\\top \\vec{v}_\\pi\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:29.456939Z",
     "start_time": "2025-06-01T13:38:29.455047Z"
    }
   },
   "source": [
    "def get_g_pi(p_S0, v_pi):\n",
    "    g_pi = p_S0 @ v_pi.reshape(-1,1)\n",
    "    return g_pi[0]"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:29.757107Z",
     "start_time": "2025-06-01T13:38:29.753856Z"
    }
   },
   "source": [
    "g_pi = get_g_pi(p_S0, v_pi1)\n",
    "g_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-3.5606060606060614)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement Theorem\n",
    "If for all $s \\in S$,\n",
    "$$\n",
    "v_\\pi(s) \\leq \\sum_a \\pi'(a\\mid s)q_\\pi(s, a) = \\mathbb{E}_{A\\sim \\pi'(s)}[q_\\pi(s,A)]\n",
    "$$\n",
    "then $\\pi \\preccurlyeq \\pi'$. I.e: $v_\\pi(s) \\leq v_{\\pi'}(s)$.\n",
    "\n",
    "Additionally, if there is a state $s \\in S$ such that the former inequality holds, there is a state $s \\in S$ such that the latter inequality also holds."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:30.557947Z",
     "start_time": "2025-06-01T13:38:30.555566Z"
    }
   },
   "source": [
    "def policy_le(v_p1, q_p1, p2):\n",
    "    return np.all(v_p1 <= (np.array([[1,1,0,0], [0,0,1,1]]) @ (p2 * q_p1)))"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:30.930704Z",
     "start_time": "2025-06-01T13:38:30.928541Z"
    }
   },
   "source": [
    "pi2 = np.array([0,1,0,1])"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:31.165701Z",
     "start_time": "2025-06-01T13:38:31.162732Z"
    }
   },
   "source": [
    "policy_le(v_pi1, q_pi1, pi2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if a Policy is Optimal\n",
    "- For each state-action pair $s \\in S$ and $a \\in A(s)$: if $\\pi(a\\mid s) > 0$ and $q_\\pi(s, a) < \\max_{a' \\in A}q_\\pi(s, a')$, then the policy is not optimal.\n",
    "- Otherwise, policy $\\pi$ is optimal and cannot be further improved."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:31.517867Z",
     "start_time": "2025-06-01T13:38:31.515324Z"
    }
   },
   "source": [
    "def is_policy_optimal(pi, q_pi):\n",
    "    m = int(len(q_pi) ** (1/2))\n",
    "    mask = (pi.reshape(m,m) > 0) & (q_pi.reshape(m,m) < np.max(q_pi.reshape(m,m), axis=1).reshape(-1,1))\n",
    "    return not np.any(mask)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:31.740347Z",
     "start_time": "2025-06-01T13:38:31.738034Z"
    }
   },
   "source": [
    "is_policy_optimal(pi, q_pi1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:31.958374Z",
     "start_time": "2025-06-01T13:38:31.955604Z"
    }
   },
   "source": [
    "p_pi2_sp_given_s = get_p_pi_sp_given_s(pi2,p_sp_given_s_a)\n",
    "r_pi2 = get_r_pi(pi2, r) \n",
    "v_pi_det, q_pi_det = get_vq1(p_pi2_sp_given_s, p_sp_given_s_a, gamma, r_pi2, r)\n",
    "v_pi_det, q_pi_det"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.18181818, 5.        ]),\n",
       " array([0.54545455, 3.18181818, 1.90909091, 5.        ]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:32.145300Z",
     "start_time": "2025-06-01T13:38:32.142334Z"
    }
   },
   "source": [
    "is_policy_optimal(pi2, q_pi_det)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement Algorithm\n",
    "- For each state $s \\in S$, set $\\pi'(s) \\leftarrow \\arg\\max_{a \\in A}q_\\pi(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:32.551044Z",
     "start_time": "2025-06-01T13:38:32.548610Z"
    }
   },
   "source": [
    "def improve_policy(pi, q_pi):\n",
    "    pi_matrix = pi.reshape(2,2)\n",
    "    q_pi_matrix = q_pi.reshape(2,2)\n",
    "    new_pi_matrix = np.zeros(pi_matrix.shape)\n",
    "    indices = np.argmax(q_pi_matrix, axis=1)\n",
    "    new_pi_matrix[:, indices] = 1\n",
    "    new_pi = new_pi_matrix.reshape(-1)\n",
    "    return new_pi"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:32.783264Z",
     "start_time": "2025-06-01T13:38:32.781198Z"
    }
   },
   "source": [
    "pi_improved = improve_policy(pi, q_pi1)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:32.971923Z",
     "start_time": "2025-06-01T13:38:32.969069Z"
    }
   },
   "source": [
    "pi_improved"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitation Frequency\n",
    "Determine how many times a state or state-action pair will be visited, with weights - **discounted visitation frequency** a.k.a. **discounted distribution**.\n",
    "\n",
    "Recall $T$ is a random variable denoting the stop time.\n",
    "\n",
    "### Discounted State Visitation Frequency (Discounted State Distribution)\n",
    "Episodic Task:\n",
    "$$\n",
    "\\eta_\\pi(s) = \\sum_{t = 0}^\\infty \\mathrm{Pr}_\\pi[T = t]\\sum_{\\tau = 0}^{t-1}\\gamma^\\tau \\mathrm{Pr}_\\pi[S_\\tau = s]\n",
    "$$\n",
    "Sequential Task:\n",
    "$$\n",
    "\\eta_\\pi(s) = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau \\mathrm{Pr}_\\pi[S_\\tau = s]\n",
    "$$\n",
    "\n",
    "### Discounted State-Action Visitation Frequency (Discounted State-Action Distribution)\n",
    "Episodic Task:\n",
    "$$\n",
    "\\rho_\\pi(s,a) = \\sum_{t = 0}^\\infty\\mathrm{Pr}_\\pi[T = t]\\sum_{\\tau = 0}^{t - 1}\\gamma^\\tau \\mathrm{Pr}_\\pi[S_\\tau = s, A_\\tau = a]\n",
    "$$\n",
    "Sequential Task:\n",
    "$$\n",
    "\\rho_\\pi(s,a) = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau \\mathrm{Pr}_\\pi[S_\\tau = s, A_\\tau = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties\n",
    "Episodic Task:\n",
    "$$\n",
    "\\sum_{s \\in S}\\eta_\\pi(s) = \\sum_{s \\in S, a \\in A(s)}\\rho_\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\frac{1 - \\gamma^T}{1 - \\gamma}\\right]\n",
    "$$\n",
    "Discounted Visitation/Discounted Distribution may not be a probability distribution.\n",
    "\n",
    "Sequential Task:\n",
    "$$\n",
    "\\sum_{s \\in S}\\eta_\\pi(s) = \\sum_{s \\in S, a \\in A(s)}\\rho_\\pi(s,a) = \\frac{1}{1 - \\gamma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed and Full Example\n",
    "The environment is a sequential task.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta_\\pi(\\mathrm{hungry}) &= \\sum_{\\tau = 0}^{\\infty}\\gamma^\\tau\\mathrm{Pr}_\\pi[S_\\tau = \\mathrm{hungry}]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Calculation of $\\eta_\\pi$ - Directly from Definition\n",
    "We can write\n",
    "```math\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:34.040192Z",
     "start_time": "2025-06-01T13:38:34.038003Z"
    }
   },
   "source": [
    "def get_p_pi_s(p_s0, tau):\n",
    "    return (np.linalg.matrix_power(p_pi_sp_given_s, tau) @ p_S0.reshape(-1,1)).reshape(-1)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:34.460507Z",
     "start_time": "2025-06-01T13:38:34.280152Z"
    }
   },
   "source": [
    "eta_pi = np.sum(np.array([gamma ** tau * get_p_pi_s(p_S0, tau) for tau in range(10000)]), axis=0)\n",
    "eta_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Calculation of $\\rho_\\pi$ - Directly from Definition"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:34.875688Z",
     "start_time": "2025-06-01T13:38:34.682347Z"
    }
   },
   "source": [
    "rho_pi = np.sum(np.array([gamma ** tau * pi.reshape(2,2) * np.repeat(get_p_pi_s(p_S0, tau).reshape(-1,1), 2, axis=1) for tau in range(10000)]), axis=0).reshape(-1)\n",
    "rho_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Properties\n",
    "Using discounted state-action visitation frequency to back up discounted state visitation frequency.\n",
    "$$\n",
    "\\eta_\\pi(s) = \\sum_{a \\in A(s)}\\rho_\\pi(s,a)\n",
    "$$\n",
    "Vectorised:\n",
    "$$\n",
    "\\vec{\\eta}_\\pi = \\mathbf{1}_{S \\mid S, A}\\vec{\\rho}_\\pi\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbf{1}_{S \\mid S, A} = (1: s \\in S, (s,a) \\in S \\times A) \\in \\{1\\}^{|S| \\times |S||A|}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:35.142035Z",
     "start_time": "2025-06-01T13:38:35.139433Z"
    }
   },
   "source": [
    "eta_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:35.341043Z",
     "start_time": "2025-06-01T13:38:35.338465Z"
    }
   },
   "source": [
    "np.sum(rho_pi.reshape(2,2), axis=1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use discounted state visitation frequency and poliicy to back up discounted state-action visitation frequency.\n",
    "$$\n",
    "\\rho_\\pi(s,a) = \\eta_\\pi(s)\\pi(a\\mid s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:35.730763Z",
     "start_time": "2025-06-01T13:38:35.727960Z"
    }
   },
   "source": [
    "rho_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:35.966327Z",
     "start_time": "2025-06-01T13:38:35.963304Z"
    }
   },
   "source": [
    "(np.repeat(eta_pi.reshape(-1,1), 2, axis=1) * pi.reshape(2,2)).reshape(-1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using discounted state-action visitation frequency and dynamics to back up discounted state visitation frequency.\n",
    "$$\n",
    "\\eta_\\pi(s') = p_{S_0}(s') + \\gamma \\sum_{s \\in S, a \\in A(s)}p(s'\\mid s, a) \\rho_\\pi(s, a)\n",
    "$$\n",
    "Vectorised:\n",
    "$$\n",
    "\\vec{\\eta}_\\pi = \\vec{p}_{S_0} + \\gamma P_{S_{t+1}\\mid S_t, A_t}\\vec{\\rho}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:36.388971Z",
     "start_time": "2025-06-01T13:38:36.386269Z"
    }
   },
   "source": [
    "eta_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:36.618432Z",
     "start_time": "2025-06-01T13:38:36.615807Z"
    }
   },
   "source": [
    "(p_S0 + gamma * p_sp_given_s_a @ rho_pi.reshape(-1,1))[:,0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equations\n",
    "Use discounted state distribution at time $t$ to back up discounted state distribution at time $t+1$:\n",
    "$$\n",
    "\\eta_\\pi(s') = p_{S_0}(s') + \\gamma\\sum_{s \\in S}p_\\pi(s'\\mid s)\\eta_\\pi(s)\n",
    "$$\n",
    "Vectorised:\n",
    "$$\n",
    "\\vec{\\eta}_\\pi = \\vec{p}_{S_0} + \\gamma P_{S_{t+1}\\mid S_t; \\pi}\\vec{\\eta}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:37.020015Z",
     "start_time": "2025-06-01T13:38:37.017381Z"
    }
   },
   "source": [
    "eta_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:37.258667Z",
     "start_time": "2025-06-01T13:38:37.255630Z"
    }
   },
   "source": [
    "p_S0 + gamma * p_pi_sp_given_s @ eta_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use discounted state-action distribution at time $t$ to back up the discounted state-action distribution at time $t+1$.\n",
    "$$\n",
    "\\rho_\\pi(s', a') = p_{S_0, A_0; \\pi}(s', a') + \\gamma\\sum_{s\\in S, a \\in A(s)}p_\\pi(s', a'\\mid s, a)\\rho_\\pi(s,a)\n",
    "$$\n",
    "Vectorised:\n",
    "$$\n",
    "\\vec{\\rho}_\\pi = \\vec{p}_{S_0, A_0;\\pi} + \\gamma P_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi}\\vec{\\rho}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:37.668102Z",
     "start_time": "2025-06-01T13:38:37.665633Z"
    }
   },
   "source": [
    "rho_pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:37.893629Z",
     "start_time": "2025-06-01T13:38:37.890907Z"
    }
   },
   "source": [
    "np.diag(p_S0_A0_pi.reshape(-1) + gamma * p_pi_sp_ap_given_s_a @ rho_pi.reshape(-1,1))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Visitation Frequency\n",
    "The following methods allow the computation of $\\vec{\\rho}_\\pi$ and $\\vec{eta}_\\pi$ using purely linear algebra - no loops, and more pure exactness.\n",
    "### Approach 1\n",
    "1. Rearrange $\\vec{\\eta}_\\pi$ Bellman Expectation Equation into $\\vec{\\eta}_\\pi = (I - \\gamma P_{S_{t+1}\\mid S_t})^{-1}\\vec{p}_{S_0}$ to obtain $\\vec{\\eta}_\\pi$.\n",
    "2. Use $\\rho_\\pi(s,a) = \\eta_\\pi(s)\\pi(a\\mid s)$ to obtain $\\vec{\\rho}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:38.322919Z",
     "start_time": "2025-06-01T13:38:38.320224Z"
    }
   },
   "source": [
    "def get_eta_rho1(gamma, p_pi_sp_given_s, p_S0, pi):\n",
    "    eta_pi = np.linalg.inv(np.eye(2) - gamma * p_pi_sp_given_s) @ p_S0\n",
    "    rho_pi = eta_pi.reshape(-1,1) * pi.reshape(2,2)\n",
    "    return eta_pi, rho_pi.reshape(-1)"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:38.576006Z",
     "start_time": "2025-06-01T13:38:38.573269Z"
    }
   },
   "source": [
    "get_eta_rho1(gamma, p_pi_sp_given_s, p_S0, pi)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.72727273, 2.27272727]),\n",
       " array([0.68181818, 2.04545455, 1.89393939, 0.37878788]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "1. Rearrange $\\vec{\\rho}_\\pi$ Bellman Expectation Equation into $\\vec{\\rho}_\\pi = (I - \\gamma P_{S_{t + 1}, A_{t + 1}\\mid S_t, A_t; \\pi})^{-1}\\vec{p}_{S_0, A_0; \\pi}$.\n",
    "2. Use $\\vec{\\eta}_\\pi = I_{S_{t+1}\\mid S_{t}, A_t}\\vec{\\rho}_\\pi$ to obtain $\\vec{\\eta}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:38.984163Z",
     "start_time": "2025-06-01T13:38:38.982109Z"
    }
   },
   "source": [
    "def get_eta_rho2(gamma, p_pi_sp_ap_given_s_a, p_S0_A0_pi):\n",
    "    rho_pi = np.linalg.inv(np.eye(4) - gamma * p_pi_sp_ap_given_s_a) @ p_S0_A0_pi.reshape(-1,1)\n",
    "    eta_pi = np.sum(rho_pi.reshape(2,2), axis=1)\n",
    "    return eta_pi, rho_pi.reshape(-1)"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:39.245007Z",
     "start_time": "2025-06-01T13:38:39.241320Z"
    }
   },
   "source": [
    "get_eta_rho2(gamma, p_pi_sp_ap_given_s_a, p_S0_A0_pi)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.72727273, 2.27272727]),\n",
       " array([0.68181818, 2.04545455, 1.89393939, 0.37878788]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalence between Visitation Frequency and Policy\n",
    "We know that discounted visitation frequencies of any $\\pi$ satisfy all the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta_\\pi(s') &= p_{S_0}(s') + \\gamma\\sum_{s \\in S, a \\in A(s)}p(s'\\mid s, a)\\rho_\\pi(s, a)\\\\\n",
    "\\eta_\\pi(s) &= \\sum_{a \\in A(s)}\\rho_\\pi(s,a)\\\\\n",
    "\\rho_\\pi(s,a) &\\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "Notice that these equations don't directly use $\\pi$. Furthermore, the solution of the above system is bijective with the policy. Therefore, if we find a solution $\\vec{\\eta}$ and $\\vec{\\rho}$ that satisfies the equation set, then we can define a policy as follows:\n",
    "$$\n",
    "\\pi(a\\mid s) = \\frac{\\rho(s, a)}{\\eta(s)}\n",
    "$$\n",
    "and the policy satisfies:\n",
    "1. $\\eta_\\pi(s) = \\eta(s)$, and\n",
    "2. $\\rho_\\pi(s, a) = \\rho(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:39.680521Z",
     "start_time": "2025-06-01T13:38:39.677918Z"
    }
   },
   "source": [
    "pi"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:39.917123Z",
     "start_time": "2025-06-01T13:38:39.914105Z"
    }
   },
   "source": [
    "eta, rho = get_eta_rho1(gamma, p_pi_sp_given_s, p_S0, pi)\n",
    "(rho.reshape(2,2) / eta.reshape(-1,1)).reshape(-1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation over visitation Frequency\n",
    "Given deterministic function $f$, can determine expectation over discounted distribution:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}_{S\\sim \\eta_\\pi}[f(S)] &= \\sum_{s \\in S}\\eta_\\pi(s)f(s)\\\\\n",
    "    \\mathbb{E}_{(S, A)\\sim \\rho_\\pi}[f(S,A)] &= \\sum_{s\\in S, a\\in A(s)}\\rho_\\pi(s,a)f(s,a)\n",
    "\\end{align*}\n",
    "$$\n",
    "### Example\n",
    "Expected discounted return at $t = 0$ can be written as\n",
    "$$\n",
    "g_\\pi = \\mathbb{E}_{(S,A)\\sim \\rho_\\pi}[r(S,A)]\n",
    "$$\n",
    "Due to this property, we can write\n",
    "$$\n",
    "g_\\pi = \\vec{r}^\\top\\vec{\\rho}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:40.366171Z",
     "start_time": "2025-06-01T13:38:40.363516Z"
    }
   },
   "source": "g_pi",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-3.5606060606060614)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:40.570990Z",
     "start_time": "2025-06-01T13:38:40.568578Z"
    }
   },
   "source": [
    "(r @ rho_pi.reshape(-1,1))[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-3.5606060606060597)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Properties of Optimal Values\n",
    "Use optimal action values at time $t$ to back up optimal state values at time $t$.\n",
    "$$v_*(s) = \\max_{a \\in A}q_*(s,a)$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use optimal state values at time $t+1$ to back up optimal action values at time $t$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_*(s,a) &= r(s,a) + \\gamma\\sum_{s'\\in S}p(s'\\mid s, a)v_*(s')\\\\\n",
    "&= \\sum_{s' \\in S^+, r\\in R}p(s', r\\mid s, a)[r + \\gamma v_*(s')]\n",
    "\\end{align*}$$\n",
    "In other words\n",
    "$$q_*(S_t, A_t) = \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1})]$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bellman Optimal Equations (BOEs)\n",
    "Using optimal state values at time $t+1$ to back up optimal state values at time $t$.\n",
    "$$v_*(s) = \\max_{a \\in A}\\left[r(s,a) + \\gamma\\sum_{s'\\in S}p(s'\\mid s, a)v_*(s')\\right]$$\n",
    "In other words:\n",
    "$$v_*(S_t) = \\max_{a \\in A}\\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1})]$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Using optimal action values at time $t+1$ to back up optimal action values at time $t$:\n",
    "$$q_*(s,a) = r(s,a) + \\gamma \\sum_{s'\\in S}p(s'\\mid s, a) \\max_{a'\\in A} q_*(s', a')$$\n",
    "In other words:\n",
    "$$q_*(S_t, A_t) = \\mathbb{E}\\left[R_{t+1} + \\gamma \\max_{a'\\in A} q_*(S_{t+1}, a')\\right]$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feed and Full Example\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_*(\\text{hungry}) &= \\max\\{q_*(\\text{hungry}, \\text{ignore}), q_*(\\text{hungry}, \\text{feed})\\}\\\\\n",
    "v_*(\\text{full}) &= \\max\\{q_*(\\text{full}, \\text{ignore}), q_*(\\text{full}, \\text{feed})\\}\\\\\n",
    "q_*(\\text{hungry}, \\text{ignore}) &= -2 + \\frac{4}{5}(1v_*(\\text{hungry}) + 0v_*(\\text{full}))\\\\\n",
    "q_*(\\text{hungry}, \\text{feed}) &= -\\frac{1}{3} + \\frac{4}{5}\\left(\\frac{1}{3}v_*(\\text{hungry}) + \\frac{2}{3}v_*(\\text{full})\\right)\\\\\n",
    "q_*(\\text{full}, \\text{ignore}) &= -1 + \\frac{4}{5}\\left(\\frac{3}{4}v_*(\\text{hungry}) + \\frac{1}{4}v_*(\\text{full})\\right)\\\\\n",
    "q_*(\\text{full}, \\text{feed}) &= 1 + \\frac{4}{5}(0v_*(\\text{hungry}) + 1v_*(\\text{full}))\n",
    "\\end{align*}\n",
    "$$\n",
    "In the code below we will consider case I from the book, where $q_*(\\text{hungry},\\text{ignore}) > q_*(\\text{hungry},\\text{feed})$ and $q_*(\\text{full}, \\text{ignore}) > q_*(\\text{full}, \\text{feed})$."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:38:42.464516Z",
     "start_time": "2025-06-01T13:38:42.461490Z"
    }
   },
   "cell_type": "code",
   "source": "r\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.        , -0.33333333, -1.        ,  1.        ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T13:56:09.140747Z",
     "start_time": "2025-06-01T13:56:09.137023Z"
    }
   },
   "cell_type": "code",
   "source": "p_sp_given_s_a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:51:47.169637Z",
     "start_time": "2025-06-01T15:51:47.166284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_g_star_q_star(idx1, idx2, p_sp_given_s_a, r, gamma):\n",
    "    assert idx1 in (0,1)\n",
    "    assert idx2 in (2,3)\n",
    "\n",
    "    v_star = (np.linalg.inv(np.eye(2) - gamma * p_sp_given_s_a[:, [idx1,idx2]].T) @ r[[idx1,idx2]].reshape(-1,1)).reshape(-1)\n",
    "    q_star = np.zeros(4)\n",
    "    q_star[[idx1,idx2]] = v_star\n",
    "\n",
    "    idx3 = 1 - idx1\n",
    "    idx4 = 5 - idx2\n",
    "    q_star[[idx3, idx4]] = (r[[idx3,idx4]].reshape(-1,1) + gamma * p_sp_given_s_a[:, [idx3,idx4]].T @ v_star.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "    valid = q_star[idx1] > q_star[idx3] and q_star[idx2] > q_star[idx4]\n",
    "\n",
    "    return v_star, q_star, valid"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:51:54.497096Z",
     "start_time": "2025-06-01T15:51:54.494029Z"
    }
   },
   "cell_type": "code",
   "source": "get_g_star_q_star(0, 2, p_sp_given_s_a, r, gamma)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-10.  ,  -8.75]),\n",
       " array([-10.        ,  -7.66666667,  -8.75      ,  -6.        ]),\n",
       " np.False_)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:52:10.635301Z",
     "start_time": "2025-06-01T15:52:10.631599Z"
    }
   },
   "cell_type": "code",
   "source": "get_g_star_q_star(1, 2, p_sp_given_s_a, r, gamma)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3. , -3.5]), array([-4.4, -3. , -3.5, -1.8]), np.False_)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:52:24.059048Z",
     "start_time": "2025-06-01T15:52:24.055018Z"
    }
   },
   "cell_type": "code",
   "source": "get_g_star_q_star(0, 3, p_sp_given_s_a, r, gamma)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-10.,   5.]),\n",
       " array([-10.        ,  -0.33333333,  -6.        ,   5.        ]),\n",
       " np.False_)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:52:28.719072Z",
     "start_time": "2025-06-01T15:52:28.716250Z"
    }
   },
   "cell_type": "code",
   "source": "get_g_star_q_star(1, 3, p_sp_given_s_a, r, gamma)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.18181818, 5.        ]),\n",
       " array([0.54545455, 3.18181818, 1.90909091, 5.        ]),\n",
       " np.True_)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
