{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:34.321789Z",
     "start_time": "2025-06-02T13:33:33.845564Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State space $S = \\{\\text{hungry}, \\text{full}\\}$.\n",
    "\n",
    "Action space $A = \\{\\text{ignore}, \\text{feed}\\}$.\n",
    "\n",
    "Initial state distribution $p_{S_0} = \\begin{bmatrix}1/2 & 1/2\\end{bmatrix}^T$.\n",
    "\n",
    "Rewards = $\\{-3, -2, 1, 2\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:39.887341Z",
     "start_time": "2025-06-02T13:33:39.884954Z"
    }
   },
   "outputs": [],
   "source": [
    "p_S0 = np.array([1/2, 1/2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "$$p(s',r\\mid s, a) = \\mathrm{Pr}[S_{t+1} = s', R_{t+1}=r\\mid S_t = s, A_t = a]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:40.128285Z",
     "start_time": "2025-06-02T13:33:40.126338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dynamics as a matrix\n",
    "dynamics_matrix = [[0, 1/3,   0,   0], # (hungry, -3)\n",
    "                   [1,   0, 3/4,   0], # (hungry, -2)\n",
    "                   [0, 2/3,   0,   1], # (full, 1)\n",
    "                   [0,   0, 1/4,   0], # (full, 2)\n",
    "                   ]\n",
    "dynamics_matrix = np.array(dynamics_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability Matrix\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(s'\\mid s,a) &= \\mathrm{Pr}\\left[S_{t+1} = s' \\mid S_t = s, A_t = a\\right]\\\\\n",
    "&= \\sum_{r \\in R} p(s', r \\mid s, a)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:40.334533Z",
     "start_time": "2025-06-02T13:33:40.331754Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_p_sp_given_s_a(dynamics_matrix):\n",
    "    p_sp_given_s_a = np.array([[1,1,0,0], [0,0,1,1]]) @ dynamics_matrix\n",
    "    return p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:40.449900Z",
     "start_time": "2025-06-02T13:33:40.445607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a = get_p_sp_given_s_a(dynamics_matrix)\n",
    "p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Reward given State-Action Pair\n",
    "$$\n",
    "\\begin{align*}\n",
    "r(s,a) &= \\mathbb{E}[R_{t + 1} \\mid S_t = s, A_t = a]\\\\\n",
    "&= \\sum_{s' \\in S, r \\in R} r \\cdot p(s', r \\mid s, a)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:41.017105Z",
     "start_time": "2025-06-02T13:33:41.015207Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_r(dynamics_matrix):\n",
    "    r = np.sum(dynamics_matrix * np.array([[-3], [-2], [1], [2]]), axis=0) # Keep in vector form just in case.\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:41.131466Z",
     "start_time": "2025-06-02T13:33:41.128878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.        , -0.33333333, -1.        ,  1.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = get_r(dynamics_matrix)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Reward given State, Action and Next State.\n",
    "$$\n",
    "\\begin{align*}\n",
    "r(s,a,s') &= \\mathbb{E}[R_{t + 1} \\mid S_t = s, A_t = a, S_{t + 1} = s']\\\\\n",
    "&= \\frac{\\sum_{r \\in R}r \\cdot p(s', r \\mid s, a)}{p(s'\\mid s, a)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:41.199982Z",
     "start_time": "2025-06-02T13:33:41.197616Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_r_sp(dynamics_matrix, p_sp_given_s_a):\n",
    "    r_sp = np.array([[1,1,0,0],[0,0,1,1]]) @ (dynamics_matrix * np.array([[-3], [-2], [1], [2]]))\n",
    "    r_sp = np.nan_to_num(r_sp / p_sp_given_s_a)\n",
    "    return r_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:41.261610Z",
     "start_time": "2025-06-02T13:33:41.258462Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\_Kookie\\AppData\\Local\\Temp\\ipykernel_18720\\1014741899.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  r_sp = np.nan_to_num(r_sp / p_sp_given_s_a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2., -3., -2.,  0.],\n",
       "       [ 0.,  1.,  2.,  1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_sp = get_r_sp(dynamics_matrix, p_sp_given_s_a)\n",
    "r_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:41.391657Z",
     "start_time": "2025-06-02T13:33:41.388728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = np.array([1/4, 3/4, 5/6, 1/6])\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial State-Action Distribution, According to $\\pi$\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_{S_0, A_0, \\pi}(s, a) &= \\mathrm{Pr}_\\pi[S_0 = s, A_0 = a]\\\\\n",
    "&= p_{S_0}(s)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:41.592931Z",
     "start_time": "2025-06-02T13:33:41.590422Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_p_S0_A0_pi(pi, p_S0):\n",
    "    pi_matrix = pi.reshape(2,2)\n",
    "    p_S0_A0_pi = p_S0.reshape(-1,1) * pi_matrix\n",
    "    return p_S0_A0_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:41.703786Z",
     "start_time": "2025-06-02T13:33:41.701151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.125     , 0.375     ],\n",
       "       [0.41666667, 0.08333333]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_S0_A0_pi = get_p_S0_A0_pi(pi, p_S0)\n",
    "p_S0_A0_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability between States, According to $\\pi$\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_\\pi(s'\\mid s) &= \\mathrm{Pr}_\\pi[S_{t + 1} = s'\\mid S_t = s]\\\\\n",
    "&= \\sum_{a \\in A}p(s'\\mid s, a)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:41.890622Z",
     "start_time": "2025-06-02T13:33:41.887965Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_p_pi_sp_given_s(pi, p_sp_given_s_a):\n",
    "    p_pi_sp_given_s = (p_sp_given_s_a * pi) @ np.array([[1,1,0,0], [0,0,1,1]]).T\n",
    "    return p_pi_sp_given_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:42.018794Z",
     "start_time": "2025-06-02T13:33:42.015870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.625],\n",
       "       [0.5  , 0.375]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pi_sp_given_s = get_p_pi_sp_given_s(pi, p_sp_given_s_a)\n",
    "p_pi_sp_given_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability between State-Action Pairs, According to $\\pi$\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_\\pi(s', a'\\mid s, a) &= \\mathrm{Pr}_\\pi[S_{t + 1} = s', A_{t + 1} = a' \\mid S_t = s, A_t = a]\\\\\n",
    "&= \\pi(a' \\mid s')p(s'\\mid s, a)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:42.210198Z",
     "start_time": "2025-06-02T13:33:42.208003Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_p_pi_sp_given_s_a(pi, p_sp_given_s_a):\n",
    "    p_pi_sp_ap_given_s_a = pi.reshape(-1,1) * np.repeat(p_sp_given_s_a,2, axis=0)\n",
    "    return p_pi_sp_ap_given_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:42.302359Z",
     "start_time": "2025-06-02T13:33:42.299795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 0.08333333, 0.1875    , 0.        ],\n",
       "       [0.75      , 0.25      , 0.5625    , 0.        ],\n",
       "       [0.        , 0.55555556, 0.20833333, 0.83333333],\n",
       "       [0.        , 0.11111111, 0.04166667, 0.16666667]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pi_sp_ap_given_s_a = get_p_pi_sp_given_s_a(pi, p_sp_given_s_a)\n",
    "p_pi_sp_ap_given_s_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected State Reward, According to $\\pi$\n",
    "$$\n",
    "\\begin{align*}\n",
    "r_\\pi(s) &= \\mathbb{E}_\\pi[R_{t+1}\\mid S_t = s]\\\\\n",
    "&= \\sum_{a \\in A}r(s,a)\\pi(a\\mid s)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:42.516989Z",
     "start_time": "2025-06-02T13:33:42.514513Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_r_pi(pi, r):\n",
    "    pi_matrix = pi.reshape(2,2)\n",
    "    r_pi = np.diag(r.reshape(2,2) @ pi_matrix.T)\n",
    "    return r_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:42.632508Z",
     "start_time": "2025-06-02T13:33:42.629851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.75      , -0.66666667])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_pi = get_r_pi(pi, r)\n",
    "r_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Return\n",
    "Episodic task without discount.\n",
    "$$\n",
    "G_t = \\sum_{\\tau = t+1}^TR_{\\tau}\n",
    "$$\n",
    "Episodic task with discount $\\gamma \\in [0,1]$:\n",
    "$$\n",
    "G_t = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau R_{t+\\tau+1}\n",
    "$$\n",
    "Recursive Relationship:\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma G_{t + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value\n",
    "State Value - expected return starting from $s$, then following policy $\\pi$.\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t\\mid S_t = s]\n",
    "$$\n",
    "Action Value - expected return starting from $(s,a)$, then following $\\pi$.\n",
    "$$\n",
    "q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t\\mid S_t = s, A_t = a]\n",
    "$$\n",
    "### Properties of Value\n",
    "Using action-value pairs to back up state values\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in A}\\pi(a\\mid s)q_\\pi(s,a)\n",
    "$$\n",
    "In other words: $v_\\pi(S_t) = \\mathbb{E}_\\pi[q_\\pi(S_t,A_t)]$.\n",
    "\n",
    "Vector form:\n",
    "$$\n",
    "\\vec{v}_\\pi = \\vec{\\pi} \\odot \\vec{q}_\\pi\n",
    "$$\n",
    "Using state values to back up action values:\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= r(s,a) + \\gamma\\sum_{s' \\in S}p(s'\\mid s,a)v_\\pi(s')\\\\\n",
    "&= \\sum_{s' \\in S^+, r \\in R}p(s',r\\mid s,a)[r + \\gamma v_\\pi(s')]\n",
    "\\end{align*}\n",
    "$$\n",
    "In other words, $q_\\pi(S_t,A_t) = \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1})]$.\n",
    "\n",
    "Vector form:\n",
    "$$\n",
    "\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}\\mid S_t,A_t}\\vec{v}_\\pi\n",
    "$$\n",
    "- $\\vec{v}_\\pi = (v_\\pi(s): s \\in S)^\\top \\in \\mathbb{R}^{|S|}$\n",
    "- $\\vec{q}_\\pi = (q_\\pi(s,a):(s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $\\vec{\\pi} = (\\pi(a\\mid s): (s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $\\vec{r} = (r(s,a) : (s,a) \\in S \\times A)^\\top \\in \\mathbb{R}^{|S||A|}$\n",
    "- $P_{S_{t+1}\\mid S_t, A_t} = (p(s'\\mid s,a): s' \\in S, (s,a) \\in S \\times A) \\in [0,1]^{|S| \\times |S||A|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:42.959387Z",
     "start_time": "2025-06-02T13:33:42.956942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4), (4,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a.shape, r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equations\n",
    "The two relations above combine to form the two relations below.\n",
    "\n",
    "### Use state values at time $t+1$ to back up the state values at time $t$\n",
    "$$\n",
    "v_\\pi(s) = r_\\pi(s) + \\gamma \\sum_{s' \\in S}p_\\pi(s'\\mid s)v_\\pi(s')\n",
    "$$\n",
    "In other words: $v_\\pi(S_t) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})]$.\n",
    "\n",
    "Vector form:\n",
    "$$\n",
    "\\vec{v}_\\pi = \\vec{r}_\\pi + \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi}\\vec{v}_\\pi\n",
    "$$\n",
    "where $\\vec{r}_\\pi = (r_\\pi(s): s\\in S)^\\top \\in \\mathbb{R}^{|S|}$.\n",
    "\n",
    "### Use action values at time $t+1$ to represent the action values at time $t$\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= r(s,a) + \\gamma \\sum_{s'\\in S, a'\\in A}p_\\pi(s', a'\\mid s, a)q_\\pi(s',a')\\\\\n",
    "&= \\sum_{s'\\in S, r\\in R}p(s', r\\mid s, a)\\left[r + \\gamma\\sum_{a'\\in A}\\pi(a'\\mid s')q_\\pi(s', a')\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "In other words, $q_\\pi(S_t, A_t) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1})]$\n",
    "\n",
    "Vector form:\n",
    "$$\n",
    "\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi} \\vec{q}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranged relations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{v}_\\pi &= (I - \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi})^{-1}\\vec{r}_\\pi\\\\\n",
    "\\vec{q}_\\pi &= (I - \\gamma P^\\top_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi})^{-1}\\vec{r}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1\n",
    "1. Use $\\vec{v}_\\pi = (I - \\gamma P^\\top_{S_{t+1}\\mid S_t;\\pi})^{-1}\\vec{r}_\\pi$ to obtain $\\vec{v}_\\pi$.\n",
    "2. Use $\\vec{q}_\\pi = \\vec{r} + \\gamma P^\\top_{S_{t+1}\\mid S_t, A_t; \\pi}\\vec{v}_\\pi$ to obtain $\\vec{q}_\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:43.495288Z",
     "start_time": "2025-06-02T13:33:43.492405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feed and Full Example\n",
    "def get_vq1(p_pi_sp_given_s, p_sp_given_s_a, gamma, r_pi, r):\n",
    "    v_pi = np.linalg.inv(np.eye(2) - gamma * p_pi_sp_given_s.T) @ r_pi\n",
    "    q_pi = r + (gamma * p_sp_given_s_a.T) @ v_pi\n",
    "    return v_pi, q_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:43.603607Z",
     "start_time": "2025-06-02T13:33:43.599762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.59848485, -3.52272727]),\n",
       " array([-4.87878788, -3.17171717, -3.86363636, -1.81818182]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 4/5\n",
    "v_pi1, q_pi1 = get_vq1(p_pi_sp_given_s, p_sp_given_s_a, gamma, r_pi, r)\n",
    "v_pi1, q_pi1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "1. Use $\\vec{q}_\\pi = (I - \\gamma P^\\top_{S_{t+1}\\mid S_t, A_t;\\pi})^{-1}\\vec{r}$ to obtain $\\vec{q}_\\pi$.\n",
    "2. Use $\\vec{v}_\\pi = \\vec{\\pi} \\odot \\vec{q}_\\pi$ to obtain $\\vec{v}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:43.855513Z",
     "start_time": "2025-06-02T13:33:43.852734Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vq2(p_pi_sp_given_s, p_sp_given_s_a, gamma, pi, r):\n",
    "    q_pi = np.linalg.inv(np.eye(4) - gamma * p_pi_sp_ap_given_s_a.T) @ r\n",
    "    v_pi = np.array([[1,1,0,0],[0,0,1,1]]) @ (pi * q_pi)\n",
    "    return v_pi, q_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:43.961414Z",
     "start_time": "2025-06-02T13:33:43.958152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.59848485, -3.52272727]),\n",
       " array([-4.87878788, -3.17171717, -3.86363636, -1.81818182]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_pi2, q_pi2 = get_vq2(p_pi_sp_given_s, p_sp_given_s_a, gamma, pi, r)\n",
    "v_pi2, q_pi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that both approaches achieve the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Expected Returns using Values\n",
    "Expected return at $t = 0$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_\\pi &= \\mathbb{E}_{S_0 \\sim p_{S_0}}[v_\\pi(S_0)]\\\\\n",
    "&= \\vec{p}_{S_0}^\\top \\vec{v}_\\pi\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:44.399021Z",
     "start_time": "2025-06-02T13:33:44.397056Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_g_pi(p_S0, v_pi):\n",
    "    g_pi = p_S0 @ v_pi.reshape(-1,1)\n",
    "    return g_pi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:44.508614Z",
     "start_time": "2025-06-02T13:33:44.505808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-3.5606060606060614)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_pi = get_g_pi(p_S0, v_pi1)\n",
    "g_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement Theorem\n",
    "If for all $s \\in S$,\n",
    "$$\n",
    "v_\\pi(s) \\leq \\sum_a \\pi'(a\\mid s)q_\\pi(s, a) = \\mathbb{E}_{A\\sim \\pi'(s)}[q_\\pi(s,A)]\n",
    "$$\n",
    "then $\\pi \\preccurlyeq \\pi'$. I.e: $v_\\pi(s) \\leq v_{\\pi'}(s)$.\n",
    "\n",
    "Additionally, if there is a state $s \\in S$ such that the former inequality holds, there is a state $s \\in S$ such that the latter inequality also holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:44.710173Z",
     "start_time": "2025-06-02T13:33:44.707616Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_le(v_p1, q_p1, p2):\n",
    "    return np.all(v_p1 <= (np.array([[1,1,0,0], [0,0,1,1]]) @ (p2 * q_p1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:44.855454Z",
     "start_time": "2025-06-02T13:33:44.852988Z"
    }
   },
   "outputs": [],
   "source": [
    "pi2 = np.array([0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:44.959930Z",
     "start_time": "2025-06-02T13:33:44.957177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_le(v_pi1, q_pi1, pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if a Policy is Optimal\n",
    "- For each state-action pair $s \\in S$ and $a \\in A(s)$: if $\\pi(a\\mid s) > 0$ and $q_\\pi(s, a) < \\max_{a' \\in A}q_\\pi(s, a')$, then the policy is not optimal.\n",
    "- Otherwise, policy $\\pi$ is optimal and cannot be further improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:45.278451Z",
     "start_time": "2025-06-02T13:33:45.275267Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_policy_optimal(pi, q_pi):\n",
    "    m = int(len(q_pi) ** (1/2))\n",
    "    mask = (pi.reshape(m,m) > 0) & (q_pi.reshape(m,m) < np.max(q_pi.reshape(m,m), axis=1).reshape(-1,1))\n",
    "    return not np.any(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:45.421927Z",
     "start_time": "2025-06-02T13:33:45.419199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_policy_optimal(pi, q_pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:45.549009Z",
     "start_time": "2025-06-02T13:33:45.545988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.18181818, 5.        ]),\n",
       " array([0.54545455, 3.18181818, 1.90909091, 5.        ]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pi2_sp_given_s = get_p_pi_sp_given_s(pi2,p_sp_given_s_a)\n",
    "r_pi2 = get_r_pi(pi2, r) \n",
    "v_pi_det, q_pi_det = get_vq1(p_pi2_sp_given_s, p_sp_given_s_a, gamma, r_pi2, r)\n",
    "v_pi_det, q_pi_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:45.654228Z",
     "start_time": "2025-06-02T13:33:45.651789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_policy_optimal(pi2, q_pi_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement Algorithm\n",
    "- For each state $s \\in S$, set $\\pi'(s) \\leftarrow \\arg\\max_{a \\in A}q_\\pi(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:45.850556Z",
     "start_time": "2025-06-02T13:33:45.847960Z"
    }
   },
   "outputs": [],
   "source": [
    "def improve_policy(pi, q_pi):\n",
    "    pi_matrix = pi.reshape(2,2)\n",
    "    q_pi_matrix = q_pi.reshape(2,2)\n",
    "    new_pi_matrix = np.zeros(pi_matrix.shape)\n",
    "    indices = np.argmax(q_pi_matrix, axis=1)\n",
    "    new_pi_matrix[:, indices] = 1\n",
    "    new_pi = new_pi_matrix.reshape(-1)\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:45.968315Z",
     "start_time": "2025-06-02T13:33:45.966365Z"
    }
   },
   "outputs": [],
   "source": [
    "pi_improved = improve_policy(pi, q_pi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:46.093094Z",
     "start_time": "2025-06-02T13:33:46.090276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visitation Frequency\n",
    "Determine how many times a state or state-action pair will be visited, with weights - **discounted visitation frequency** a.k.a. **discounted distribution**.\n",
    "\n",
    "Recall $T$ is a random variable denoting the stop time.\n",
    "\n",
    "### Discounted State Visitation Frequency (Discounted State Distribution)\n",
    "Episodic Task:\n",
    "$$\n",
    "\\eta_\\pi(s) = \\sum_{t = 0}^\\infty \\mathrm{Pr}_\\pi[T = t]\\sum_{\\tau = 0}^{t-1}\\gamma^\\tau \\mathrm{Pr}_\\pi[S_\\tau = s]\n",
    "$$\n",
    "Sequential Task:\n",
    "$$\n",
    "\\eta_\\pi(s) = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau \\mathrm{Pr}_\\pi[S_\\tau = s]\n",
    "$$\n",
    "\n",
    "### Discounted State-Action Visitation Frequency (Discounted State-Action Distribution)\n",
    "Episodic Task:\n",
    "$$\n",
    "\\rho_\\pi(s,a) = \\sum_{t = 0}^\\infty\\mathrm{Pr}_\\pi[T = t]\\sum_{\\tau = 0}^{t - 1}\\gamma^\\tau \\mathrm{Pr}_\\pi[S_\\tau = s, A_\\tau = a]\n",
    "$$\n",
    "Sequential Task:\n",
    "$$\n",
    "\\rho_\\pi(s,a) = \\sum_{\\tau = 0}^\\infty \\gamma^\\tau \\mathrm{Pr}_\\pi[S_\\tau = s, A_\\tau = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties\n",
    "Episodic Task:\n",
    "$$\n",
    "\\sum_{s \\in S}\\eta_\\pi(s) = \\sum_{s \\in S, a \\in A(s)}\\rho_\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\frac{1 - \\gamma^T}{1 - \\gamma}\\right]\n",
    "$$\n",
    "Discounted Visitation/Discounted Distribution may not be a probability distribution.\n",
    "\n",
    "Sequential Task:\n",
    "$$\n",
    "\\sum_{s \\in S}\\eta_\\pi(s) = \\sum_{s \\in S, a \\in A(s)}\\rho_\\pi(s,a) = \\frac{1}{1 - \\gamma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed and Full Example\n",
    "The environment is a sequential task.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta_\\pi(\\mathrm{hungry}) &= \\sum_{\\tau = 0}^{\\infty}\\gamma^\\tau\\mathrm{Pr}_\\pi[S_\\tau = \\mathrm{hungry}]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Calculation of $\\eta_\\pi$ - Directly from Definition\n",
    "We can write\n",
    "```math\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:46.674018Z",
     "start_time": "2025-06-02T13:33:46.671785Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_p_pi_s(p_s0, tau):\n",
    "    return (np.linalg.matrix_power(p_pi_sp_given_s, tau) @ p_S0.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:47.016972Z",
     "start_time": "2025-06-02T13:33:46.778989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_pi = np.sum(np.array([gamma ** tau * get_p_pi_s(p_S0, tau) for tau in range(10000)]), axis=0)\n",
    "eta_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Calculation of $\\rho_\\pi$ - Directly from Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:47.367600Z",
     "start_time": "2025-06-02T13:33:47.112271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_pi = np.sum(np.array([gamma ** tau * pi.reshape(2,2) * np.repeat(get_p_pi_s(p_S0, tau).reshape(-1,1), 2, axis=1) for tau in range(10000)]), axis=0).reshape(-1)\n",
    "rho_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Properties\n",
    "Using discounted state-action visitation frequency to back up discounted state visitation frequency.\n",
    "$$\n",
    "\\eta_\\pi(s) = \\sum_{a \\in A(s)}\\rho_\\pi(s,a)\n",
    "$$\n",
    "Vectorised:\n",
    "$$\n",
    "\\vec{\\eta}_\\pi = \\mathbf{1}_{S \\mid S, A}\\vec{\\rho}_\\pi\n",
    "$$\n",
    "Where:\n",
    "- $\\mathbf{1}_{S \\mid S, A} = (1: s \\in S, (s,a) \\in S \\times A) \\in \\{1\\}^{|S| \\times |S||A|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:47.448384Z",
     "start_time": "2025-06-02T13:33:47.445536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:47.475942Z",
     "start_time": "2025-06-02T13:33:47.472788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(rho_pi.reshape(2,2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use discounted state visitation frequency and poliicy to back up discounted state-action visitation frequency.\n",
    "$$\n",
    "\\rho_\\pi(s,a) = \\eta_\\pi(s)\\pi(a\\mid s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:47.616471Z",
     "start_time": "2025-06-02T13:33:47.613655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:47.734445Z",
     "start_time": "2025-06-02T13:33:47.731228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.repeat(eta_pi.reshape(-1,1), 2, axis=1) * pi.reshape(2,2)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using discounted state-action visitation frequency and dynamics to back up discounted state visitation frequency.\n",
    "$$\n",
    "\\eta_\\pi(s') = p_{S_0}(s') + \\gamma \\sum_{s \\in S, a \\in A(s)}p(s'\\mid s, a) \\rho_\\pi(s, a)\n",
    "$$\n",
    "Vectorised:\n",
    "$$\n",
    "\\vec{\\eta}_\\pi = \\vec{p}_{S_0} + \\gamma P_{S_{t+1}\\mid S_t, A_t}\\vec{\\rho}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:47.941807Z",
     "start_time": "2025-06-02T13:33:47.938780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:48.072578Z",
     "start_time": "2025-06-02T13:33:48.069819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p_S0 + gamma * p_sp_given_s_a @ rho_pi.reshape(-1,1))[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equations\n",
    "Use discounted state distribution at time $t$ to back up discounted state distribution at time $t+1$:\n",
    "$$\n",
    "\\eta_\\pi(s') = p_{S_0}(s') + \\gamma\\sum_{s \\in S}p_\\pi(s'\\mid s)\\eta_\\pi(s)\n",
    "$$\n",
    "Vectorised:\n",
    "$$\n",
    "\\vec{\\eta}_\\pi = \\vec{p}_{S_0} + \\gamma P_{S_{t+1}\\mid S_t; \\pi}\\vec{\\eta}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:48.270772Z",
     "start_time": "2025-06-02T13:33:48.268106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:48.416565Z",
     "start_time": "2025-06-02T13:33:48.413203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.72727273, 2.27272727])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_S0 + gamma * p_pi_sp_given_s @ eta_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use discounted state-action distribution at time $t$ to back up the discounted state-action distribution at time $t+1$.\n",
    "$$\n",
    "\\rho_\\pi(s', a') = p_{S_0, A_0; \\pi}(s', a') + \\gamma\\sum_{s\\in S, a \\in A(s)}p_\\pi(s', a'\\mid s, a)\\rho_\\pi(s,a)\n",
    "$$\n",
    "Vectorised:\n",
    "$$\n",
    "\\vec{\\rho}_\\pi = \\vec{p}_{S_0, A_0;\\pi} + \\gamma P_{S_{t+1}, A_{t+1}\\mid S_t, A_t;\\pi}\\vec{\\rho}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:48.810676Z",
     "start_time": "2025-06-02T13:33:48.807868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:49.058916Z",
     "start_time": "2025-06-02T13:33:49.056100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68181818, 2.04545455, 1.89393939, 0.37878788])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(p_S0_A0_pi.reshape(-1) + gamma * p_pi_sp_ap_given_s_a @ rho_pi.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Visitation Frequency\n",
    "The following methods allow the computation of $\\vec{\\rho}_\\pi$ and $\\vec{eta}_\\pi$ using purely linear algebra - no loops, and more pure exactness.\n",
    "### Approach 1\n",
    "1. Rearrange $\\vec{\\eta}_\\pi$ Bellman Expectation Equation into $\\vec{\\eta}_\\pi = (I - \\gamma P_{S_{t+1}\\mid S_t})^{-1}\\vec{p}_{S_0}$ to obtain $\\vec{\\eta}_\\pi$.\n",
    "2. Use $\\rho_\\pi(s,a) = \\eta_\\pi(s)\\pi(a\\mid s)$ to obtain $\\vec{\\rho}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:49.517833Z",
     "start_time": "2025-06-02T13:33:49.515823Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_eta_rho1(gamma, p_pi_sp_given_s, p_S0, pi):\n",
    "    eta_pi = np.linalg.inv(np.eye(2) - gamma * p_pi_sp_given_s) @ p_S0\n",
    "    rho_pi = eta_pi.reshape(-1,1) * pi.reshape(2,2)\n",
    "    return eta_pi, rho_pi.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:50.221411Z",
     "start_time": "2025-06-02T13:33:50.218598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.72727273, 2.27272727]),\n",
       " array([0.68181818, 2.04545455, 1.89393939, 0.37878788]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_eta_rho1(gamma, p_pi_sp_given_s, p_S0, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2\n",
    "1. Rearrange $\\vec{\\rho}_\\pi$ Bellman Expectation Equation into $\\vec{\\rho}_\\pi = (I - \\gamma P_{S_{t + 1}, A_{t + 1}\\mid S_t, A_t; \\pi})^{-1}\\vec{p}_{S_0, A_0; \\pi}$.\n",
    "2. Use $\\vec{\\eta}_\\pi = I_{S_{t+1}\\mid S_{t}, A_t}\\vec{\\rho}_\\pi$ to obtain $\\vec{\\eta}_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:51.906793Z",
     "start_time": "2025-06-02T13:33:51.904532Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_eta_rho2(gamma, p_pi_sp_ap_given_s_a, p_S0_A0_pi):\n",
    "    rho_pi = np.linalg.inv(np.eye(4) - gamma * p_pi_sp_ap_given_s_a) @ p_S0_A0_pi.reshape(-1,1)\n",
    "    eta_pi = np.sum(rho_pi.reshape(2,2), axis=1)\n",
    "    return eta_pi, rho_pi.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:52.158872Z",
     "start_time": "2025-06-02T13:33:52.155293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.72727273, 2.27272727]),\n",
       " array([0.68181818, 2.04545455, 1.89393939, 0.37878788]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_eta_rho2(gamma, p_pi_sp_ap_given_s_a, p_S0_A0_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalence between Visitation Frequency and Policy\n",
    "We know that discounted visitation frequencies of any $\\pi$ satisfy all the following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta_\\pi(s') &= p_{S_0}(s') + \\gamma\\sum_{s \\in S, a \\in A(s)}p(s'\\mid s, a)\\rho_\\pi(s, a)\\\\\n",
    "\\eta_\\pi(s) &= \\sum_{a \\in A(s)}\\rho_\\pi(s,a)\\\\\n",
    "\\rho_\\pi(s,a) &\\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "Notice that these equations don't directly use $\\pi$. Furthermore, the solution of the above system is bijective with the policy. Therefore, if we find a solution $\\vec{\\eta}$ and $\\vec{\\rho}$ that satisfies the equation set, then we can define a policy as follows:\n",
    "$$\n",
    "\\pi(a\\mid s) = \\frac{\\rho(s, a)}{\\eta(s)}\n",
    "$$\n",
    "and the policy satisfies:\n",
    "1. $\\eta_\\pi(s) = \\eta(s)$, and\n",
    "2. $\\rho_\\pi(s, a) = \\rho(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:53.610937Z",
     "start_time": "2025-06-02T13:33:53.607987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:53.826878Z",
     "start_time": "2025-06-02T13:33:53.823756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.75      , 0.83333333, 0.16666667])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta, rho = get_eta_rho1(gamma, p_pi_sp_given_s, p_S0, pi)\n",
    "(rho.reshape(2,2) / eta.reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation over visitation Frequency\n",
    "Given deterministic function $f$, can determine expectation over discounted distribution:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}_{S\\sim \\eta_\\pi}[f(S)] &= \\sum_{s \\in S}\\eta_\\pi(s)f(s)\\\\\n",
    "    \\mathbb{E}_{(S, A)\\sim \\rho_\\pi}[f(S,A)] &= \\sum_{s\\in S, a\\in A(s)}\\rho_\\pi(s,a)f(s,a)\n",
    "\\end{align*}\n",
    "$$\n",
    "### Example\n",
    "Expected discounted return at $t = 0$ can be written as\n",
    "$$\n",
    "g_\\pi = \\mathbb{E}_{(S,A)\\sim \\rho_\\pi}[r(S,A)]\n",
    "$$\n",
    "Due to this property, we can write\n",
    "$$\n",
    "g_\\pi = \\vec{r}^\\top\\vec{\\rho}_\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:54.547044Z",
     "start_time": "2025-06-02T13:33:54.544321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-3.5606060606060614)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:54.891712Z",
     "start_time": "2025-06-02T13:33:54.888978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-3.5606060606060597)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(r @ rho_pi.reshape(-1,1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Optimal Values\n",
    "Use optimal action values at time $t$ to back up optimal state values at time $t$.\n",
    "$$v_*(s) = \\max_{a \\in A}q_*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use optimal state values at time $t+1$ to back up optimal action values at time $t$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_*(s,a) &= r(s,a) + \\gamma\\sum_{s'\\in S}p(s'\\mid s, a)v_*(s')\\\\\n",
    "&= \\sum_{s' \\in S^+, r\\in R}p(s', r\\mid s, a)[r + \\gamma v_*(s')]\n",
    "\\end{align*}$$\n",
    "In other words\n",
    "$$q_*(S_t, A_t) = \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimal Equations (BOEs)\n",
    "Using optimal state values at time $t+1$ to back up optimal state values at time $t$.\n",
    "$$v_*(s) = \\max_{a \\in A}\\left[r(s,a) + \\gamma\\sum_{s'\\in S}p(s'\\mid s, a)v_*(s')\\right]$$\n",
    "In other words:\n",
    "$$v_*(S_t) = \\max_{a \\in A}\\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using optimal action values at time $t+1$ to back up optimal action values at time $t$:\n",
    "$$q_*(s,a) = r(s,a) + \\gamma \\sum_{s'\\in S}p(s'\\mid s, a) \\max_{a'\\in A} q_*(s', a')$$\n",
    "In other words:\n",
    "$$q_*(S_t, A_t) = \\mathbb{E}\\left[R_{t+1} + \\gamma \\max_{a'\\in A} q_*(S_{t+1}, a')\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed and Full Example\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_*(\\text{hungry}) &= \\max\\{q_*(\\text{hungry}, \\text{ignore}), q_*(\\text{hungry}, \\text{feed})\\}\\\\\n",
    "v_*(\\text{full}) &= \\max\\{q_*(\\text{full}, \\text{ignore}), q_*(\\text{full}, \\text{feed})\\}\\\\\n",
    "q_*(\\text{hungry}, \\text{ignore}) &= -2 + \\frac{4}{5}(1v_*(\\text{hungry}) + 0v_*(\\text{full}))\\\\\n",
    "q_*(\\text{hungry}, \\text{feed}) &= -\\frac{1}{3} + \\frac{4}{5}\\left(\\frac{1}{3}v_*(\\text{hungry}) + \\frac{2}{3}v_*(\\text{full})\\right)\\\\\n",
    "q_*(\\text{full}, \\text{ignore}) &= -1 + \\frac{4}{5}\\left(\\frac{3}{4}v_*(\\text{hungry}) + \\frac{1}{4}v_*(\\text{full})\\right)\\\\\n",
    "q_*(\\text{full}, \\text{feed}) &= 1 + \\frac{4}{5}(0v_*(\\text{hungry}) + 1v_*(\\text{full}))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:57.113553Z",
     "start_time": "2025-06-02T13:33:57.111139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.        , -0.33333333, -1.        ,  1.        ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:57.474783Z",
     "start_time": "2025-06-02T13:33:57.471963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:58.035361Z",
     "start_time": "2025-06-02T13:33:58.032644Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_g_star_q_star(idx1, idx2, p_sp_given_s_a, r, gamma):\n",
    "    assert idx1 in (0,1)\n",
    "    assert idx2 in (2,3)\n",
    "\n",
    "    v_star = (np.linalg.inv(np.eye(2) - gamma * p_sp_given_s_a[:, [idx1,idx2]].T) @ r[[idx1,idx2]].reshape(-1,1)).reshape(-1)\n",
    "    q_star = np.zeros(4)\n",
    "    q_star[[idx1,idx2]] = v_star\n",
    "\n",
    "    idx3 = 1 - idx1\n",
    "    idx4 = 5 - idx2\n",
    "    q_star[[idx3, idx4]] = (r[[idx3,idx4]].reshape(-1,1) + gamma * p_sp_given_s_a[:, [idx3,idx4]].T @ v_star.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "    valid = q_star[idx1] > q_star[idx3] and q_star[idx2] > q_star[idx4]\n",
    "\n",
    "    return v_star, q_star, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case I:** $q_*(\\text{hungry},\\text{ignore}) > q_*(\\text{hungry},\\text{feed})$ and $q_*(\\text{full}, \\text{ignore}) > q_*(\\text{full}, \\text{feed})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:58.700173Z",
     "start_time": "2025-06-02T13:33:58.697311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-10.  ,  -8.75]),\n",
       " array([-10.        ,  -7.66666667,  -8.75      ,  -6.        ]),\n",
       " np.False_)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_g_star_q_star(0, 2, p_sp_given_s_a, r, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case II:** $q_*(\\text{hungry},\\text{feed}) > q_*(\\text{hungry},\\text{ignore})$ and $q_*(\\text{full}, \\text{ignore}) > q_*(\\text{full}, \\text{feed})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:33:59.450662Z",
     "start_time": "2025-06-02T13:33:59.447379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3. , -3.5]), array([-4.4, -3. , -3.5, -1.8]), np.False_)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_g_star_q_star(1, 2, p_sp_given_s_a, r, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case III:** $q_*(\\text{hungry},\\text{ignore}) > q_*(\\text{hungry},\\text{feed})$ and $q_*(\\text{full}, \\text{feed}) > q_*(\\text{full}, \\text{ignore})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:34:00.082225Z",
     "start_time": "2025-06-02T13:34:00.078931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-10.,   5.]),\n",
       " array([-10.        ,  -0.33333333,  -6.        ,   5.        ]),\n",
       " np.False_)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_g_star_q_star(0, 3, p_sp_given_s_a, r, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case IV:** $q_*(\\text{hungry},\\text{feed}) > q_*(\\text{hungry},\\text{ignore})$ and $q_*(\\text{full}, \\text{feed}) > q_*(\\text{full}, \\text{ignore})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:34:00.432216Z",
     "start_time": "2025-06-02T13:34:00.429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.18181818, 5.        ]),\n",
       " array([0.54545455, 3.18181818, 1.90909091, 5.        ]),\n",
       " np.True_)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_star, q_star, valid = get_g_star_q_star(1, 3, p_sp_given_s_a, r, gamma)\n",
    "v_star, q_star, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOEs only depend on dynamics of environment, and not on $p_{S_0}$.\n",
    "### Linear Programming Method\n",
    "We want to choose a $\\rho(s,a)$ that maximises\n",
    "$$\n",
    "\\sum_{s\\in S, a \\in A(s)}r(s,a)\\rho(s,a)\n",
    "$$\n",
    "such that for all $s' \\in S$ and $a \\in A$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{a'\\in A(s')}\\rho(s', a') - \\gamma \\sum_{s\\in S, a\\in A(s)}p(s'\\mid s,a)\\rho(s,a) &= p_{S_0}(s')\\\\\n",
    "\\rho(s,a) &\\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "In vectorised form:\n",
    "\n",
    "Choose a $\\vec{\\rho}$ that maximises $\\vec{r}^\\top\\vec{\\rho}$, such that\n",
    "$$\n",
    "\\begin{align*}\n",
    "(\\mathbf{1}_{S\\mid S, A} - \\gamma P_{S_{t+1}\\mid S_t, A_t})\\vec{\\rho} &= \\vec{p}_{S_0}\\\\\n",
    "\\vec{\\rho} &\\geq \\vec{0}\n",
    "\\end{align*}\n",
    "$$\n",
    "By duality of linear programming systems (Interdisciplinary Reference 2.2), we can rewrite the system as\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{\\vec{v}}\\,\\vec{p}^\\top_{S_0}\\vec{v}\\\\\n",
    "(\\mathbf{1}_{S\\mid S, A} - \\gamma P_{S_{t+1}\\mid S_t, A_t})^\\top \\vec{v} \\geq \\vec{r}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33333333, 0.75      , 0.        ],\n",
       "       [0.        , 0.66666667, 0.25      , 1.        ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_sp_given_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:34:02.088339Z",
     "start_time": "2025-06-02T13:34:02.085356Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def get_v_linprog(gamma, p_sp_given_s_a, r, p_S0):\n",
    "    I = np.array([[1, 1, 0, 0], [0, 0, 1, 1]])\n",
    "    A = I - gamma * p_sp_given_s_a\n",
    "    result = linprog(p_S0, A_ub=-A.T, b_ub=-r, method='highs') # 'highs' is a good default method\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
       "        success: True\n",
       "         status: 0\n",
       "            fun: 4.090909090909092\n",
       "              x: [ 3.182e+00  5.000e+00]\n",
       "            nit: 0\n",
       "          lower:  residual: [ 3.182e+00  5.000e+00]\n",
       "                 marginals: [ 0.000e+00  0.000e+00]\n",
       "          upper:  residual: [       inf        inf]\n",
       "                 marginals: [ 0.000e+00  0.000e+00]\n",
       "          eqlin:  residual: []\n",
       "                 marginals: []\n",
       "        ineqlin:  residual: [ 2.636e+00  0.000e+00  3.091e+00  0.000e+00]\n",
       "                 marginals: [-0.000e+00 -6.818e-01 -0.000e+00 -4.318e+00]\n",
       " mip_node_count: 0\n",
       " mip_dual_bound: 0.0\n",
       "        mip_gap: 0.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = get_v_linprog(gamma, p_sp_given_s_a, r, p_S0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.18181818, 5.        ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we get q from v\n",
    "v_result = result.x\n",
    "v_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54545455],\n",
       "       [3.18181818],\n",
       "       [1.90909091],\n",
       "       [5.        ]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_result = r.reshape(-1,1) + gamma * p_sp_given_s_a.T @ v_result.reshape(-1,1)\n",
    "q_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we don't need to use $\\vec{p}_{S_0}$. The optimal values do not depend on the initial state distribution. Consequently, **the optimal values stay the same even if we change the initial state distribution to an arbitrary distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.18181818, 5.        ])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = get_v_linprog(gamma, p_sp_given_s_a, r, np.array([5,2]))\n",
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.18181818, 5.        ])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = get_v_linprog(gamma, p_sp_given_s_a, r, np.array([100,-1]))\n",
    "result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common for of linear programming method is to use \n",
    "$$\\vec{c} = \\begin{bmatrix}1 & 1 & \\cdots & 1\\end{bmatrix}^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- May be difficult to get all params for linear programming, because doing so requires complete understanding of dynamics - may be impossible for real-world tasks which are extremely complex.\n",
    "- High space and time complexity because we have $|S|$ decision variables and $|S||A|$ constraints, which very quickly scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Optimal Values to Find Optimal Strategy\n",
    "May exist multiple optimal policies for a set of dynamics, but the optimal values are unique - **all optimal policies share same optimal values.**\n",
    "\n",
    "Picking any optimal policy occurs WLOG. For example:\n",
    "$$\\pi_*(s) = {\\arg\\max}_{a \\in A}q_*(s,a)$$\n",
    "If multiple different actions attain the maximal value, we choose any one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
