{
 "cells": [
  {
   "cell_type": "code",
   "id": "2cb8a645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:57.736152Z",
     "start_time": "2025-06-15T06:21:57.669401Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c1e1910f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:57.820198Z",
     "start_time": "2025-06-15T06:21:57.814982Z"
    }
   },
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.spec.reward_threshold)\n",
    "print(env.spec.max_episode_steps)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n",
      "0.7\n",
      "100\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8bacec63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:57.901631Z",
     "start_time": "2025-06-15T06:21:57.895407Z"
    }
   },
   "source": [
    "P = env.env.env.env.P\n",
    "P"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4271da8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:58.008386Z",
     "start_time": "2025-06-15T06:21:58.005734Z"
    }
   },
   "source": [
    "P[14]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True)],\n",
       " 2: [(0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "8edf000b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:58.124784Z",
     "start_time": "2025-06-15T06:21:58.122344Z"
    }
   },
   "source": [
    "env # Location alone cannot fully determine the state, rather location + step."
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "d09f611f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:58.225891Z",
     "start_time": "2025-06-15T06:21:58.223287Z"
    }
   },
   "source": [
    "def play_policy(env, policy, render=False):\n",
    "    episode_reward = 0\n",
    "    observation, _ = env.reset()\n",
    "    while True:\n",
    "        if render: env.render()\n",
    "        action = np.random.choice(env.action_space.n, p=policy[observation])\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return episode_reward"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "8bd6cee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:58.349278Z",
     "start_time": "2025-06-15T06:21:58.329536Z"
    }
   },
   "source": [
    "# Create a random policy that selects each action via a uniform distribution.\n",
    "random_policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "episode_rewards = [play_policy(env, random_policy) for _ in range(100)]\n",
    "np.mean(episode_rewards), np.std(episode_rewards)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.04), np.float64(0.19595917942265423))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1e01281b",
   "metadata": {},
   "source": [
    "## Book Implementation of Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f9f4c09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:58.552440Z",
     "start_time": "2025-06-15T06:21:58.549470Z"
    }
   },
   "source": [
    "def v2q(env, P, v, state=None, gamma=1):\n",
    "    # Calculate action value from state value\n",
    "    if state is not None:\n",
    "        q = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            for prob, next_state, reward, terminated in P[state][action]:\n",
    "                q[action] += prob * (reward + gamma * v[next_state] * (1 - terminated))\n",
    "    else:\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for state in range(env.observation_space.n):\n",
    "            q[state] = v2q(env, P, v, state, gamma)\n",
    "    return q"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "61c57fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:58.660770Z",
     "start_time": "2025-06-15T06:21:58.656821Z"
    }
   },
   "source": [
    "rewards = set()\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "n_rewards = 2\n",
    "\n",
    "P_tensor = np.zeros((n_states, n_rewards, n_states, n_actions))\n",
    "terminals = np.zeros(n_states).astype(bool)\n",
    "for state in P:\n",
    "    s_d = P[state]\n",
    "    for action in s_d:\n",
    "        sa_d = s_d[action]\n",
    "        for prob, next_state, reward, terminated in sa_d:\n",
    "            P_tensor[next_state, int(reward), state, action] += prob\n",
    "            terminals[next_state] = terminals[next_state] or terminated\n",
    "P_tensor.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2, 16, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "0688a222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:58.762171Z",
     "start_time": "2025-06-15T06:21:58.759306Z"
    }
   },
   "source": [
    "terminals = terminals.astype(int)\n",
    "rewards = np.array([0,1])\n",
    "terminals"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "393e4005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:58.874373Z",
     "start_time": "2025-06-15T06:21:58.871165Z"
    }
   },
   "source": [
    "np.sum(P_tensor, axis=(0,1))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "ddce0dac",
   "metadata": {},
   "source": [
    "## Vectorised v2q Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "3262959b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:59.109200Z",
     "start_time": "2025-06-15T06:21:59.106874Z"
    }
   },
   "source": [
    "def v2q_vectorised(P_tensor, rewards, v, terminals, gamma=1):\n",
    "    r_sa = np.sum(rewards[np.newaxis, :, np.newaxis, np.newaxis] * P_tensor, axis=(0,1)) # (S, A)\n",
    "    q = r_sa + gamma * np.sum(np.sum(P_tensor, axis=1) * \n",
    "                              v[:, np.newaxis, np.newaxis] *\n",
    "                              (1 - terminals[:, np.newaxis, np.newaxis]), axis=0)\n",
    "    return q"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "94102140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:59.215670Z",
     "start_time": "2025-06-15T06:21:59.211873Z"
    }
   },
   "source": [
    "# Test that the vectorised and non-vectorised v2q functions are the same.\n",
    "v_test = np.random.rand(n_states)\n",
    "start = time.time()\n",
    "non_vectorised = v2q(env, P, v_test)\n",
    "non_vectorised_time = time.time() - start\n",
    "start = time.time()\n",
    "vectorised = v2q_vectorised(P_tensor, rewards, v_test, terminals)\n",
    "vectorised_time = time.time() - start\n",
    "print(\"Vectorised is\", non_vectorised_time / vectorised_time, \" times faster.\")\n",
    "print(\"L2 norm difference:\", np.linalg.norm(vectorised - non_vectorised))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised is 0.9036458333333334  times faster.\n",
      "L2 norm difference: 1.1443916996305594e-16\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "c2f77c9b",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "756e49bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:59.440120Z",
     "start_time": "2025-06-15T06:21:59.437285Z"
    }
   },
   "source": [
    "def evaluate_policy(env, P, policy, gamma=1, tolerance=1e-6):\n",
    "    \"\"\"Book implementation.\"\"\"\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.observation_space.n):\n",
    "            vs = sum(policy[state] * v2q(env, P, v, state, gamma))\n",
    "            delta = max(delta, abs(v[state] - vs))\n",
    "            v[state] = vs\n",
    "        if delta < tolerance: break\n",
    "    return v"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "952ad7e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:59.560889Z",
     "start_time": "2025-06-15T06:21:59.558042Z"
    }
   },
   "source": [
    "def evaluate_polcy_vectorised(P_tensor, policy, rewards, terminals, gamma=1, tolerance=1e-6):\n",
    "    \"\"\"Our implementation.\"\"\"\n",
    "    v = np.zeros(P_tensor.shape[0])\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        v_next = np.sum(policy * v2q_vectorised(P_tensor, rewards, v, terminals, gamma), axis=1)\n",
    "        delta = max(delta, np.max(np.abs(v - v_next)))\n",
    "        v = v_next\n",
    "        if delta < tolerance: break\n",
    "    return v"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "3788ba4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:59.675626Z",
     "start_time": "2025-06-15T06:21:59.657506Z"
    }
   },
   "source": [
    "start = time.time()\n",
    "non_vectorised = evaluate_policy(env, P, random_policy, tolerance=1e-20)\n",
    "non_vectorised_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "vectorised = evaluate_polcy_vectorised(P_tensor, random_policy, rewards, terminals, tolerance=1e-20)\n",
    "vectorised_time = time.time() - start\n",
    "\n",
    "print(\"Vectorised is\", non_vectorised_time / vectorised_time, \" times faster.\")\n",
    "print(\"L2 norm difference:\", np.linalg.norm(vectorised - non_vectorised))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised is 3.1137452872388014  times faster.\n",
      "L2 norm difference: 1.2827573411350428e-16\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "de1748a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:21:59.777677Z",
     "start_time": "2025-06-15T06:21:59.775061Z"
    }
   },
   "source": [
    "v_random = vectorised\n",
    "v_random"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0139398 , 0.01163093, 0.02095299, 0.01047649, 0.01624867,\n",
       "       0.        , 0.04075154, 0.        , 0.0348062 , 0.08816993,\n",
       "       0.14205316, 0.        , 0.        , 0.17582037, 0.43929118,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "b4aa4c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:00.005538Z",
     "start_time": "2025-06-15T06:22:00.002763Z"
    }
   },
   "source": [
    "q_random = v2q_vectorised(P_tensor, rewards, v_random, terminals)\n",
    "q_random"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01470942, 0.0139398 , 0.0139398 , 0.01317017],\n",
       "       [0.00852357, 0.01163093, 0.0108613 , 0.0155079 ],\n",
       "       [0.02444515, 0.02095299, 0.02406034, 0.01435347],\n",
       "       [0.01047649, 0.01047649, 0.00698433, 0.01396866],\n",
       "       [0.02166489, 0.01701829, 0.01624867, 0.01006282],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.05433538, 0.04735105, 0.05433538, 0.00698433],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.01701829, 0.04099204, 0.0348062 , 0.04640827],\n",
       "       [0.07020886, 0.11755991, 0.10595784, 0.05895312],\n",
       "       [0.18940422, 0.17582037, 0.16001424, 0.04297382],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.08799677, 0.20503718, 0.23442716, 0.17582037],\n",
       "       [0.25238824, 0.53837052, 0.52711478, 0.43929118],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "21c1e8c1",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "id": "99cc1fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:00.508950Z",
     "start_time": "2025-06-15T06:22:00.506339Z"
    }
   },
   "source": [
    "def improve_policy(env, P, v, policy, gamma=1):\n",
    "    \"\"\"Book implementation.\"\"\"\n",
    "    optimal = True\n",
    "    for state in range(env.observation_space.n):\n",
    "        q = v2q(env, P, v, state, gamma)\n",
    "        action = np.argmax(q)\n",
    "        if policy[state][action] != 1:\n",
    "            optimal = False\n",
    "            policy[state] = 0\n",
    "            policy[state][action] = 1\n",
    "    return optimal"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "4ecbdafd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:00.762772Z",
     "start_time": "2025-06-15T06:22:00.760351Z"
    }
   },
   "source": [
    "def improve_policy_vectorised(P_tensor, v, rewards, policy, gamma=1):\n",
    "    \"\"\"Our implementation.\"\"\"\n",
    "    optimal = True\n",
    "    q = v2q_vectorised(P_tensor, rewards, v, terminals, gamma)\n",
    "    actions = np.argmax(q, axis=1)\n",
    "    updated_policy = np.zeros(policy.shape)\n",
    "    updated_policy[np.arange(P_tensor.shape[0]), actions] = 1\n",
    "    if not np.all(np.isclose(policy - updated_policy, 0)):\n",
    "        optimal = False\n",
    "    return optimal, updated_policy"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "037d5efd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:00.933703Z",
     "start_time": "2025-06-15T06:22:00.931785Z"
    }
   },
   "source": [
    "policy = random_policy.copy()"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "bc78bbd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:01.091615Z",
     "start_time": "2025-06-15T06:22:01.088166Z"
    }
   },
   "source": [
    "vectorised_optimal, vectorised_policy = improve_policy_vectorised(P_tensor, v_random, rewards, policy)\n",
    "vectorised_optimal, vectorised_policy"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " array([[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "1fc658a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:25:43.482854Z",
     "start_time": "2025-06-15T06:25:43.479445Z"
    }
   },
   "source": [
    "optimal = improve_policy(env, P, v_random, policy)\n",
    "policy"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "6b104b88",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab2abff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:01.579676Z",
     "start_time": "2025-06-15T06:22:01.576487Z"
    }
   },
   "source": [
    "def iterate_policy(env, P, gamma=1, tolerance=1e-6):\n",
    "    \"\"\"Book Implementation.\"\"\"\n",
    "    # Can initialise to any policy that we want.\n",
    "    policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        v = evaluate_policy(env, P, policy, gamma, tolerance)\n",
    "        iterations += 1\n",
    "        if improve_policy(env, P, v, policy, gamma): break\n",
    "    return policy, v, iterations"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "3c106739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:01.829427Z",
     "start_time": "2025-06-15T06:22:01.826881Z"
    }
   },
   "source": [
    "def iterate_policy_vectorised(P_tensor, rewards, terminals, gamma=1, tolerance=1e-6):\n",
    "    \"\"\"Our Implementation.\"\"\"\n",
    "    policy = np.ones(P_tensor.shape[2:]) / P_tensor.shape[3]\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        v = evaluate_polcy_vectorised(P_tensor, policy, rewards, terminals, gamma, tolerance)\n",
    "        iterations += 1\n",
    "        optimal, policy = improve_policy_vectorised(P_tensor, v, rewards, policy, gamma)\n",
    "        if optimal: break\n",
    "    return policy, v, iterations"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "bc5449b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:02.372877Z",
     "start_time": "2025-06-15T06:22:02.071457Z"
    }
   },
   "source": [
    "start = time.time()\n",
    "new_policy, v, iterations = iterate_policy(env, P, tolerance=1e-20)\n",
    "non_vectorised_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "new_policy_vec, v_vec, iterations_vec = iterate_policy(env, P, tolerance=1e-20)\n",
    "vectorised_time = time.time() - start\n",
    "print(\"Vectorised version is\", non_vectorised_time / vectorised_time, \"times faster.\")\n",
    "np.isclose(np.linalg.norm(v - v_vec), 0), np.all(np.isclose(new_policy, new_policy_vec)), iterations, iterations_vec"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised version is 0.9970250240446551 times faster.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.True_, np.True_, 3, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:25:21.821013Z",
     "start_time": "2025-06-15T06:25:21.817150Z"
    }
   },
   "cell_type": "code",
   "source": "v_vec.reshape(4,4)",
   "id": "76c8d63344ca14b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82352941, 0.82352941, 0.82352941, 0.82352941],\n",
       "       [0.82352941, 0.        , 0.52941176, 0.        ],\n",
       "       [0.82352941, 0.82352941, 0.76470588, 0.        ],\n",
       "       [0.        , 0.88235294, 0.94117647, 0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:22:21.809651Z",
     "start_time": "2025-06-15T06:22:21.806832Z"
    }
   },
   "cell_type": "code",
   "source": "np.argmax(new_policy_vec, axis=1).reshape(4,4)",
   "id": "60ff57cc4ea23fdb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 3],\n",
       "       [0, 0, 0, 0],\n",
       "       [3, 1, 0, 0],\n",
       "       [0, 2, 1, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:23:18.356860Z",
     "start_time": "2025-06-15T06:23:18.298038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episode_rewards = [play_policy(env, new_policy_vec) for _ in range(100)]\n",
    "np.mean(episode_rewards), np.std(episode_rewards)"
   ],
   "id": "9903a17f96a45cfa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.79), np.float64(0.40730823708832603))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mean average reward is only 0.79 because the calculation of optimal state values does not consider upper bounds on the steps.",
   "id": "c2cdd2601c9c4085"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using Value Iteration (VI)",
   "id": "ecb7b027af67e7fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T06:43:07.882550Z",
     "start_time": "2025-06-15T06:43:07.879364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def iterate_value(env, P, gamma=1, tolerance=1e-6):\n",
    "    \"\"\"Book Implementation.\"\"\"\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.observation_space.n):\n",
    "            vmax = max(v2q(env, P, v, state, gamma))\n",
    "            delta = max(delta, abs(v[state] - vmax))\n",
    "            v[state] = vmax\n",
    "        if delta < tolerance: break\n",
    "\n",
    "    # Calculate optimal policy\n",
    "    policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for state in range(env.observation_space.n):\n",
    "        action = np.argmax(v2q(env, P, v, state, gamma))\n",
    "        policy[state][action] = 1\n",
    "    return policy, v"
   ],
   "id": "2b21a936622ed28d",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:40:04.749393Z",
     "start_time": "2025-06-15T07:40:04.746316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def iterate_value_vectorised(P_tensor, rewards, terminals, gamma=1, tolerance=1e-6):\n",
    "    \"\"\"Our implementation.\"\"\"\n",
    "    v = np.zeros(P_tensor.shape[0])\n",
    "    while True:\n",
    "        delta = 0\n",
    "        v_next = np.max(v2q_vectorised(P_tensor, rewards, v, terminals, gamma), axis=1)\n",
    "        delta = max(delta, np.max(np.abs(v - v_next)))\n",
    "        v = v_next\n",
    "        if delta < tolerance: break\n",
    "\n",
    "    q = v2q_vectorised(P_tensor, rewards, v_next, terminals, gamma)\n",
    "    actions = np.argmax(q, axis=1)\n",
    "    policy = np.zeros(P_tensor.shape[2:])\n",
    "    policy[np.arange(len(actions)), actions] = 1\n",
    "    return policy, v"
   ],
   "id": "374a5f8325a2003f",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T07:42:42.873471Z",
     "start_time": "2025-06-15T07:42:42.780629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "policy_non_vec, v_non_vec = iterate_value(env, P, tolerance=1e-20)\n",
    "time_non_vec = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "policy_vec, v_vec = iterate_value_vectorised(P_tensor, rewards, terminals, tolerance=1e-20)\n",
    "time_vec = time.time() - start\n",
    "print(\"Vectorised version is\", time_non_vec / time_vec, \"times faster.\")\n",
    "np.all(np.isclose(policy_non_vec, policy_vec)), np.all(np.isclose(v_non_vec, v_vec))"
   ],
   "id": "b0c7e701aeaeb580",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised version is 2.671810001475144 times faster.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.True_, np.True_)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f03936c08310b258"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
