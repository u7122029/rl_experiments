{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6521ad0",
   "metadata": {},
   "source": [
    "# Bellman Operators and their Properties\n",
    "- Value space is complete metric space\n",
    "- 2 operators on value space: Bellman expectation operators and Bellman optimal operators\n",
    "    - proof that both operators and contraction mapping.\n",
    "- Values of policy and optimal values are fixed points of Bellman equation operators and Bellman optimal operators respectively.\n",
    "- Can use Banach fixed point theorem to find values of policy and optimal values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae89f8b",
   "metadata": {},
   "source": [
    "## Metric Spaces\n",
    "Given set $\\mathcal{X}$ and functional $d : \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$, the pair $(\\mathcal{X}, d)$ is a metric space if it satisfies:\n",
    "- Non-negativity: $d(x', x'') \\geq 0$,\n",
    "- Uniformity: $d(x', x'') = 0 \\implies x' = x''$,\n",
    "- Symmetry: $d(x', x'') = d(x'', x')$,\n",
    "- Triangle inequality: $d(x', x''') \\leq d(x', x'') + d(x', x''')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c1394",
   "metadata": {},
   "source": [
    "### Completeness\n",
    "All Cauchy sequences converge in the metric space.\n",
    "\n",
    "### What is a Cauchy Sequence?\n",
    "$$(\\forall \\epsilon > 0)(\\exists N \\in \\mathbb{N})(n, m > N \\implies d(x_n, x_m) < \\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac4b62",
   "metadata": {},
   "source": [
    "### Example Complete Metric Space\n",
    "Consider $(V = \\mathbb{R}^{|S|}, d_{\\infty})$, which is the set of all possible state values $v(s)$ for $s \\in S$ with\n",
    "$$d_{\\infty}(v, v') = \\max_{s}|v(s) - v'(s)|$$\n",
    "We are going to prove that $(V, d_\\infty)$ is a complete metric space.\n",
    "\n",
    "First consider an arbitrary Cauchy sequence $\\{v_k\\}_{k = 0}^\\infty$. Then, for every $\\epsilon > 0$, there is an $N \\in \\mathbb{N}$ such that for any $n, m > N$ we have\n",
    "$$d_{\\infty}(v_{m}, v_{n}) = \\max_{s}|v_m(s) - v_n(s)| < \\epsilon$$\n",
    "But due to the maximum, we can say that for any $s \\in S$,\n",
    "$$|v_m(s) - v_n(s)| \\leq \\max_{s}|v_m(s) - v_n(s)| < \\epsilon$$\n",
    "which makes the sequence $\\{v_k(s)\\}_{k = 0}^\\infty \\subseteq \\mathbb{R}$ for any $s$ Cauchy too, under the metric space $(\\mathbb{R}, |\\cdot|)$. Since this space is complete, then $\\{v_k(s)\\}_{k = 0}^\\infty \\subseteq \\mathbb{R}$ converges in $(\\mathbb{R}, |\\cdot|)$ to some $v_\\infty(s)$ as $k \\to \\infty$.\n",
    "\n",
    "Consequently, for any $\\epsilon > 0$, there exists a positive integer $k(s)$ such that $|v_k(s) - v_\\infty(s)| < \\epsilon$. \n",
    "\n",
    "Let $k(S) = \\max_s k(s)$, then we should have $|v_k(s) - v_\\infty(s)| < \\epsilon$ for all $k > k(S)$. \n",
    "\n",
    "Since the inequality is independent of $s$, then we can very much say that\n",
    "$$\\max_s |v_k(s) - v_\\infty(s)| = d_\\infty(v_k, v_\\infty) < \\epsilon$$\n",
    "Which means the Cauchy sequence ${v_k}_{k = 0}^\\infty$ converges under $d_{\\infty}$ and therefore $(V, d_\\infty)$ is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1509e",
   "metadata": {},
   "source": [
    "### Another Complete Metric Space\n",
    "The metric space $(Q = \\mathbb{R}^{|S||A|}, d_\\infty)$ is also complete, where.\n",
    "$$d_\\infty(q, q') = \\max_{s,a}|q(s,a) - q'(s,a)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91e053",
   "metadata": {},
   "source": [
    "### Expectation Operators, Optimal Operators\n",
    "Expectation operator on state-value space $\\mathfrak{b}_\\pi : V \\to V$:\n",
    "$$\\mathfrak{b}_\\pi(v)(s) = r_\\pi(s) + \\gamma\\sum_{s'}p_\\pi(s'\\mid s)v(s')$$\n",
    "Expectation operator on action-value space $\\mathfrak{b}_\\pi : Q \\to Q$:\n",
    "$$\\mathfrak{b}_\\pi(q)(s,a) = r(s,a) + \\gamma\\sum_{s', a'}p_\\pi(s', a'\\mid s, a)q(s', a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714eedc",
   "metadata": {},
   "source": [
    "Optimal operator on state-value space $\\mathfrak{b}_* : V \\to V$:\n",
    "$$\\mathfrak{b}_*(v)(s) = \\max_a\\left[r(s, a) + \\gamma\\sum_{s'}p(s'\\mid s, a)v(s')\\right]$$\n",
    "Optimal operator on action-value space $\\mathfrak{b}_* : Q \\to Q$:\n",
    "$$\\mathfrak{b}_*(q)(s, a) = r(s,a) + \\gamma\\sum_{s'}p(s'\\mid s, a)\\max_{a'}q(s', a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e29bf",
   "metadata": {},
   "source": [
    "## Contraction Mapping\n",
    "$\\mathfrak{f}: \\mathcal{X} \\to \\mathcal{X}$ is a contraction mapping (Lipschitz) over $(\\mathcal{X}, d)$ if there is a $\\gamma \\in (0,1)$ such that for all $x, x' \\in \\mathcal{X}$:\n",
    "$$d(\\mathfrak{f}(x), \\mathfrak{f}(x')) < \\gamma \\cdot d(x, x')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21393b62",
   "metadata": {},
   "source": [
    "### $\\mathfrak{b}_\\pi$ is a Contraction Mapping over $(V, d_\\infty)$\n",
    "We have, for all $v$, $v'$ and $s \\in S$:\n",
    "\\begin{align*}\n",
    "\\max_{s}|\\mathfrak{b}_\\pi(v)(s) - \\mathfrak{b}_\\pi(v')(s)| &= \\gamma \\max_{s}\\left|\\sum_{s'}p_\\pi(s'\\mid s)[v(s') - v'(s')]\\right|\\\\\n",
    "&\\leq \\gamma \\sum_{s'}p_\\pi(s'\\mid s)\\max_{s}|v(s) - v'(s)|\\\\\n",
    "&= \\gamma \\sum_{s'}p_\\pi(s'\\mid s)d_{\\infty}(v, v')\\\\\n",
    "&= \\gamma d_{\\infty}(v, v')\n",
    "\\end{align*}\n",
    "Very similar steps can be used to show that $\\mathfrak{b}_\\pi : Q \\to Q$ is also a contraction mapping over $(V, d_\\infty)$. I.e:\n",
    "$$d_\\infty(\\mathfrak{b}_\\pi(q), \\mathfrak{b}_\\pi(q')) \\leq \\gamma d_\\infty(q, q')$$\n",
    "This implies that $\\mathfrak{b}_\\pi$ is a contraction mapping when $0 < \\gamma < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedbfac",
   "metadata": {},
   "source": [
    "### $\\mathfrak{b}_*$ is a Contraction Mapping over $(V, d_\\infty)$\n",
    "We first prove a lemma that\n",
    "$$|\\max_x f(x) - \\max_x g(x)| \\leq \\max_x |f(x) - g(x)|$$\n",
    "Here we have two inequalities to show.\n",
    "\\begin{align*}\n",
    "\\max_x f(x) - \\max_x g(x) &\\leq \\max_x |f(x) - g(x)|\\\\\n",
    "\\max_x f(x) - \\max_x g(x) &\\geq -\\max_x |f(x) - g(x)|\n",
    "\\end{align*}\n",
    "To show the first, let $a = \\arg\\max_x f(x)$. Then,\n",
    "\\begin{align*}\n",
    "    \\max_x f(x) - \\max_x g(x) &= f(a) - \\max_x g(x)\\\\\n",
    "    &\\leq f(a) - g(a)\\\\\n",
    "    &\\leq \\max_x |f(x) - g(x)|\n",
    "\\end{align*}\n",
    "To show the second, let $b = \\arg\\max_x g(x)$. Then,\n",
    "\\begin{align*}\n",
    "    \\max_x f(x) - \\max_x g(x) &\\geq \\max_x f(x) - g(b)\\\\\n",
    "    &\\leq f(b) - g(b)\\\\\n",
    "    &\\leq -\\max_x |f(x) - g(x)|\n",
    "\\end{align*}\n",
    "and we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648391cd",
   "metadata": {},
   "source": [
    "For the main theorem we want to show that\n",
    "$$d_\\infty(\\mathfrak{b}_*(v), \\mathfrak{b}_*(v')) \\leq \\max_s|v(s) - v'(s)|$$\n",
    "We have, with use of the previous lemma:\n",
    "\\begin{align*}\n",
    "d_\\infty(\\mathfrak{b}_*(v), \\mathfrak{b}_*(v')) &= \\max_s\\left|\\max_a\\left[r(s,a) + \\gamma\\sum_{s'}p(s'\\mid s, a)v(s')\\right] - \\max_a\\left[r(s,a) + \\gamma\\sum_{s'}p(s'\\mid s, a)v'(s')\\right]\\right|\\\\\n",
    "&\\leq \\max_{s, a}\\left|\\left[r(s,a) + \\gamma\\sum_{s'}p(s'\\mid s, a)v(s')\\right] - \\left[r(s,a) + \\gamma\\sum_{s'}p(s'\\mid s, a)v'(s')\\right]\\right|\\\\\n",
    "&= \\max_{s, a}\\left|\\gamma\\sum_{s'}p(s'\\mid s, a)[v(s') - v'(s')]\\right|\\\\\n",
    "&\\leq \\gamma\\sum_{s'}(\\max_{s, a}p(s'\\mid s, a))\\max_s|v(s) - v'(s)|\\\\\n",
    "&= \\gamma \\max_s|v(s) - v'(s)|\\\\\n",
    "&= \\gamma \\cdot d_\\infty(v, v')\n",
    "\\end{align*}\n",
    "Very similar steps can be used to show that \n",
    "$$d_\\infty(\\mathfrak{b}_*(q), \\mathfrak{b}_*(q')) = \\max_{s,a}|\\mathfrak{b}_*(q)(s, a) - \\mathfrak{b}_*(q')(s, a)| \\leq \\gamma \\cdot d_\\infty(q, q')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb87ec",
   "metadata": {},
   "source": [
    "### Fixed Point Definition\n",
    "Consider a functional $f: \\mathcal{X} \\to \\mathcal{X}$ over the set $\\mathcal{X}$. An element $x \\in \\mathcal{X}$ is a fixed point if $f(x) = x$.\n",
    "\n",
    "- State values of policy satisfy Bellman expectation equations that use state values to back up state values. Therefore, state values are a fixed point of Bellman expectation operators on state-value space. Action-values of policy satisfy Bellman expectation equations that use action values to back up action values. Hence action values are fixed point of Bellman expectation operator on action-value space.\n",
    "- Same things can be said when using the Bellman optimal operators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ee6c3",
   "metadata": {},
   "source": [
    "## Banach Fixed Point Theorem\n",
    "Let $(\\mathcal{x}, d)$ be nonempty complete metric space, and $f: \\mathcal{X} \\to \\mathcal{X}$ is a contraction mapping over it. Then $f$ has a unique fixed point $x_{+\\infty} \\in \\mathcal{X}$. Furthermore, fixed point $x_{+\\infty}$ can be found as follows:\n",
    "1. Start from $x_0 \\in \\mathcal{X}$\n",
    "2. Iteratively define sequence $\\{x_k = f^k(x_0)\\}_{k = 1}^\\infty$. Then this sequence will converge to $x_{+\\infty}$.\n",
    "\n",
    "To prove this, we first show that $\\{x_k\\}$ is a Cauchy sequence. Fix $\\epsilon > 0$. We want to choose $N \\in \\mathbb{N}$ such that for arbitrary $m, n > N$, $d(x_m, x_n) < \\epsilon$. Assuming WLOG that $m < n$, We have:\n",
    "\\begin{align*}\n",
    "d(x_m, x_n) &\\leq \\gamma^m d(x_0, x_{n - m})\\\\\n",
    "&\\leq \\gamma^m\\sum_{k=0}^{n-m-1}d(x_k, x_{k+1})\\\\\n",
    "&\\leq \\gamma^m\\sum_{k=0}^{n-m-1}\\gamma^k d(x_0, x_1)\\\\\n",
    "&\\leq \\gamma^m d(x_0, x_1)\\sum_{k=0}^{\\infty}\\gamma^k\\\\\n",
    "&= \\gamma^m d(x_0, x_1)\\cdot \\frac{1}{1 - \\gamma}\\\\\n",
    "&< \\gamma^N d(x_0, x_1)\\cdot \\frac{1}{1 - \\gamma}\n",
    "\\end{align*}\n",
    "If we want the last expression to be equal to $\\epsilon$, we must set\n",
    "$$N = \\log_{\\gamma}\\left(\\frac{\\epsilon (1 - \\gamma)}{d(x_0, x_1)}\\right)$$\n",
    "And we now have\n",
    "$$d(x_m, x_n) < \\epsilon$$\n",
    "for arbitrary $m < n < N$.\n",
    "\n",
    "Since $\\{x_k\\}$ is Cauchy, then since $(\\mathcal{X}, d)$ is complete, then $\\{x_k\\}$ converges to a fixed point $x_{+\\infty}$.\n",
    "\n",
    "To show uniqueness of the fixed point, let's suppose that $x$ and $x'$ are both fixed points and $x \\neq x'$ for a contradiction. Then we have\n",
    "$$d(x, x') = d(f(x), f(x')) \\leq \\gamma \\cdot d(x, x')$$\n",
    "which implies that $d(x, x') = 0$, and consequently $x = x'$ which is a contradiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f0974",
   "metadata": {},
   "source": [
    "## Model-Based Policy Iteration\n",
    "Introduce some approaches to calculate values of given policy - numerical iterations to evaluate a policy.\n",
    "\n",
    "### Algorithm 3.1: Model-based numerical iterative policy evaluation to estimate state values.\n",
    "Inputs: $p_\\pi(s'\\mid s)$ ($s \\in S$, $s' \\in S^+$) and $r_\\pi(s)$.\n",
    "\n",
    "Output: State value estimates.\n",
    "\n",
    "Parameters: Error tolerance $\\theta_{\\max}$, or the maximal number of iterations $k_{\\max}$.\n",
    "\n",
    "1. (Initialise) Set $v_0(s) = $ arbitrary value for some $s \\in S$. If there is a terminal state, set this value to 0, i.e: $v_0(s_{\\text{end}}) \\leftarrow 0$.\n",
    "2. (Iterate) For $k \\leftarrow 0, 1, 2, \\dots$:\n",
    "    1. (Update) For each state $s \\in S$, set $v_{k+1}(s) \\leftarrow r_\\pi(s) + \\sum_{s'}p_\\pi(s'\\mid s) v_k(s')$.\n",
    "    2. (Check and break) If terminal condition for iterations is met (e.g: $|v_{k+1}(s) - v_k(s)| < \\theta_{\\max}$ holds for all $s$, or $k = k_{\\max}$), **break**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ef982",
   "metadata": {},
   "source": [
    "Performing this algorithm gives us a sequence of $v_0, v_1, \\dots$ which converges to the true state values. Why? $v_i(s) \\in \\mathbb{R}$ for all $s$ and $d(x,y) = |x - y|$ which means we are working with $(\\mathbb{R}, |\\cdot|)$ which we know is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6481329",
   "metadata": {},
   "source": [
    "### Algorithm 3.2: Model-based numerical iterative policy evaluation to estimate action values.\n",
    "Inputs: $p_\\pi(s', a'\\mid s, a)$ ($s \\in S$, $a \\in A$, $s' \\in S^+$, $a' \\in A$) and $r(s,a)$.\n",
    "\n",
    "Output: action value estimates.\n",
    "\n",
    "Parameters: $\\theta_{\\max}$, $k_{\\max}$.\n",
    "\n",
    "1. (Initialise) Set $q_0(s, a) = $ arbitrary value for some $s \\in S$, $a\\in A$. If there is a terminal state, set this value to 0, i.e: $q_0(s_{\\text{end}}, a) \\leftarrow 0$.\n",
    "2. (Iterate) For $k \\leftarrow 0, 1, 2, \\dots$:\n",
    "    1. (Update) For each state $s \\in S$, set $q_{k+1}(s,a) \\leftarrow r(s, a) + \\sum_{s', a'}p_\\pi(s', a'\\mid s, a) q_k(s', a')$.\n",
    "    2. (Check and break) If terminal condition for iterations is met (e.g: $|q_{k+1}(s, a) - q_k(s, a)| < \\theta_{\\max}$ holds for all $s$ and $a$, or $k = k_{\\max}$), **break**. \n",
    "\n",
    "Usually rely on Algorithm 3.1 because the state space is smaller, and we can obtain the action values after getting the state values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e761475",
   "metadata": {},
   "source": [
    "### Algorithm 3.3: Improved Algorithm 3.1\n",
    "Instead of keeping track of each $v_k$, we will only need to consider the previous one, and make inplace updates.\n",
    "\n",
    "1. (Initialise) Set $v(s) \\leftarrow $ arbitrary value ($s \\in S$). If there is a terminal state, set its value to 0, i.e: $v(s_\\text{end}) = 0$.\n",
    "2. (Iterate) For $k \\leftarrow 0, 1, 2, \\dots$:\n",
    "    1. If using max update diff as terminal condition, set max update diff as $\\theta \\leftarrow 0$.\n",
    "    2. For each $s \\in S$:\n",
    "        1. Calculate new state value $v_\\text{new} \\leftarrow r_\\pi(s) + \\sum_{s'}p_\\pi(s'\\mid s)v(s')$.\n",
    "        2. If using maximum update diff as terminal condition, update $\\theta \\leftarrow \\max\\left\\{\\theta, |v_\\text{new} - v(s)|\\right\\}$.\n",
    "        3. Update state value: $v(s) \\leftarrow v_\\text{new}$.\n",
    "    3. If terminal loop condition is met (e.g: $\\theta < \\theta_{\\max}$ or $k = k_{\\max}$), **break** from loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632cdda",
   "metadata": {},
   "source": [
    "### Algorithm 3.4: Alternate Improved Algorithm 3.1\n",
    "1. (Initialise) Set $v(s) \\leftarrow $ arbitrary value $s$. If there is a terminal state, $v(s_{\\text{end}}) \\leftarrow 0$.\n",
    "2. (Iterate) For $k \\leftarrow 0, 1, 2, \\dots$:\n",
    "    1. If using max update diff as terminal condition, initialise $\\theta \\leftarrow 0$.\n",
    "    2. For each $s \\in S$:\n",
    "        1. $q_\\text{new}(a) \\leftarrow r(s,a) + \\sum_{s'}p(s'\\mid s, a)v(s')$.\n",
    "        2. $v_\\text{new} \\leftarrow \\sum_a\\pi(a\\mid s)q_\\text{new}(a)$.\n",
    "        3. If using max update diff as terminal condition, then $\\theta \\leftarrow \\max\\{\\theta, |v_\\text{new} - v(s)|\\}$.\n",
    "        4. Update state value $v(s) \\leftarrow v_\\text{new}$.\n",
    "    3. If terminal condition is met($\\theta <\\theta_{\\max}$ or $k = k_{\\max}$), **break** from loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bc245",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "Combines policy evaluation and policy improvement to find optimal policy iteratively.\n",
    "\n",
    "Policy improvement loop:\n",
    "$$\\pi_0 \\overset{evaluate}{\\longrightarrow}v_{\\pi_0}, q_{\\pi_0} \\overset{improve}{\\longrightarrow} \\pi_1 \\overset{evaluate}{\\longrightarrow} \\cdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d22f74",
   "metadata": {},
   "source": [
    "Policy improvement is a strict improvement. So new policy will differ from old policy.\n",
    "\n",
    "For finite MDP, both state adn action spaces finite, so possible policies are finite.\n",
    "\n",
    "Consequently, sequence of policies must converge ($\\pi_k \\to \\pi$ as $k \\to \\infty$). Furthermore, there will exist some $K$ such that $k > K$ implies $\\pi_k = \\pi_{k+1}$ for every $k$ and state $s$.\n",
    "\n",
    "Furthermore, we will have $\\pi_k(s) = \\pi_{k+1}(s) = \\arg\\underset{a}{\\max}\\,q_{\\pi_k}(s,a)$.\n",
    "\n",
    "Hence, $v_{\\pi_k}(s) = \\underset{a}{\\max}\\, q_{\\pi_k}(s,a)$, which satisfies Bellman Optimal Equations. So $\\pi_k$ is optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59263e",
   "metadata": {},
   "source": [
    "### Algorithm 3.5: Model-based Policy Iteration\n",
    "Input: dynamics $p$.\n",
    "\n",
    "Output: Optimal policy estimate.\n",
    "\n",
    "Parameters: as required.\n",
    "\n",
    "1. (Intialise) Initialise $\\pi_0$ as arbitrary deterministic policy.\n",
    "2. For $k \\leftarrow 0, 1, 2, \\dots$:\n",
    "    1. (Evaluate Policy) Calculate the values of $\\pi_k$ using policy evaluation algorithm (3.1, 3.3 or 3.4). Save these in $q_{\\pi_k}$.\n",
    "    2. (Improve Policy) Use action values in $q_{\\pi_k}$ to improve deterministic policy $\\pi_k$, resulting in improved $\\pi_{k+1}$.\n",
    "    3. If $\\pi_{k+1} = \\pi_k$, **break** from loop, return policy $\\pi_k$ as optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedad614",
   "metadata": {},
   "source": [
    "### Algorithm 3.6: Model-based Policy Iteration (space-saving person).\n",
    "1. (Initialise) Initialise $\\pi$ as arbitrary deterministic policy.\n",
    "2. Repeat:\n",
    "    1. (Evaluate Policy) Use policy evaluation algorithm (Algorithm 3.1, 3.3 or 3.4) to calculate values of $\\pi$ and save action values in $q$.\n",
    "    2. (Improve Policy) Use action values $q$ to improve policy, and save updated policy in $\\pi$.\n",
    "    3. If policy improvement algorithm indicates that $\\pi$ is optimal, break loop and return $\\pi$ as optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac2204",
   "metadata": {},
   "source": [
    "## Value Iteration (VI)\n",
    "Method to find optimal values iteratively. Algorithm 3.1 uses Bellman expectation operator to find state values of given state.\n",
    "\n",
    "Here we use similar structure, but use Bellman Optimal Operator to find optimal values and optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddf1b8",
   "metadata": {},
   "source": [
    "### Algorithm 3.7 (Model-based VI)\n",
    "1. (Initialise) Set $v_0(s) \\leftarrow$ arbitrary value $s \\in S$, with $v_0(s_\\text{end})\\leftarrow 0$.\n",
    "2. (Iterate) For $k \\leftarrow 0, 1, 2, \\dots$:\n",
    "    1. (Update) For each $s \\in S$, set\n",
    "    $$v_{k+1}(s)\\leftarrow \\max_a\\left[r(s,a) + \\gamma \\sum_{s'}p(s'\\mid s, a)v_k(s')\\right]$$\n",
    "    2. (Check and break) If terminal condition met ($|v_{k+1}(s) - v_k(s)| < \\theta$ for all $s$ or $k = k_{\\max}$), then **break** from loop.\n",
    "3. (Calculate optimal policy) For each state $s \\in S$, calculate action of optimal deterministic policy:\n",
    "$$\\pi(s) \\leftarrow \\arg\\max_a\\left[r(s,a) + \\gamma \\sum_{s'}p(s'\\mid s, a)v_{k+1}(s')\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69676d",
   "metadata": {},
   "source": [
    "### Algorithm 3.8 (Space-saving Algorithm 3.7)\n",
    "1. (Initialise) Set $v(s) \\to 0$ and $v(s_\\text{enc}) \\to 0$.\n",
    "2. (Iterate) for $k \\leftarrow 0, 1, 2, \\dots$:\n",
    "    1. If using max update diff as terminal condition, then set $\\theta \\leftarrow 0$.\n",
    "    2. For each $s \\in S$:\n",
    "        1. Calculate new state value\n",
    "        $$v_\\text{new} \\leftarrow \\max_a\\left[r(s,a) + \\gamma \\sum_{s'}p(s'\\mid s, a)v_k(s')\\right]$$\n",
    "        2. If using max update diff as terminal condition then update $\\theta \\leftarrow \\max\\{\\theta, |v_\\text{new} - v(s)|\\}$.\n",
    "        3. Update state value $v(s) \\leftarrow v_\\text{new}$.\n",
    "    3. If terminal condition met ($\\theta < \\theta_{\\max}$ or $k = k_{\\max}$), then **break** from loop.\n",
    "3. (Calculate Optimal Policy) For each $s \\in S$, calculate action of optimal deterministic policy:\n",
    "$$\\pi(s) \\leftarrow \\arg\\max_a\\left[r(s,a) + \\gamma \\sum_{s'}p(s'\\mid s, a)v_{k+1}(s')\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237aec30",
   "metadata": {},
   "source": [
    "## Bootstrapping and Dynamic Programiming\n",
    "The optimised algorithms employ the use of bootstrapping and dynamic programming.\n",
    "\n",
    "Bootstrapping describes a non-parametric method that uses existing samples and their own statistics to generate new samples and their stats.\n",
    "\n",
    "Here, we estimate $v_{k+1}$ based on v_k$. Previous $v_k$ can be biased and estimating subsequent values will propagate it, but it makes use of existing estimates for further estimation.\n",
    "### Dynamic Programming\n",
    "- Divide a complex problem into many easier subproblems, each of which can be further divided into subproblems.\n",
    "- Lots of subproblems are the same kind. So we can save a lot of computation by reusing the results of previously calculated subproblems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febfc4b8",
   "metadata": {},
   "source": [
    "DP is important here because in the utilisation of state values to back up state values, we needed to find the solution to $|S|$ simultaneous equations, but the space complexity of performing this is high.\n",
    "\n",
    "Instead, we used previous estimates to compute subsequent $v_k$ values, and with enough iterations we converge to a unique solution. This in many cases is easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885f8a5",
   "metadata": {},
   "source": [
    "Even then, some real-world problems will encounter difficulty in using DP directly because the state space is very large (e.g: Go), making it impossible to sweep all states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa40e9f",
   "metadata": {},
   "source": [
    "### Prioritised Sweeping\n",
    "Select states as follows:\n",
    "- After state value updated, consider all states that can be impacted.\n",
    "- Calculate Bellman error of those states.\n",
    "$$\\delta = \\max_a\\left[r(s,a) + \\gamma \\sum_{s'}p(s'\\mid s,a)v(s')\\right] - v(s)$$\n",
    "Larger $|\\delta|$ indicates that updating state $s$ will have bigger impacts. Hence may choose state with largest Bellman error.\n",
    "\n",
    "May use Priority Queue to maintain values of $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ea1d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
